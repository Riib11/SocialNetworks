{"entities":["Algorithm","C++","Central processing unit","Execution unit","Graphics processing unit","Heterogeneous computing","Manycore processor","Model of computation","Multi-core processor","Open-source software","Parallel computing","Performance Evaluation","SIMD","Sequence database","Shattered World","Smith–Waterman algorithm"],"journalVolume":"","journalPages":"42-51","pmid":"","year":2017,"outCitations":["86cc3ad9ba843d729df2692d2b24038c029a1e89","6cee78bbad9731a015d1084176add9afd3420b5c","583a320a9c612124d62da5741fede120495126fc","27a9635d723b08c543f4ed979b58ed56d1816965","785ba684ef8219a51db12dce9fc587a1e619cd70","649e858552ed8a289b94ae1f33846b1255b3f07d","90732d2e1914688cfaa8ec6b0cca59ce5ad031a4","1b7a288a22f580bbf5698e2ac26b289a26eaa180","babb04bc6d22d4d9c6606d9ab02ad0bc6dc9c5f8","ce39082a41392f1691f62ea1cc505119d6862c84","042a1f231982b9618facdcb16bd7f91135a6fdfd","a80b76991ea3c2bd09b63d31037f3d2c794c305e","950bca8374bf36421957b416e4f58425e9d43095","f9685e1cacb684ec988250924c35ca339c709948","03a2edb24a141d529252d882dd5e15ab8351d247","7d255de4859046e3c0ebae94d507e7670c21bf9d","d0b9559b098b224008adc82ba2d0d86009ba807c","668a3711456d466588ff7ca0a30d672807b0229d","4bc3b6d26e404869a2d7db0b642423f51c8ce86a","75c9230a3388480d146022a9ee41b8585e677a55","139954109f31d2988cceeec1f8b382c3e6d38bbf","230d4da76106248060201fe327eb56c8b5d52625","28552ecf4eaedb3461edca97304b29082b02fbab","4a875009283ad5f262eb828ab270e3af3c5b12b2","40c5441aad96b366996e6af163ca9473a19bb9ad","85bb2a3f3684334ba1e5ad6bc7795a0330cf5421","1c15f69566af6198f336b961aecf418b5ccd07d3"],"s2Url":"https://semanticscholar.org/paper/bc866300facbd7f28c294748a89d1505e9e1df1f","s2PdfUrl":"","id":"bc866300facbd7f28c294748a89d1505e9e1df1f","authors":[{"name":"Haidong Lan","ids":["2827531"]},{"name":"Weiguo Liu","ids":["3429925"]},{"name":"Yongchao Liu","ids":["2916386"]},{"name":"Bertil Schmidt","ids":["38613433"]}],"journalName":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","paperAbstract":"Computer architectures continue to develop rapidly towards massively parallel and heterogeneous systems. Thus, easily extensible yet highly efficient parallelization approaches for a variety of platforms are urgently needed. In this paper, we present SWhybrid, a hybrid computing framework for large-scale biological sequence database search on heterogeneous computing environments with multi-core or many-core processing units (PUs) based on the Smith- Waterman (SW) algorithm. To incorporate a diverse set of PUs such as combinations of CPUs, GPUs and Xeon Phis, we abstract them as SIMD vector execution units with different number of lanes. We propose a machine model, associated with a unified programming interface implemented in C++, to abstract underlying architectural differences. Performance evaluation reveals that SWhybrid (i) outperforms all other tested state-of-the-art tools on both homogeneous and heterogeneous computing platforms, (ii) achieves an efficiency of over 80% on all tested CPUs and GPUs and over 70% on Xeon Phis, and (iii) achieves utlization rates of over 80% on all tested heterogeneous platforms. Our results demonstrate that there is enough commonality between vector-like instructions across CPUs and GPUs that one can develop higher-level abstractions and still specialize with close-to-peak performance. SWhybrid is open-source software and freely available at https://github.com/turbo0628/swhybrid.","inCitations":[],"pdfUrls":["https://doi.org/10.1109/IPDPS.2017.42"],"title":"SWhybrid: A Hybrid-Parallel Framework for Large-Scale Protein Sequence Database Search","doi":"10.1109/IPDPS.2017.42","sources":["DBLP"],"doiUrl":"https://doi.org/10.1109/IPDPS.2017.42","venue":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)"},
{"entities":["Algorithms for Recovery and Isolation Exploiting Semantics","Blocking (computing)","Concurrency (computer science)","Direct memory access","Heuristic","High- and low-level","InfiniBand","Non-blocking algorithm","Remote direct memory access","Revolution in Military Affairs","SPMD","Scheduling (computing)","Software portability","Speedup","Synthetic data","Throughput"],"journalVolume":"","journalPages":"988-997","pmid":"","year":2017,"outCitations":["150c6c618a3ea2b3180f53cf0a63e172add8a872","201b01c3d3c87dec8e09fc44536d1d30adcffbf0","0d3e6362886b326901c5d740767d9aa7172bdb55","0ca1e465dd85b8254bcdd7053032d7eab6e2d4b4","4fafd03a57348374f894823b0c7cfe6c85e5bd93","9860074998cf01059d46bec2063f059276e749d8","5d8e5e3eef73dbd4cc3b01cb323dcc6369532f66","41ab20fc1a3815dc8aa4e23b0316e26f7acb5f8e","8beda53e2b3cd04434aaaabef463ef8e18706df1","f8e9b050c93af6dea582563f61b6460b590bc3af","09cc6a87c7ae189ae6dbc2fd246c1b06726ab3c5","057a8310124ef6565fbd13ae1ec1412b96dedae8","25f017efd2905c6d0c6a92f2dfe19113ee42938e","30c8c8f389ee00b6321814d35412698d0a28307b","f4c217923ceebd709e8eb106b1f7d25fd5d088c2","4110d5ad162fbf43a3418f28b4d46609c2a147be"],"s2Url":"https://semanticscholar.org/paper/2fbac3e9572e08ce9ce63a64e9e0e7a7ffbe7083","s2PdfUrl":"","id":"2fbac3e9572e08ce9ce63a64e9e0e7a7ffbe7083","authors":[{"name":"Wim Lavrijsen","ids":["2998413"]},{"name":"Costin Iancu","ids":["1702354"]}],"journalName":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","paperAbstract":"We present methods for the effective application level reordering of non-blocking RDMA operations. We supplement out-of-order hardware delivery mechanisms with heuristics to account for the CPU side overhead of communication and for differences in network latency: a runtime scheduler takes into account message sizes, destination and concurrency and reorders operations to improve overall communication throughput. Results are validated on InfiniBand and Cray Aries networks, for SPMD and hybrid (SPMD+OpenMP) programming models. We show up to 5! potential speedup, with 30-50% more typical, for synthetic message patterns in microbenchmarks. We also obtain up to 33% improvement in the communication stages in application settings. While the design space is complex, the resulting scheduler is simple, both internally and at the application level interfaces. It also provides performance portability across networks and programming models. We believe these techniques can be easily retrofitted within any application or runtime framework that uses one-sided communication, e.g. using GASNet, MPI 3.0 RMA or low level APIs such as IBVerbs.","inCitations":[],"pdfUrls":["https://doi.org/10.1109/IPDPS.2017.98"],"title":"Application Level Reordering of Remote Direct Memory Access Operations","doi":"10.1109/IPDPS.2017.98","sources":["DBLP"],"doiUrl":"https://doi.org/10.1109/IPDPS.2017.98","venue":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)"},
{"entities":["Amazon Web Services","Apache Hadoop","Complex system","Data center","Fairness measure","Interaction","Job scheduler","Jumpstart Our Business Startups Act","Linear temporal logic","MapReduce","Multitenancy","Power management","Reinforcement learning","Scheduling (computing)","Temporal logic"],"journalVolume":"","journalPages":"133-142","pmid":"","year":2017,"outCitations":["a8b5bb125c1f3eb3ccf5ad860abacbd8b042e5b6","117fc19e17a12c49eebcfce9e1b9870fd04bd230","573f79a888993a3cd32d3380ba2cfe8668539332","f1269591359fddc20f95da10c7bd4c054080b447","5b03317403f4cd2526b5e8c1df74d5d0adf96641","3e257f01e3ee71545d824a1615c35659525b856a","073e26aa7192825a8d872fb0c6f25bc31aca77cf","980773ca869fc17562e4fbcf4202a8f21893b114","1b2a950e79eec4bd731b0b5f76ac0946481e328a","6c429bd0b68e685af16f98866d05bb6c561289de","aeda1dca3f62ea3e16b17a846f32e9f5e98fe6ce","c3c262b8e56536d14826926b69af59eaefc29bc2","071f0054c73024be125f4c9daaea516b3b6ea4cf","5c5f8717f31b7e4334b450df15442223f988ff41","5b9631561a89a3e071d8ec386a616a120220bfd9","f4bd6691f59eb95c58b6d104d5122158ab7ddaa4","2988e34168fa91398fa397baf823af2063893e9c","0c83169bf4ebb29979bfe47708cb6b79b6e28755","03564af5d388c08123f527cea3c1252c20093c05","6707147b1a2f4c037fc1b55f509a0ba7c6701e56","1a617b1c29596e3d09b8bf378ae20568932834cb","c39e98c6ef583a5555ce2f5823e242afb64a5432","8c05becd4d5e39d3d8c3b1fd36b643068bc4254b","8d26f3a3726f6dd2741bf12fcd3bc1abdeab482a","661b19ff987b9ed9d9252324d4a72ab1fbd588ae"],"s2Url":"https://semanticscholar.org/paper/3386403da8d9dc141f11e92854f3d830e1c8f401","s2PdfUrl":"","id":"3386403da8d9dc141f11e92854f3d830e1c8f401","authors":[{"name":"Hao He","ids":["7146460"]},{"name":"Jiang Hu","ids":["33797843"]},{"name":"Dilma Da Silva","ids":["2045541"]}],"journalName":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","paperAbstract":"Resource management of modern datacenters needs to consider multiple competing objectives that involve complex system interactions. In this work, Linear Temporal Logic (LTL) is adopted in describing such interactions by leveraging its ability to express complex properties. Further, LTL-based constraints are integrated with reinforcement learning according the recent progress on control synthesis theory. The LTL-constrained reinforcement learning facilitates desired balance among the competing objectives in managing resources for datacenters. The effectiveness of this new approach is demonstrated by two scenarios. In datacenter power management, the LTL-constrained manager reaches the best balance among power, performance and battery stress compared to the previous work and other alternative approaches. In multitenant job scheduling, 200 MapReduce jobs are emulated on the Amazon AWS cloud. The LTL-constrained scheduler achieves the best balance between system performance and fairness compared to several other methods including three Hadoop schedulers.","inCitations":[],"pdfUrls":["https://doi.org/10.1109/IPDPS.2017.27"],"title":"Enhancing Datacenter Resource Management through Temporal Logic Constraints","doi":"10.1109/IPDPS.2017.27","sources":["DBLP"],"doiUrl":"https://doi.org/10.1109/IPDPS.2017.27","venue":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)"},
{"entities":["Algorithm","Cholesky decomposition","Computation","Linear algebra","Linear system","Matrix multiplication","Numerical analysis","Numerical linear algebra","Numerical stability","Parallel algorithm","Scalability","System of linear equations","Tamper-resistant security module","Triangular matrix"],"journalVolume":"","journalPages":"678-687","pmid":"","year":2017,"outCitations":["ad5e1e6c5b48f7f2cdafe306fbcac55b0be755f0","c640098d8fec542f85bab54fd02d8abc4a9b21ea","bf980d3cc50ae14ce104207882ee1fbbadf7a5f1","0777845ab14d61d970354acd0a0ca8aaf57f0041","4ff7a5d31c2524f62662da67a22560867e025456","a0d3306999eacc7fab93955eb1223eef10312708","035c542402de661b544603d84b7ec45bada14e7f","04373d13bedbf3c4276a8b3b86311a1bff99db75","3adaacec6270c00060bddce342ef9503ce7c648c","b582d4a005c3288858eb3910e9233edb35323f49","c83ced20b5ebc150a5eb0769d45dee5bf28207df","45c3066f6cc0262a5b5bc56124f4d2187a961c42","2a0c9f8248aad793810dfc2ec2bc21a8ebdca6f4","8a269f794c54b62d81ba76d23aaa4bdf12301ec8","5cfeda94aaa59702e57647045de1488b8258abef","84ba025c6b28617241274699dccd9e5308fba766","1aa8ad634d1879af9b5ac34b44ecc3de8debd276","3e58c1263047ccc126ca0c06dcb150ff9d172512","b7bb051c2376345f5c5e80f165b15f2f2e68ecc9","fc00ada92d8b65c4e4599a82b621da5e509c84d0","6fa09bcca34aae148dbec5cc8aaad283febfe037"],"s2Url":"https://semanticscholar.org/paper/68acde368151954316e22985e394a591d9bb85ed","s2PdfUrl":"","id":"68acde368151954316e22985e394a591d9bb85ed","authors":[{"name":"Tobias Wicky","ids":["7541393"]},{"name":"Edgar Solomonik","ids":["2880213"]},{"name":"Torsten Hoefler","ids":["1713648"]}],"journalName":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","paperAbstract":"We present a new parallel algorithm for solving triangular systems with multiple right hand sides (TRSM). TRSM is used extensively in numerical linear algebra computations, both to solve triangular linear systems of equations as well as to compute factorizations with triangular matrices, such as Cholesky, LU, and QR. Our algorithm achieves better theoretical scalability than known alternatives, while maintaining numerical stability, via selective use of triangular matrix inversion. We leverage the fact that triangular inversion and matrix multiplication are more parallelizable than the standard TRSM algorithm. By only inverting triangular blocks along the diagonal of the initial matrix, we generalize the usual way of TRSM computation and the full matrix inversion approach. This flexibility leads to an efficient algorithm for any ratio of the number of right hand sides to the triangular matrix dimension. We provide a detailed communication cost analysis for our algorithm as well as for the recursive triangular matrix inversion. This cost analysis makes it possible to determine optimal block sizes and processor grids a priori. Relative to the best known algorithms for TRSM, our approach can require asymptotically fewer messages, while performing optimal amounts of computation and communication in terms of words sent.","inCitations":["41875eaea7cb58024b1bd46f9d9df80d19208e6b"],"pdfUrls":["https://arxiv.org/pdf/1612.01855v1.pdf","https://doi.org/10.1109/IPDPS.2017.104","https://arxiv.org/pdf/1612.01855.pdf","https://arxiv.org/pdf/1612.01855v2.pdf","https://htor.inf.ethz.ch/publications/img/wicky-commavoiding-trsm.pdf","http://arxiv.org/abs/1612.01855"],"title":"Communication-Avoiding Parallel Algorithms for Solving Triangular Systems of Linear Equations","doi":"10.1109/IPDPS.2017.104","sources":["DBLP"],"doiUrl":"https://doi.org/10.1109/IPDPS.2017.104","venue":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)"},
{"entities":["Computer cooling","Java HotSpot Virtual Machine","Job scheduler","Jumpstart Our Business Startups Act","Location awareness","Next-generation network","Scheduling (computing)","Simulation","Supercomputer","Throughput"],"journalVolume":"","journalPages":"728-737","pmid":"","year":2017,"outCitations":["9ba533adf4776c0a708d2f5a2431ce2ab35bf915","3583241ac041f60b845395b13ce0d90caf41f49f","bb58f3858c937d6769ea8a3b6fc02e04a6521e82","239e046347d5075b3eeef5439050e9f2ca760b7b","075cac76a487db1c11751f340ada8cd59e1e2017","8303554a48d900acf0a432fe06e48d48c5962601","7d21404a90d7bf9b75c140bc0b6546551bd91979","4c059a8900d24058c9cb27b85df96cc430a79970","30a001817f503fa9aa46b01e6dbde887e94cfc3d","208a5e499a2836effd9d15c2ff867cf5697796ac","494c4c60ab265415d29fd378583e1e295f20bcfe","6318fbbde6eb3cc0175b7fb1856b7dd116b8b710","2ca6b56fd65b4fa486d754af55e19771f56a3b60","3b43f4fca2fcfd7f351ccd78076032b312b52221","35e646293776581b01700c1d2d5ac4885a9d56b9"],"s2Url":"https://semanticscholar.org/paper/03c7a20c919dd3b6996124a96b199b0b2836d462","s2PdfUrl":"","id":"03c7a20c919dd3b6996124a96b199b0b2836d462","authors":[{"name":"Thang Cao","ids":["12898292"]},{"name":"Wei Huang","ids":["1730584"]},{"name":"Yuan He","ids":["40498718"]},{"name":"Masaaki Kondo","ids":["1683736"]}],"journalName":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","paperAbstract":"Limited power budget is becoming one of the most crucial challenges in developing supercomputer systems. Hardware overprovisioning which installs a larger number of nodes beyond the limitations of the power constraint is an attractive way to design next generation supercomputers. In air cooled HPC centers, about half of the total power is consumed by cooling facilities. Reducing cooling power and effectively utilizing power resource for computing nodes are important challenges. It is known that the cooling power depends on the hotspot temperature of the node inlets. Therefore, if we minimize the hotspot temperature, performance efficiency of the HPC system will be increased. One of the ways to reduce the hotspot temperature is to allocate power-hungry jobs to compute nodes whose effect on the hotspot temperature is small. It can be accomplished by optimizing job-to-node mapping in the job scheduler. In this paper, we propose a cooling and node location-aware job scheduling strategy which tries to optimize job-to-node mapping while improving the total system throughput under the constraint of total system (compute nodes and cooling facilities) power consumption. Experimental results with the job scheduling simulation show that our scheduling scheme achieves 1.49X higher total system throughput than the conventional scheme.","inCitations":["d206e9a132b7eb00840b47da84e2960b720065e3"],"pdfUrls":["https://doi.org/10.1109/IPDPS.2017.19"],"title":"Cooling-Aware Job Scheduling and Node Allocation for Overprovisioned HPC Systems","doi":"10.1109/IPDPS.2017.19","sources":["DBLP"],"doiUrl":"https://doi.org/10.1109/IPDPS.2017.19","venue":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)"},
{"entities":["Cache (computing)","Central processing unit","Critical path method","Dynamic program analysis","Locality of reference","Microarchitecture","OSI model","Performance prediction","Scalability"],"journalVolume":"","journalPages":"317-326","pmid":"","year":2017,"outCitations":["147ed9f73bff0513be17a6026cad14dea68186f7","48f59efbc20fef3e571b9fb81d039bfa9619a9bf","6800600e6451d0bf0a4e866a483cac8c8617da88","bf33756bc551581e0f7a1d43e148b305ff2296d7","1824405dd805a02dccc85ce4341abfe0799282a8","15708276fbb98a7d3f8835a2c51cb522eeab9967","69cd319c93692acc0822eeec743706515c693471","85a96bb5d97a1f8aafb65d88848bc7ebb0a0e3be","705c20122d0f139e747c14a9879f9bb5ae65387a","7a4a6c94aa2edb834ef19bd4568a4e84673fd8d6","e240f361ee577a1662b719a23c6117ecad3d307c","0aa48caa248c27ed8905a0123cd1c29ff0dc4968","0f9080d297fc22dcf24dfd8ffcd3de5cea04c689","e1fe8bda7373de0a8bc4382de18ec086c10de3b3","aae636bd99bc4bae4cd4afcfa4621ef573a55c26","a0ba323d58a1879fb877cc92293ed0f631317af4","3d19057951ec74bd5b7ad8e4fe09eaea1dfca3b1","178599e5e976e82528e71cb2e1b812d588fa0e44","e9fa7e93d2f4e958cde16c6cf9d5bc966f8e7ffc","05941c054493f241523dc6545c825ee38df3959a","021d7ca29cea1a55e5095e17ddb658e54e054793","28d43fffcbaf2eb7e2d9b931a9cb2082399e8409","42be8c9380613754c82782ae86291d3c379f2ead"],"s2Url":"https://semanticscholar.org/paper/a810c97eb8aee081747bda8af7749cc6ecc28769","s2PdfUrl":"","id":"a810c97eb8aee081747bda8af7749cc6ecc28769","authors":[{"name":"Ryan D. Friese","ids":["19231918"]},{"name":"Nathan R. Tallent","ids":["3247453"]},{"name":"Abhinav Vishnu","ids":["1692279"]},{"name":"Darren J. Kerbyson","ids":["1715527"]},{"name":"Adolfy Hoisie","ids":["1753153"]}],"journalName":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","paperAbstract":"Many applications have irregular behavior &#x2014; e.g., input-dependent solvers, irregular memory accesses, or unbiased branches &#x2014; that cannot be captured using today's automated performance modeling techniques. We describe new hierarchical critical path analyses for the Palm model generation tool. To obtain a good tradeoff between model accuracy, generality, and generation cost, we combine static and dynamic analysis. To create a model's outer structure, we capture tasks along representative MPI critical paths. We create a histogram of critical tasks with parameterized task arguments and instance counts. To model each task, we identify hot instruction-level paths and model each path based on data flow, data locality, and microarchitectural constraints. We describe application models that generate accurate predictions for strong scaling when varying CPU speed, cache and memory speed, microarchitecture, and (with supervision) input data class. Our models' errors are usually below 8%; and always below 13%.","inCitations":[],"pdfUrls":["https://doi.org/10.1109/IPDPS.2017.61"],"title":"Generating Performance Models for Irregular Applications","doi":"10.1109/IPDPS.2017.61","sources":["DBLP"],"doiUrl":"https://doi.org/10.1109/IPDPS.2017.61","venue":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)"},
{"entities":["Artificial neural network","Artificial neuron","Byzantine fault tolerance","Computation","Distributed computing","Failure rate","Fault tolerance","Neural Networks","Neuron","Social network","Synaptic Package Manager"],"journalVolume":"","journalPages":"1028-1037","pmid":"","year":2017,"outCitations":["563e821bb5ea825efb56b77484f5287f08cf3753","01d47976b70be92ed2a5e06f8bf19a3af5439aad","11951e035379c1dc039af868fbaba8d9737702c3","38ad1e2d6e85d2e68901fe20c45769fa343726d1","34f25a8704614163c4095b3ee2fc969b60de4698","58ceeb151558c1f322b9f6273b47e90e9c04e6b1","126df9f24e29feee6e49e135da102fbbd9154a48","04113e8974341f97258800126d05fd8df2751b7e","171ef6765ddf9d22806146d8327ba082028ec32f","f74ded11f72099d16591a1191d72262ae6b5f14a","443044b5873706aadaeeb87c8594528d83687462","351ff36dff3c2f8b4bf0b49c0d1a46042e4e0716","17e1bb7fc17b45fe5ad8724a635d285ed000efa8","17c0a7de3c17d31f79589d245852b57d083d386e","9952d4d5717afd4a27157ed8b98b0ee3dcb70d6c","1766e97c83f698ce3a292bc851a3bdee8179fba6","3e0080a34eca4eabb9b371c2b3c369dc4dc90112","80c593a0668f4eb157a525831b7daad3bdb44381","64192e5b1ce902b9dc397c53641d876a8a5506cd","43cab718dbfdb9e9b0a515e897f8e26f3e0ac935","2c94451cfcd8399fd0030c1846a5a012b49660a6","f4416d7035103e9d39622178b65f229d68bb6a96","4dc1641582a60abdc66a9d818c313a9d783a74be","0122e063ca5f0f9fb9d144d44d41421503252010","891e61c55b49dc55e95c4ed1803cd0801df02d00","222e13bfa2eddcfe9a3eb6895f05186a3bd05b22","d12d1289d2384c2ce642f01855637b9f0519e189","251b6726fcad6b853de3d22939a4b2e36ef802ea","b05b2d3a93cf01229cfbe124475d70674d91134d","e637d66ac3c76f805b4873095a748d1035f1f435"],"s2Url":"https://semanticscholar.org/paper/3fde9f175dd91ebd5accbb337c9abdefef31c81b","s2PdfUrl":"","id":"3fde9f175dd91ebd5accbb337c9abdefef31c81b","authors":[{"name":"El Mahdi El Mhamdi","ids":["9623412"]},{"name":"Rachid Guerraoui","ids":["1727558"]}],"journalName":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","paperAbstract":"Neural networks have been traditionally considered robust in the sense that their precision degrades gracefully with the failure of neurons and can be compensated by additional learning phases. Nevertheless, critical applications for which neural networks are now appealing solutions, cannot afford any additional learning at run-time. In this paper, we view a multilayer neural network as a distributed system of which neurons can fail independently, and we evaluate its robustness in the absence of any (recovery) learning phase. We give tight bounds on the number of neurons that can fail without harming the result of a computation. To determine our bounds, we leverage the fact that neuralactivation functions are Lipschitz-continuous. Our bound isgiven in the form of quantity, we call the Forward ErrorPropagation, computing this quantity only requires looking atthe topology of the network, while experimentally assessingthe robustness of a network requires the costly experiment oflooking at all the possible inputs and testing all the possibleconfigurations of the network corresponding to different failuresituations, facing a discouraging combinatorial explosion. We distinguish the case of neurons that can fail and stop their activity (crashed neurons) from the case of neurons that can fail by transmitting arbitrary values (Byzantine neurons). In the crash case, our bound involves the number of neuronsper layer, the Lipschitz constant of the neural activationfunction, the number of failing neurons, the synaptic weightsand the depth of the layer where the failure occurred. In thecase of Byzantine failures, our bound involves, in addition, thesynaptic transmission capacity. Interestingly, as we show inthe paper, our bound can easily be extended to the case wheresynapses can fail. We present three applications of our results. The first is aquantification of the effect of memory cost reduction on theaccuracy of a neural network. The second is a quantification ofthe amount of information any neuron needs from its precedinglayer, enabling thereby a boosting scheme that prevents neuronsfrom waiting for unnecessary signals. Our third applicationis a quantification of the trade-off between neural networksrobustness and learning cost.","inCitations":["3c93f07a402dd865c86604628c3b019de2f484d9","18f2a39119a83297bfd2d86bc78fb25e5dcc736f","6b25106ad8f0a8167516921c3d3966c89f639d13","3bcba9b71275c764c95ccf6b202cfdec1b233dd7"],"pdfUrls":["https://infoscience.epfl.ch/record/217561/files/When_Neurons_Fail_1.pdf","http://infoscience.epfl.ch/record/217561/files/When_Neurons_Fail_1.pdf","http://infoscience.epfl.ch/record/217561/files/When_Neurons_Fail.pdf","https://arxiv.org/pdf/1706.08884v1.pdf","https://doi.org/10.1109/IPDPS.2017.66","http://arxiv.org/abs/1706.08884"],"title":"When Neurons Fail","doi":"10.1109/IPDPS.2017.66","sources":["DBLP"],"doiUrl":"https://doi.org/10.1109/IPDPS.2017.66","venue":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)"},
{"entities":["Data parallelism","Iteration","Iterator","Linear function (calculus)","Load balancing (computing)","Loop optimization","OpenMP","Parallel algorithm","Parallel computing","Polyhedron","Polynomial","Program optimization","Runtime system","Scheduling (computing)","X86"],"journalVolume":"","journalPages":"778-787","pmid":"","year":2017,"outCitations":["b7b7725104805e87b8656a8b456a9e1c5fd33f30","59b474e992ac78ee9e3df3c6abc282eaea996ce0","21b4e156771b53721dcc7311c9a820af9483161b","1c5b15587e4034c97610b2017697ad1ea663a8fa","56d002964786aea0fef64d6f1c81d96c22195070","533d60a68d64caf88db21e25a3f16c9a4d0e4d92","96b2efa9ea1ee6d071005c523ba69680eaf93da1","936295d41f89aa0532b3bbda268b924e3dc9dc13","33df2bddce39f19455280d9042894707b712b083","524242205bed90261f9a70fe7c122e9226ede249","0ecac51f093db517b11c0853fa761f2b36d17201","4635754a1c407c58ade574cb7dcba7968626e324","233cfa61d01fcf26effc64508250cb90396e8a78"],"s2Url":"https://semanticscholar.org/paper/e1bd4d45476525ff40268c8ef740dac3d9405098","s2PdfUrl":"","id":"e1bd4d45476525ff40268c8ef740dac3d9405098","authors":[{"name":"Philippe Clauss","ids":["1886895"]},{"name":"Ervin Altintas","ids":["19191072"]},{"name":"Matthieu Kuhn","ids":["40157972"]}],"journalName":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","paperAbstract":"Loop collapsing is a well-known loop transformation which combines some loops that are perfectly nested into one single loop. It allows to take advantage of the whole amount of parallelism exhibited by the collapsed loops, and provides a perfect load balancing of iterations among the parallel threads. However, in the current implementations of this loop optimization, as the ones of the OpenMP language, automatic loop collapsing is limited to loops with constant loop bounds that define rectangular iteration spaces, although load imbalance is a particularly crucial issue with non-rectangular loops. The OpenMP language addresses load balance mostly through dynamic runtime scheduling of the parallel threads. Nevertheless, this runtime schedule introduces some unavoidable executiontime overhead, while preventing to exploit the entire parallelism of all the parallel loops. In this paper, we propose a technique to automatically collapse any perfectly nested loops defining non-rectangular iteration spaces, whose bounds are linear functions of the loop iterators. Such spaces may be triangular, tetrahedral, trapezoidal, rhomboidal or parallelepiped. Our solution is based on original mathematical results addressing the inversion of a multi-variate polynomial that defines a ranking of the integer points contained in a convex polyhedron. We show on a set of non-rectangular loop nests that our technique allows to generate parallel OpenMP codes that outperform the original parallel loop nests, parallelized either by using options &#x201c;static&#x201d; or &#x201c;dynamic&#x201d; of the OpenMPschedule clause.","inCitations":["56b58e9434c95a68c9486f8e0ddfd13f364e79aa"],"pdfUrls":["https://doi.org/10.1109/IPDPS.2017.34"],"title":"Automatic Collapsing of Non-Rectangular Loops","doi":"10.1109/IPDPS.2017.34","sources":["DBLP"],"doiUrl":"https://doi.org/10.1109/IPDPS.2017.34","venue":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)"},
{"entities":["Block code","Library","Library (computing)","Message passing","Multithreading (computer architecture)","Parallel computing","Runtime system","Thread (computing)"],"journalVolume":"","journalPages":"469-478","pmid":"","year":2017,"outCitations":["178cf471f97b7acd72780028659648329423679e","346c5896ff2032d7c7a8400cbbd3bd2f61c72f1a","058e086bb1235376491470143244502f6678bbc5","4cbdbf2ca478894817ae18e0f4d007b298efd6d0","9e899a90a24787f391346810bfe1c72480f344d2","262c123c6325d7b4c2edc904d6e85d352ab266b9","149e6aa3e77eed60305ec51f8cda94e9706a38ca","7421d28428e041c271fe6370c331353f4a3fa974","602dcccc2bf6af1ca84355d530ff1e0a79391217","7067391b82722e3b8b0434dcf0a5ada48e76a209","069ec88e2d30784746ab2224bc096e494c745382","5efbddc09b301537d559b5876756b18df32a0e02","48aa3489d4290c0f3683771bd9dc5f23745b4b56","16b14944bdfa3c34da1367cb7882667c4d09cb99","6396e234e8e03ca2a7a6c7be78dfd2c8f775ac3a","5e50ffa96fa021c85ccedb1bb8b84b59ee268de8","b1afc99d24247003eb5f969374fff3bd2af71f27"],"s2Url":"https://semanticscholar.org/paper/3730e37138e719a161a22aa7b0f62af6edcf5e4e","s2PdfUrl":"","id":"3730e37138e719a161a22aa7b0f62af6edcf5e4e","authors":[{"name":"Samuel K. Gutierrez","ids":["2673238"]},{"name":"Kei Davis","ids":["7645272"]},{"name":"Dorian C. Arnold","ids":["1689138"]},{"name":"Randal S. Baker","ids":["37509486"]},{"name":"Robert W. Robey","ids":["2768905"]},{"name":"Patrick S. McCormick","ids":["34694816"]},{"name":"Daniel Holladay","ids":["15037751"]},{"name":"Jon A. Dahl","ids":["31418261"]},{"name":"R. Joe Zerr","ids":["19269703"]},{"name":"Florian Weik","ids":["12419611"]},{"name":"Christoph Junghans","ids":["1711228"]}],"journalName":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","paperAbstract":"Hybrid parallel program models that combine message passing and multithreading (MP+MT) are becoming more popular, extending the basic message passing (MP) model that uses single-threaded processes for both inter- and intra-node parallelism. A consequence is that coupled parallel applications increasingly comprise MP libraries together with MP+MT libraries with differing preferred degrees of threading, resulting in thread-level heterogeneity. Retroactively matching threading levels between independently developed and maintained libraries is difficult; the challenge is exacerbated because contemporary parallel job launchers provide only static resource binding policies over entire application executions. A standard approach for accommodating thread-level heterogeneity is to under-subscribe compute resources such that the library with the highest degree of threading per process has one processing element per thread. This results in libraries with fewer threads per process utilizing only a fraction of the available compute resources. We present and evaluate a novel approach for accommodating thread-level heterogeneity. Our approach enables full utilization of all available compute resources throughout an application's execution by providing programmable facilities to dynamically reconfigure runtime environments for compute phases with differing threading factors and memory affinities. We show that our approach can improve overall application performance by up to 5.8x in real-world production codes. Furthermore, the practicality and utility of our approach has been demonstrated by continuous production use for over one year, and by more recent incorporation into a number of production codes.","inCitations":["a2921d217af6dc2c39089e784a06a6ef2b2cda2d"],"pdfUrls":["https://doi.org/10.1109/IPDPS.2017.13"],"title":"Accommodating Thread-Level Heterogeneity in Coupled Parallel Applications","doi":"10.1109/IPDPS.2017.13","sources":["DBLP"],"doiUrl":"https://doi.org/10.1109/IPDPS.2017.13","venue":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)"},
{"entities":["Algorithm","Baseline (configuration management)","Benchmark (computing)","Best-effort delivery","Centralisation","Challenge-Handshake Authentication Protocol","Global network","Interconnection","Network on a chip","Parsec (parser)","Power gating","Router (computing)","Routing","Synthetic data"],"journalVolume":"","journalPages":"708-717","pmid":"","year":2017,"outCitations":["0d01b42384dd92c400052a05e3d24cebaecd4056","7631275e3266f627df6cc29441f69ab9f5f2b1c6","8d0581b30015b3d5afb56347362ffdfec6622346","c1818d4f005f0be1ee0eae940052bc3c05885923","45fa92ee8e5be638c78da79c214bfb6ec7ed5a97","0e98aeb6638084085dc40ed57d10d2d6a3ba94fe","0c89d5d37a25f693bc31e65c0c5b9bea63148e57","25f0cbae4ee4237012865bce97b9449e74fd6599","3c2916d16e119852a4da9476a4144a53cb83c47f","56bf58eb183dbe8f6d420fae194f2c2be35fc850","41236387e01eacb63cefad6318dc48fc60e9829e","0d60537e54e10cd0fdf678532b4a41c86b0a485c","23b564bfb4e3f84e9676247f90781d04cd8b6c71","063700ef01aad15a1981553fde02e8d162a553e7","08408ab6b2000662e63d431b424ce31c6e09fb70","373b88e34295875fdab7f6cdee1438edbd0571cb","960904bf8dd1de618d606a95fbca8d345d1e769a","18e98cd806cc79fb2869d3819ba1469e6515a22d","2266e34950bfcbc2af57618748ab4d7bcead8ad9","03e53dddc865bf688fe313a94ad186a4d96bffe0","329756f2d29829e1b2e713360016995855d0ea26","d311c8941818f400df3ca27381938e8a3d066051","3c1a32c467c628cf72fea9f7d7720e1fccd13b46","3ec18371eed24707fb16bf7cc258f3043088207f","3e74ae88cdaa33bf89136800258bde97ab397ec9","55deba3bb943391e07d571d89c049b12b76a0c75","167481d1f156698a2bb177049f67131e818eec61","36ce17dd3f7734b7578fd6580c886b3d1b2a475b","d589123c9665f52c1c06a0b3c80aa94c423a8908","f246db6a2eaedf8f3eb84af60c29703ae9aef504","423437335211cfd0fd61aad08b822ff349ecef3d","30c5b89ef93b564781b9a7b8f03be0056d926876","2c2e32267c43161f80241a2e1ba21d1f0f871dd4"],"s2Url":"https://semanticscholar.org/paper/2c7ead91dc8a1acdef1047ef4688bcd06b90bb6a","s2PdfUrl":"","id":"2c7ead91dc8a1acdef1047ef4688bcd06b90bb6a","authors":[{"name":"Rahul Boyapati","ids":["2949392"]},{"name":"Jiayi Huang","ids":["3406930"]},{"name":"Ningyuan Wang","ids":["3450932"]},{"name":"Kyung Hoon Kim","ids":["2224877"]},{"name":"Ki Hwan Yum","ids":["1680392"]},{"name":"Eun Jung Kim","ids":["1692009"]}],"journalName":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","paperAbstract":"Scalable Networks-on-Chip (NoCs) have become the de facto interconnection mechanism in large scale Chip Multiprocessors. Not only are NoCs devouring a large fraction of the on-chip power budget but static NoC power consumption is becoming the dominant component as technology scales down. Hence reducing static NoC power consumption is critical for energy-efficient computing. Previous research has proposed to power-gate routers attached to inactive cores so as to save static power, but requires centralized control and global network knowledge. In this paper, we propose Fly-Over (FLOV), a light-weight distributed mechanism for power-gating routers, which encompasses FLOV router architecture, handshake protocols, and a partition-based dynamic routing algorithm to maintain network functionalities. With simple modifications to the baseline router architecture, FLOV can facilitate FLOV links over power-gated routers. Then we present two handshake protocols for FLOV routers, restricted FLOV that can power-gate routers under restricted conditions and generalized FLOV with more power saving capability. The proposed routing algorithm provides best-effort minimal path routing without the necessity for global network information. We evaluate our schemes using synthetic workloads as well as real workloads from PARSEC 2.1 benchmark suite. Our full system evaluations show that FLOV reduces the total and static energy consumption by 18% and 22% respectively, on average across several benchmarks, compared to state-of-the-art NoC power-gating mechanism while keeping the performance degradation minimal.","inCitations":[],"pdfUrls":["http://faculty.cs.tamu.edu/ejkim/HPC_WEB/docs/ipdps17_flov.pdf","http://engineering.tamu.edu/media/2551734/2015-7-1.pdf","http://engineering.tamu.edu/media/3427396/2016_3_1.pdf","http://faculty.cs.tamu.edu/ejkim/HPC_WEB/docs/poster_pact.pdf","https://doi.org/10.1109/IPDPS.2017.77","https://engineering.tamu.edu/media/3427396/2016_3_1.pdf","https://engineering.tamu.edu/media/2551734/2015-7-1.pdf"],"title":"Fly-Over: A Light-Weight Distributed Power-Gating Mechanism for Energy-Efficient Networks-on-Chip","doi":"10.1109/IPDPS.2017.77","sources":["DBLP"],"doiUrl":"https://doi.org/10.1109/IPDPS.2017.77","venue":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)"},
{"entities":["Algorithm","Automaton","Central processing unit","Computation","Computer vision","Database","Field-programmable gate array","Graphics processing unit","Internet bottleneck","K-nearest neighbors algorithm","Machine learning","Natural language processing","Program optimization","Recommender system","Robotics","Similarity search","Speedup","Throughput"],"journalVolume":"abs/1608.03175","journalPages":"","pmid":"","year":2016,"outCitations":["d4e445281bd0cffb900a7605872849ac2a2a5e31","2731046193a1d034ca5544e9cc642957c2d6310d","268cd46a06e8e3052bbd64e96fac73d600430281","19d2b786fec5ded4d6cdca0e21f3c3f5264ecadf","1b20afbd2d2a349737ed3dc246e44bbdba203190","396514fb219879a4a18762cddfae2a6a607f439f","cd800900fbd9bc89a35ff597a7ddfe74aceefa55","06902cb95ede2c305db4000852014f276b25c082","630eb0c8cf211e95afc1696a2c627abe9e779bb3","a8ab0fc2f42476d2f76629747ecb981a438ea8ed","06c00c5de872edbb24ba5d67992cfbc912ffa7fb","1fcfbc935db4d3297dc69e96d5b6741b7d151a2b","086d4ffac8b60821aa05fd14cae101e32eb1e462","149ad380837451a3903dafbb13f6de3815547852","bf70d60fc8d1de5fa53e8220a014fe463de4b7e5","01e6176d319e3bfecc3667447c5bbaf2d00b9c7c","179f80848143cf109fa6aebae6c3844da03b062c","a888f137b7d821497ad3a1264ae28a93852c0d75","93c25da1b96dba6a83defeb05ebd5bd3c66feb87","56ef240a30a228ea6a6885d09dd3c60d2b021788","2871f115e7a11c903258491c75d4171fac679344","3f1e54ed3bd801766e1897d53a9fc962524dd3c2","9952d4d5717afd4a27157ed8b98b0ee3dcb70d6c","4f86a09f1c3778203807c60f968605b139efe8d3","10b014e882764f5800ecdcbaba1fa08795d0c54d","99d80987446ecc7fb546826e7bccebb2fdc5fa12","8d0bb67313c489aa90116c0c7df367a6ce46616d","0bd156b327f14b915a44848b1a0267fe9c30198c","6bccf2ba321177023d0f1d83484ae81fba687d97","10911d4e163f7eeca5b53786814a01694643267a","12d0c11d546d91e776a170898ebf3a38c010695c","3ad277770454b1f53a1ae8109c35b1b59a22d33f","10e8ebc9a2397336cd03dda18842ad6e7e7299bb","3f3a44e1ef5acb51d6c53099fd296aa7d40355e0","1b68aa68c70af87fc3b712ff7a4a9aa289bf23bf","4cd6b5e470b4205cd1560de42cde8108fa42ba4b","8deafe947207eab416d8791f2e750289bd9ac73b","d4f2f0b971984fa5235ccd76a8bb1441a736bfa5","3ac6671a0c61544b9dab543b116eccdaccc6469e","2d680892a7318ab7eff879054ea7ab6aeeb51fe9","1fbbc34d163b42a6cfd14eaff9556359c072e210"],"s2Url":"https://semanticscholar.org/paper/42318f40185f0bcddf60566b8586dacb557ab0cf","s2PdfUrl":"http://pdfs.semanticscholar.org/bf4c/99cbc36a4f09ae7d7148c0dc03ae48ea8327.pdf","id":"42318f40185f0bcddf60566b8586dacb557ab0cf","authors":[{"name":"Vincent T. Lee","ids":["31901851"]},{"name":"Justin Kotalik","ids":["3451487"]},{"name":"Carlo C. del Mundo","ids":["2896556"]},{"name":"Armin Alaghi","ids":["1698528"]},{"name":"Luis Ceze","ids":["1717411"]},{"name":"Mark Oskin","ids":["1723213"]}],"journalName":"CoRR","paperAbstract":"Similarity search is a critical primitive for a wide variety of applications including natural language processing, content-based search, machine learning, computer vision, databases, robotics, and recommendation systems. At its core, similarity search is implemented using the k-nearest neighbors (kNN) algorithm, where computation consists of highly parallel distance calculations and a global top-k sort. In contemporary von-Neumann architectures, kNN is bottlenecked by data movement which limits throughput and latency. In this paper, we present and evaluate a novel automata-based algorithm for kNN on the Micron Automata Processor (AP), which is a nonvon Neumann near-data processing architecture. By employing near-data processing, the AP minimizes the data movement bottleneck and is able to achieve better performance. Unlike prior work in the automata processing space, our work combines temporal encodings with automata design to augment the space of applications for the AP. We evaluate our design’s performance on the AP and compare to state-of-the-art CPU, GPU, and FPGA implementations; we show that the current generation of AP hardware can achieve over 50× speedup over CPUs while maintaining competitive energy efficiency gains. We also propose several automata optimization techniques and simple architectural extensions that highlight the potential of the AP hardware. Keywords-k-nearest neighbors, similarity search, automata processors, near-data processing","inCitations":[],"pdfUrls":["https://arxiv.org/pdf/1608.03175v1.pdf","http://arxiv.org/abs/1608.03175","https://arxiv.org/pdf/1608.03175v2.pdf","http://arxiv.org/pdf/1608.03175v1.pdf"],"title":"Near Memory Similarity Search on Automata Processors","doi":"","sources":["DBLP"],"doiUrl":"","venue":"ArXiv"},
{"entities":["Algorithm","Collaborative product development","Computation","Computer security","Data-intensive computing","Dynamic random-access memory","E-commerce","High memory","Knights","Load balancing (computing)","MCDRAM","Manycore processor","Memory bandwidth","Multi-core processor","Sparse matrix","Speedup"],"journalVolume":"","journalPages":"1058-1067","pmid":"","year":2017,"outCitations":["47a6a274c648aeb5ff02eb09aff7ea310eae122e","86159c2269566286a5e8f724deab749c9e2750b1","08368dae4f102176b8e50a64ddde8a8150cde26e","53ced1bf79a5ff0db025fee76a67a33f37e4286d","41c4ccf14aa43d5694d69788894cbaa17f91f6ca","1d0f25989452abbbc8feaf00a034ff110fc4b350","38b389580d774ce513284e671ff3bbcef0258de2","7d50b6883c38e34016a4841ec4ab2b92bfdfe3ad","28552ecf4eaedb3461edca97304b29082b02fbab","2e8ab628bc9f256c11c898aa44f049143c74d05d","a9653a27052d666b7ed47524871dc9c3a9b92cc4","31af4b8793e93fd35e89569ccd663ae8777f0072","255aeb5c2a8eea15db08c617481ddbb35a41bfe4","008a6e4b2763736d2c6363ee6b546b09c0022e53","ac0a0828c17c040c065a9285264094ba2560497d","2d03baec8ac1568e6813aa43d625d552524f977e","53a225f2843e8544ca9c615ecfcc5fad26083e49","00dbf46a7a4ba6222ac5d44c1a8c09f261e5693c","231f97057e1efed073c20ccdf3aa3c5aaf063ffb","00ca166ea4521f5cc3d23e74a1b1090386b6831f","5a3c8589d63fcee5dd40ef43aea6ef38e2fda9a8","d6c4c76076efecb15655274adc648af8a445ed3a","2d8d293baed5060034326781b261ca5f6464be11"],"s2Url":"https://semanticscholar.org/paper/1de6ac748387859f43bc15e15ff5380df05bae34","s2PdfUrl":"","id":"1de6ac748387859f43bc15e15ff5380df05bae34","authors":[{"name":"Shaden Smith","ids":["34854131"]},{"name":"Jongsoo Park","ids":["1686843"]},{"name":"George Karypis","ids":["1681616"]}],"journalName":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","paperAbstract":"HPC systems are increasingly used for data intensive computations which exhibit irregular memory accesses, non-uniform work distributions, large memory footprints, and high memory bandwidth demands. To address these challenging demands, HPC systems are turning to many-core architectures that feature a large number of energy-efficient cores backed by high-bandwidth memory. These features are exemplified in Intel's recent Knights Landing many-core processor (KNL), which typically has 68 cores and 16GB of on-package multi-channel DRAM (MCDRAM). This work investigates how the novel architectural features offered by KNL can be used in the context of decomposing sparse, unstructured tensors using the canonical polyadic decomposition (CPD). The CPD is used extensively to analyze large multi-way datasets arising in various areas including precision healthcare, cybersecurity, and e-commerce. Towards this end, we (i) develop problem decompositions for the CPD which are amenable to hundreds of concurrent threads while maintaining load balance and low synchronization costs; and (ii) explore the utilization of architectural features such as MCDRAM. Using one KNL processor, our algorithm achieves up to 1.8x speedup over a dual socket Intel Xeon system with 44 cores.","inCitations":["7c3c5b282948121244d330651e36b05f31c382cb","dab53f03682789b483822bc521204bfb39ee2458","235d090c8549ff3b353103380313d70e33c47e4e","232d892b423c24aaefcec9eb2ae211316be0f025"],"pdfUrls":["https://doi.org/10.1109/IPDPS.2017.84","http://shaden.io/pdf/2017-Smith-KNL-slides.pdf","http://shaden.io/pdf/2017-Smith-KNL.pdf","https://www.cs.umn.edu/sites/cs.umn.edu/files/tech_reports/14-006_0.pdf","http://glaros.dtc.umn.edu/gkhome/fetch/papers/shaden2017ipdps.pdf"],"title":"Sparse Tensor Factorization on Many-Core Processors with High-Bandwidth Memory","doi":"10.1109/IPDPS.2017.84","sources":["DBLP"],"doiUrl":"https://doi.org/10.1109/IPDPS.2017.84","venue":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)"},
{"entities":["Algorithm","Fail-stop","Failure rate","Overhead (computing)","Scheduling (computing)","Work stealing"],"journalVolume":"","journalPages":"397-408","pmid":"","year":2017,"outCitations":["a8171f306b021d52d9af9f6f474748ef75492210","5c0e8af36e20b8ea213561e8c3d706b4e2f2cc8d","dc12b7531e42ffed5ce81398388cf04cb4ca539c","029d525dd48347fa4b8a48dbf4b41b4b37199a6a","455d253c61379bce5626fba8ef9897d3ac1307dc","03fb875d5022a5e98f19c271e2403232acc55318","345ff2f19178c983f2742b1f3198fa045cca2121","01d62cd850496455ce1616500f491690effa5c98","8a317f50aadff9853a9796a2235d11b1471af7f3","34f310dffd51a8f1585b0a6a5ccaf83094d0d663","0f6a32792d0882db35fe9391445d4322232b619e","81b7a5dc0e03d1f1a0e01fde3af8684f610db591","41cb3498cc12fbf2145e023e43f4d0b220e6a2f9","ee8ee09169e9fb633c86e6ec3ff4c9bd4585eb66","0fb659af82f2277c8a62ac888f4bfd01570e5470","0ab6d7acaf684285fc256fff262f3e185b266c6c","504986a7de84bc763bcce07c70ec7527b9ffe494","700534b13da5b314e9735ad5fb3ae5fe543fa2f3","2a88cb605d1fbc7dfa15aae9041c69bf03be85a1","0df37799cedef8c3625cc554aee51e65cbcedd51","238c566910b81c09e18b5b6812d067420500d9d9","1a661e3daa3b5f1e04df994ca3afcf7a258bdaa6","6e1ceacdaa0c979cf52af5f478cc2a9891e2b6a0","292919bc42727aea7d8e22671b7d624874022854","a0b51ccf50b61d58047aa73e3fb33df8bde410e9","19df5d05b6da98f99619ee4584c5177bd02c8a2a"],"s2Url":"https://semanticscholar.org/paper/7e3fdbbad04a39b2f44436287668f1a682e26ab0","s2PdfUrl":"","id":"7e3fdbbad04a39b2f44436287668f1a682e26ab0","authors":[{"name":"Gokcen Kestor","ids":["1746771"]},{"name":"Sriram Krishnamoorthy","ids":["1679176"]},{"name":"Wenjing Ma","ids":["33955807"]}],"journalName":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","paperAbstract":"Nested fork-join programs scheduled using work stealing can automatically balance load and adapt to changes in the execution environment. In this paper, we design an approach to efficiently recover from faults encountered by these programs. Specifically, we focus on localized recovery of the task space in the presence of fail-stop failures. We present an approach to efficiently track, under work stealing, the relationships between the work executed by various threads. This information is used to identify and schedule the tasks to be re-executed without interfering with normal task execution. The algorithm precisely computes the work lost, incurs minimal re-execution overhead, and can recover from an arbitrary number of failures. Experimental evaluation demonstrates low overheads in the absence of failures, recovery overheads on the same order as the lost work, and much lower recovery costs than alternative strategies.","inCitations":["1130a13b74e11b99d5233dce7f157d54cfea4ed1","b3811891b083090da650459bcee017594d794389","36f83f372ec018a79ad563e5a78f3cf7f4bad292"],"pdfUrls":["https://doi.org/10.1109/IPDPS.2017.75"],"title":"Localized Fault Recovery for Nested Fork-Join Programs","doi":"10.1109/IPDPS.2017.75","sources":["DBLP"],"doiUrl":"https://doi.org/10.1109/IPDPS.2017.75","venue":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)"},
{"entities":["Algorithm","Graphics processing unit","Java Caps","Jumpstart Our Business Startups Act","Scheduling (computing)","Throughput"],"journalVolume":"","journalPages":"967-977","pmid":"","year":2017,"outCitations":["2ae2d80ffb19521bcd7fdbf26e9ed2a5d9641bb0","cdf26f11e3280497b02d168aaa6edd7b820e7800","7f4ae7c055c638a07ab1e4d5030be9007fcca3a9","96def001f76a9254345ccc7ced9d1b81bbeed1b1","34d67f1f2578013ec828f2840e28bc3961d8a98c","aaa59e57b2f96ead6a61592e968c0b9907c6d128","8823b1c6aee66ed0a36240a884a8a71578c5eacb","60f068dea641df784a379411c57aa8f2b23d1a98","c111caed42f318d59e3d30bfd875bcee8581652b","02b141ddc423469afde9c99cf76028095ef28127","45de7137919e20513db715056da4d853caec603a","83bbec4d4f56b5631f48607b7b6c75a505a8b448","f016d23ffca72cdf1eb584613452720eaacafd9c","2f4b9ac4a0694f0b1681348334befba0bfe9d897","8bd32accc1244ba9add521ca5605f397374de518","3462fb38042f0bde20c758728d7c8c28a1f47e09","4b30df44b074595ee73b67ea901e44c7c440ef73","13cf7f6f047ab5bed7eb419a604dd1668f0ee9c1","3ee47780011ee618bd5a64624a662375e1958e0a","66f47514ca03fd4a52121836bdb767bb5fac95cc","8db3c11cd85195f459b8ba82fe3326e8f86f1d52","441f99e6b0e40ce36716252a0b0a1c97690170d1","832573e996403371beb2403821df33d8082c1121","0e935b1f654b5e3fa12f7dc6277bdc7f2ca36c9a","96d2e5456b8d7b8ad763781a16b61beabf2d7fcf","1108af609469e420aeae551ba8a893c3200e07fa","839e7a4fdc95da41c34eafb4f5b0517ba8c759a6","b04c9e851ae605592d693aa65f0d753b8af08feb","14505c2bdd3822d7a62385121d28ba3eb36fea1d","02308b39a7a53b6e313a7acbd1f1896292cebe01"],"s2Url":"https://semanticscholar.org/paper/88646d19e0111eed813e5cd326889556b0379d29","s2PdfUrl":"","id":"88646d19e0111eed813e5cd326889556b0379d29","authors":[{"name":"Qi Zhu","ids":["1711920"]},{"name":"Bo Wu","ids":["40485705"]},{"name":"Xipeng Shen","ids":["37914192"]},{"name":"Li Shen","ids":["40391089"]},{"name":"Zhiying Wang","ids":["1690770"]}],"journalName":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","paperAbstract":"This paper presents the first systematic study on co-scheduling independent jobs on integrated CPU-GPU systems with power caps considered. It reveals the performance degradations caused by the co-run contentions at the levels of both memory and power. It then examines the problem of using job co-scheduling to alleviate the degradations in this less understood scenario. It offers several algorithms and a lightweight co-run performance and power predictive model for computing the performance bounds of the optimal co-schedules and finding appropriate schedules. Results show that the method can efficiently find co-schedules that significantly improve the system throughput (9-46% on average over the default schedules).","inCitations":[],"pdfUrls":["https://doi.org/10.1109/IPDPS.2017.124","https://people.engr.ncsu.edu/xshen5/Publications/ipdps17.pdf"],"title":"Co-Run Scheduling with Power Cap on Integrated CPU-GPU Systems","doi":"10.1109/IPDPS.2017.124","sources":["DBLP"],"doiUrl":"https://doi.org/10.1109/IPDPS.2017.124","venue":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)"},
{"entities":["Distributed computing","Load balancing (computing)","Power of two","Randomized algorithm","Server (computing)","Simulation","Time complexity"],"journalVolume":"","journalPages":"1068-1077","pmid":"","year":2017,"outCitations":["529462c0d6d8aab16b4cd76317439d1545f8b7fa","73750f2ef08f6943495f8d7d3de7a799908901bd","20480eb94103d5ed4b5925a86430c872e4fabfdc","34d537178cf343a1b1036616938659b4ba76cd46","0f72abea4de4d3b1a47bfa114163b318c9507827","00b26163a1b2c0ddd49766ef4614ec4024f732ab","2fe57efa9d15d4db871173e29032c1687ef33292","8cf989ebadf69674c8ddc56f645ad9e61e7ad431","4cead48e2eac91560105871b78268e3164eb382b","075321f87a446af752efa991acc30819377c7788","3f065d08c925153314c77015b99aa843859f4cfe","346965e005bbffdfb02d6536b057859dc51c879b","6e044cdfbe2e8e3bbfa67e74e55d69c974aa6e97","406d6f7a2a21d3b31a32c2bf306132def7163e06","337e4b7f57ccbb7485950b93da9c5bb4ec4dc9ad","4ef750a115b108f233623e6da2ab17cd266d8c59","3d601571fdf082b3f2bbf16befbeaa6702664dff","a3578399c4c73a17d93e4f2bbf190d65a669881e","66e4fd683171f38fcf8719c4bf19b32f1a691698","0bd1b088142f413345b0f2f2020a792811a76d71","284495671bf2b16fca3f8784586e3d248ea10801","2209890b596593c697dc8e1ab02517fbbaea21bf","638c917d981915bc7a00bb0941cdd38111df51de","15871f885a879ae12668a1ce0d95cc54dc53c8d3","320778bd3e5c1e9b6e0438149964d56f3900730f","4383810bfb6043a0cc4085f412db320cd095a925","234e6be0d4238f76b3ac038ee422be39f391c625","0056edb1bd74d150fdd385f74f3e6dc0534cc678","1fc083dac294f88f912ee22645f3ddf0db2c73c7","247beb4be0164c97878d2e6336d3922e2d595bbc","a8bdf1665b828e13dd8bcc2c73bf94863685f964","0160729ec657235f10cfe76dc892c08ae4d0f2e7","401fa67d01d8725ee301c9a2464dead1879a4c53"],"s2Url":"https://semanticscholar.org/paper/42dba360569d6a8b6eaaf7922dcac85e01cb5924","s2PdfUrl":"","id":"42dba360569d6a8b6eaaf7922dcac85e01cb5924","authors":[{"name":"Ali Pourmiri","ids":["1844903"]},{"name":"Mahdi Jafari Siavoshani","ids":["1743607"]},{"name":"Seyed Pooya Shariatpanahi","ids":["1708300"]}],"journalName":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","paperAbstract":"We consider load balancing in a network of caching servers delivering contents to end users. Randomized load balancing via the so-called power of two choices is a well-known approach in parallel and distributed systems that reduces network imbalance. In this paper, we propose a randomized load balancing scheme which simultaneously considers cache size limitation and proximity in the server redirection process. Since the memory limitation and the proximity constraint cause correlation in the server selection process, we may not benefit from the power of two choices in general. However, we prove that in certain regimes, in terms of memory limitation and proximity constraint, our scheme results in the maximum load of order &#x398;(log log n) (here n is the number of servers and requests), and at the same time, leads to a low communication cost. This is an exponential improvement in the maximum load compared to the scheme which assigns each request to the nearest available replica. Finally, we investigate our scheme performance by extensive simulations.","inCitations":["054155d089205a85a3d0acb3a713ad5092095932","f71c309987c6f6c9d7452a8a3dc817dbcb60f53d"],"pdfUrls":["https://doi.org/10.1109/IPDPS.2017.24","https://arxiv.org/pdf/1610.05961v1.pdf","http://arxiv.org/abs/1610.05961","https://arxiv.org/pdf/1610.05961v2.pdf"],"title":"Proximity-Aware Balanced Allocations in Cache Networks","doi":"10.1109/IPDPS.2017.24","sources":["DBLP"],"doiUrl":"https://doi.org/10.1109/IPDPS.2017.24","venue":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)"},
{"entities":["Algorithm","Average path length","Data center","Fault tolerance","High availability","Performance per watt","Routing","Server (computing)","Simulation","Throughput"],"journalVolume":"","journalPages":"718-727","pmid":"","year":2017,"outCitations":["57c1cb35aeff251a4ebc3118598ff4e60cca96c0","46e3fb8bacbb39a7db86c30a8adda227496f0283","19ff9dac013d1ebca1ea1c9845325c9ddafdf93a","92c873750a26240e416f315442c0efd257f0cd5d","ed2db031b9a02147777b7d3ad72da2e0cae19e10","640af017aa8d11f9f31480155c8d5d1a0d8865d7","1d912b67ba7cda4d341d834c1c6de96db01888fc","c068615b084aceef73e11628486ac2bfdab5fc26","2629b70da4e456b54e96f1d1d2703bd518239415","0f8b04cb89e455ceadf0c88fd5dd9f9a7f338ba9","61aa0f67a22fb72abc41241c358e5f0e2b565def","84b24b6d01c1b9c624d2e31fd839994353e6a243","70cfadda2de05949e8908e0ba35aa18f29928cd4","b72bf8d50a699f1edccc68bf8e7d80921d88b7a2","11a4744f7f0883c4232a9f5aaca8b9d29bfaa762","0541d5338adc48276b3b8cd3a141d799e2d40150","402405280ef60ffc35b3ce6c0845805b787bbb87","5fa66ec92e8b0a1a33fae62ddbdab154b938a992","663e064469ad91e6bda345d216504b4c868f537b","138856ad6b8b4cca92965aacb20961aaa4e34a92","3365897178130e985acf671d6564f5dd6fa0ea1c","235da9c0f828b60300f7e5cfa2ca6aaa116dd14c","70afebbe36ffacc827bf995257f01cad7a1160bc"],"s2Url":"https://semanticscholar.org/paper/1339ff7f37047f7e8517295b8542d99788e16497","s2PdfUrl":"","id":"1339ff7f37047f7e8517295b8542d99788e16497","authors":[{"name":"Zhenhua Li","ids":["3543872"]},{"name":"Yuanyuan Yang","ids":["33830008"]}],"journalName":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","paperAbstract":"Designing a cost-effective network for data centers that can deliver sufficient bandwidth and provide high availability has drawn tremendous attentions recently. In this paper, we propose a novel server-centric network structure called RCube, which is energy efficient and can deploy a redundancy scheme to improve the availability of data centers. Moreover, RCube shares many good properties with BCube, a well known server-centric network structure, yet its network size can be adjusted more conveniently. We also present a routing algorithm to find paths in RCube and an algorithm to build multiple parallel paths between any pair of source and destination servers. In addition, we theoretically analyze the power efficiency of the network and availability of RCube under server failure. Our comprehensive simulations demonstrate that RCube provides higher availability and flexibility to make trade-off among many factors, such as power consumption and aggregate throughput, than BCube, while delivering similar performance to BCube in many critical metrics, such as average path length, path distribution and graceful degradation, which makes RCube a very promising empirical structure for an enterprise data center network product.","inCitations":[],"pdfUrls":["https://doi.org/10.1109/IPDPS.2017.50"],"title":"RCube: A Power Efficient and Highly Available Network for Data Centers","doi":"10.1109/IPDPS.2017.50","sources":["DBLP"],"doiUrl":"https://doi.org/10.1109/IPDPS.2017.50","venue":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)"},
{"entities":["CUDA","Code refactoring","Computation","Field-programmable gate array","Graphics","Graphics processing unit","Naivety","OpenACC","OpenCL API","OpenMP","Pipeline (computing)","Scheduling (computing)","Speedup","Supercomputer"],"journalVolume":"","journalPages":"575-584","pmid":"","year":2017,"outCitations":["5f3cce1bc739ebfc03e003010d3438bb318efc14","e423ed8b4824b6b63ac558ebb1332ccb5954a7ba","2373aa844bdc6c2cf3e07e291f3090b7f5b941ad","034b5cb0eb2506096ae6f30790834b4af0da9158","c251fc6c99d8b515f3f0844604a21af92cce647f","145ba8f0070a5e2ad061b358a66762ce1765c241","47f05344d0d5fd252ebf645dddb8a1c5118cffc6","679541d90bcfb71019c7407b4c408a80e88db99d","3000c714a7d5afad6d3b498f3505685890fade46","31db730ff99fb6d7d32569473b8136e81a7b1d7f","ee1011d3de50c376361468daf6e40d567a18b3e5","84993fad8fabe361cf5d69e05ac5045cf6ac99ae","4b434f94fafc3ffc76e0c440897ccd222eaa38ac","4dbdfcf09af83ffb0a46989b981357540113f0fa","6348bb3b140c47ea29621d1dc5218db52433840b","262c123c6325d7b4c2edc904d6e85d352ab266b9","2bb29fce377e1ec9024ea7c45fd40fa178922602","ddaeb6c5b5fe0154e3b8f7b4707825f900512993","44673760887d14a7e8516f8b487a662785ee1480","e44585b020c93b6755fd9637d235d08b72d8fb7c","ba1d069dd686d0fb44a56778652ca5b7259d5ae5","4721ad0db596f3f78ddb31b4305ddbde35f8f181","5d70bd2207d2d28c9c7c284a8ac3ca5b7a6b016c","b7e3dec8a041102c6febb9a9ba46d45da8f2e787","03daf2d17337f000538d9d4727fa49d52bdb922c","2868d51b26c3e2a7dd237af7c6809388db76ed4f","a4bb437b1452d4a2b513c288bacee071d9050c88","7fed9ac269bc998483fefd5480b7a3c209b890f2","f2560793c90159397adbbed07da439f180289fbd"],"s2Url":"https://semanticscholar.org/paper/26c08f006ffad89e2d76f138373ce41942d7fd98","s2PdfUrl":"","id":"26c08f006ffad89e2d76f138373ce41942d7fd98","authors":[{"name":"Xuewen Cui","ids":["2212753"]},{"name":"Thomas Scogland","ids":["2125407"]},{"name":"Bronis R. de Supinski","ids":["1687845"]},{"name":"Wu-chun Feng","ids":["1688860"]}],"journalName":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","paperAbstract":"The community needs simpler mechanisms to access the performance available in accelerators, such as GPUs, FPGAs, and APUs, due to their increasing use in state-of-the-art supercomputers. Programming models like CUDA, OpenMP, OpenACC and OpenCL can efficiently offload compute-intensive workloads to these devices. By default these models naively offload computation without overlapping it with communication (copying data to or from the device). Achieving performance can require extensive refactoring and hand-tuning to apply optimizations such as pipelining. Further, users must manually partition the dataset whenever its size is larger than device memory, which can be especially difficult when the device memory size is not exposed to the user. We propose a directive-based partitioning and pipelining extension for accelerators appropriate for either OpenMP or OpenACC. Its interface supports overlap of data transfers and kernel computation without explicit user splitting of data. It can map data to a pre-allocated device buffer and automate memory-constrained array indexing and sub-task scheduling. We evaluate a prototype implementation with four different applications. The experimental results show that our approach can reduce memory usage by 52% to 97% while delivering a 1.41&#xd7; to 1.65&#xd7; speedup over the naive offload model.","inCitations":["3b603228bf9419868e7518614c85338b7a132989"],"pdfUrls":["https://doi.org/10.1109/IPDPS.2017.96","http://synergy.cs.vt.edu/pubs/papers/cui-pipelining-ipdps17.pdf"],"title":"Directive-Based Partitioning and Pipelining for Graphics Processing Units","doi":"10.1109/IPDPS.2017.96","sources":["DBLP"],"doiUrl":"https://doi.org/10.1109/IPDPS.2017.96","venue":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)"},
{"entities":["2.5D","Adversary (cryptography)","Algorithm","Best, worst and average case","Failure rate","Online algorithm","Radio jamming","Scheduling (computing)","Throughput"],"journalVolume":"","journalPages":"347-356","pmid":"","year":2017,"outCitations":["1c852777f2ecc9763eca2c376347ebcc69aedbdd","d0c82a11f7ce32a060ffbb2cf2f30a207a2b7d63","f345ecb8e361107bd6c8ca9b8f712c42a102c4cd","760de5ae1fa7d8ca69db08cbd504982eda94dabc","2dd47a99081828629dfe38bfeb36311a54f1ab1b","a56835a713cec9f3c18cd45d38faa1309145b00b","31dccb2338fc1d2b38463f32b485ab7ee5ed5d9c","22a9148898411fbe6b55ad852c0f01df26bf85a9","13d6d06c7ba1274188ed59de6dec7f590115a088","43a2d43972183f86696e073d5e55f228a7d717e8","053393e06597c1cc81bc47b8414b608e853a6559","678d52cb631cfb479179c8a01a0270178717b904"],"s2Url":"https://semanticscholar.org/paper/37676b91fdf1e4b7ff5edeb230e3dce67033d717","s2PdfUrl":"","id":"37676b91fdf1e4b7ff5edeb230e3dce67033d717","authors":[{"name":"Pawel Garncarek","ids":["39611936"]},{"name":"Tomasz Jurdzinski","ids":["1681361"]},{"name":"Krzysztof Lorys","ids":["2878308"]}],"journalName":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","paperAbstract":"We consider the problem of scheduling packets of different lengths via k directed parallel communication links. The links are prone to simultaneous errors --- if an error occurs, all links are affected. Dynamic packet arrivals and errors are modelled by a worst-case adversary. The goal is to optimize competitive throughput of online scheduling algorithms. Two types of failures are considered: jamming, when currently scheduled packets are simply not delivered, and crashes, when additionally the channel scheduler crashes losing its current state. For the former, milder type of failures, we prove an upper bound on competitive throughput of 3/4 - 1/(4k) for odd values of k, and 3/4 - 1/(4k+4) for even values of k. On constructive side, we design an online algorithm that, for packets of two different lengths, matches the upper bound on competitive throughput. To compare, scheduling on independent channels, that is, when adversary could cause errors on each channel independently, reaches throughput of 1/2. This shows that scheduling under simultaneous jamming is provably more efficient than scheduling under channel-independent jamming. In the setting with crash failures we prove a general upper bound for competitive throughput of (&#x221a;5-1)/2 and design an algorithm achieving it for packets of two different lengths. This result has two interesting implications. First, simultaneous crashes are significantly stronger than simultaneous jamming. Second, due to the above mentioned upper bound of 1/2 on throughput under channel-independenterrors, scheduling under simultaneous crashes is significantly stronger than channel-independent crashes, similarly as in the case of jamming errors.","inCitations":["a28c00aea28c7a0d6fe484fc1b8a9da56d094531"],"pdfUrls":["https://doi.org/10.1109/IPDPS.2017.105"],"title":"Fault-Tolerant Online Packet Scheduling on Parallel Channels","doi":"10.1109/IPDPS.2017.105","sources":["DBLP"],"doiUrl":"https://doi.org/10.1109/IPDPS.2017.105","venue":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)"},
{"entities":["Algorithm","Broadcast automation","Central processing unit","Embodied energy","For loop","Graphics processing unit","Heterogeneous computing","Heuristic","Load balancing (computing)","OpenMP","Parallel computing","Parallel language","Runtime system"],"journalVolume":"","journalPages":"788-798","pmid":"","year":2017,"outCitations":["230b075c5726dfdbc03f445883befef1170a9858","04717b51d7c7dd598d732fb8a293d69d45c0fa9f","a5895946af933fc3fb32f7e975a35ded0b63d619","679541d90bcfb71019c7407b4c408a80e88db99d","4958354ca96a04beee8eacc909fd2eea5cf788b4","ddd6bf11cd8564f413b9f5dd9c9a295c5076249f","0652168cd1dfe44892ef6c42004b5dec267ac254","03028a78daf97a01a26975a72c59c8d97cb18810","30930f09fd054056f60bc4626ebca47beb2dc3ef","66f0c57db9a5fe49645e3ed6eadafd649b004790","10c8ae128003be22c29e3afac6dfbc701323ce72","55220bc99ffe36591a4b31a2ee9e40620381e0ca","8bae0d458d6b9bd76774f30e37288ebe36b89f9d","87145a355e6648faa20d189cf991556f2eaa413e","42be94de2b517cb19b0af8aff489122d2500759c","3ce340cb7a11120c8b8a51512269125b53c080cc","a4192f5a30a095ec79fc8e5984530366f248a314","2c2fa29dfbbab106f1b94cdfdc67939d3355bd30","1041d3f00afb5f5a53196813ceb2ebfab6d0a6ee","7e44eadb263e5de0b3495bc1427e2cc27b9bc271","7ce5c4e3cdbad16ea8718bb78c3ba227c98c21b8","8db3c11cd85195f459b8ba82fe3326e8f86f1d52","3c59a0befe85eea6e4917b639bb43e12a35db029","a3c10798e153b92dba64075904f4314badc38149","2042b469be68653afcb2b7b38490c16369b4501a","092217c2267f6e0673590aa151d811e579ff7760"],"s2Url":"https://semanticscholar.org/paper/3fa5de3b8c3933885becdd3fe8540109309ab863","s2PdfUrl":"","id":"3fa5de3b8c3933885becdd3fe8540109309ab863","authors":[{"name":"Yonghong Yan","ids":["3671570"]},{"name":"Jiawen Liu","ids":["3775274"]},{"name":"Kirk W. Cameron","ids":["1717511"]},{"name":"Mariam Umar","ids":["40083137"]}],"journalName":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","paperAbstract":"Heterogeneous computing systems, e.g., those with accelerators than the host CPUs, offer the accelerated performance for a variety of workloads. However, most parallel programming models require platform dependent, time-consuming hand-tuning efforts for collectively using all the resources in a system to achieve efficient results. In this work, we explore the use of OpenMP parallel language extensions to empower users with the ability to design applications that automatically and simultaneously leverage CPUs and accelerators to further optimize use of available resources. We believe such automation will be key to ensuring codes adapt to increases in the number and diversity of accelerator resources for future computing systems. The proposed system combines language extensions to OpenMP, load-balancing algorithms and heuristics, and a runtime system for loop distribution across heterogeneous processing elements. We demonstrate the effectiveness of our automated approach to program on systems with multiple CPUs, GPUs, and MICs.","inCitations":["806c7bd6734e408f6c1e855a31e47a030cb9c577","8296beee679b5f6f65903b62ce740a1089e728f2"],"pdfUrls":["https://doi.org/10.1109/IPDPS.2017.99"],"title":"HOMP: Automated Distribution of Parallel Loops and Data in Highly Parallel Accelerator-Based Systems","doi":"10.1109/IPDPS.2017.99","sources":["DBLP"],"doiUrl":"https://doi.org/10.1109/IPDPS.2017.99","venue":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)"},
{"entities":["AVX-512","Advanced Vector Extensions","Algorithm","Bit array","Bit-level parallelism","Central processing unit","Computational problem","Hamming distance","Knights","Manycore processor","Microprocessor","Multi-core processor","Open-source software","Parallel computing","Random seed","SIMD","Seed","Smith–Waterman algorithm","Speedup","Streaming SIMD Extensions","Thread (computing)","Xeon Phi"],"journalVolume":"","journalPages":"52-61","pmid":"","year":2017,"outCitations":["034ddc1749424d0335be143a7c7d07b7c559c2ad","974ebdc4cd7ef424ed158a3d5dc349fefef5d35f","1c15f69566af6198f336b961aecf418b5ccd07d3","0ee3a1f7a363b16ceda8f1053a8172f051fd8d4c","43b52543490aa654705afad0dd6825180ff1b066","87f3d7730e190c695c84683830a702cc7dd6e296","66515254c60f8b754b5ce7ae7b42911653d7ed50","8598507f8bd3f00164bda383f4d3f5ca9d9b488c","25f017efd2905c6d0c6a92f2dfe19113ee42938e","65a3022b84914ffc0b19a119dcb44146e0c1ecda","b582cc447347c12e181eb6a3ee3a7f67aebf64d8","032af12c228e39f2e483588419bb81c44c905b8e","9be98def5602c99b620a42999dc7a8a38793c4a5","eec6d4664256c49a9e312b17f7455121cac90b25","000a6f63c588697d6ae8db6cb6ffd6394d961cb7","2f6c307c0e251afb0c93a9e8e9b6f5d1a0fb1f7e","efa5558bddd68abe4adc81adbbef6f739e648392","40c5441aad96b366996e6af163ca9473a19bb9ad","1ef38c80b1bc4352ce0df0ef7c05249fb64bf78d","0a908373dd5e87446ba85db0e590b3e3004e04f7","b0312b82ee0017f7bbfc78ff50fcb0561d70bc9b","627aa62eec8edda82481f429785b10ccc1818416","4ca9ea95a0a9846965e86619e646d9ca36930c18","71150718ec7affbc4f9130f55f925af0dd956651","f5a88d8561bf6a64b43aa7e88beff8220e792bee","72b6bc8f313219e2571f3234ea5ae8fc9b7bce27","f168f5e9b77e627aeb08a5073902c6a212992d77"],"s2Url":"https://semanticscholar.org/paper/47d9b984b2d6327098b39fc43245fc9bd3e08ffe","s2PdfUrl":"","id":"47d9b984b2d6327098b39fc43245fc9bd3e08ffe","authors":[{"name":"Yuandong Chan","ids":["2812433"]},{"name":"Kai Xu","ids":["31638960"]},{"name":"Haidong Lan","ids":["2827531"]},{"name":"Weiguo Liu","ids":["3429925"]},{"name":"Yongchao Liu","ids":["2916386"]},{"name":"Bertil Schmidt","ids":["38613433"]}],"journalName":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","paperAbstract":"The progress of next-generation sequencing has a major impact on medical and genomic research. This technology can now produce billions of short DNA fragments (reads) in a single run. One of the most demanding computational problems used by almost every sequencing pipeline is short-read alignment; i.e. determining where each fragment originated from in the original genome. Most current solutions are based on a seed-and-extend approach, where promising candidate regions (seeds) are first identified and subsequently extended in order to verify whether a full high-scoring alignment actually exists in the vicinity of each seed. Seed verification is the main bottleneck in many state-of-the-art aligners and thus finding fast solutions is of high importance. We present a parallel ungapped-alignment-featured seed verification (PUNAS) algorithm, a fast filter for effectively removing the majority of false positive seeds, thus significantly accelerating the short-read alignment process. PUNAS is based on bit-parallelism and takes advantage of SIMD vector units of modern microprocessors. Our implementation employs a vectorize-and-scale approach supporting multi-core CPUs and many-core Knights Landing (KNL)-based Xeon Phi processors. Performance evaluation reveals that PUNAS is over three orders-of-magnitude faster than seed verification with the Smith-Waterman algorithm and around one order-of-magnitude faster than seed verification with the banded version of Myers bit-vector algorithm. Using a single thread it achieves a speedup of up to 7.3, 27.1, and 11.6 compared to the shifted Hamming distance filter on a SSE, AVX2, and AVX-512 based CPU/KNL, respectively. The speed of our framework further scales almost linearly with the number of cores. PUNAS is open-source software available at https://github.com/Xu-Kai/PUNASfilter.","inCitations":[],"pdfUrls":["https://doi.org/10.1109/IPDPS.2017.35"],"title":"PUNAS: A Parallel Ungapped-Alignment-Featured Seed Verification Algorithm for Next-Generation Sequencing Read Alignment","doi":"10.1109/IPDPS.2017.35","sources":["DBLP"],"doiUrl":"https://doi.org/10.1109/IPDPS.2017.35","venue":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)"},
{"entities":["Algorithm","Attribute–value pair","BLAST","Big data","Computation","Distributed computing","MapReduce","Message Passing Interface","Partition (database)","Programmer"],"journalVolume":"","journalPages":"605-614","pmid":"","year":2017,"outCitations":["0ad8e89091eed09217e66adc98136126addc2619","6de2f02cfcc10d514431953a623898bfa61c1580","5c4d550c5b35a99d0b8a041be8e66d11ff350020","6474100a17b82d028e7131e8e0769cbc4e110914","257adee470c54280da48d448a064b35537d51fbd","63da1eca58997dc4d1b655095fd70edc9996a74a","39bd8e34616ef998d722749a8e20e207f1d69078","29542c65c237364e339e6789211db0f9e6db3287","9ee76efb171dbc1264ab4b22933e3deedfd7fde8","6de3915df2b9927a78f213629f3bcb052ec21e8b","c737aa8b2c916fe1f13a6fd4e847fa45da1e5434","11e4d4d00c7b1e3aa9fcf3c490b635df98827dd9","4d36d376d891d30aca684a397bdd0b924d9b444f","d6c4c76076efecb15655274adc648af8a445ed3a","1802e0d4c6fbe1867779f4181826b2b9e5096888","0f014693b25d9846025219b88f8ca480fac68b0a","14a80b973aeb96e6f2f8b9e292fc05b0d5f9aad0","168f2e12ae9fbb6c96146f4a7ded040d73e7b44b","2b0cc03aa4625a09958c20dc721f4e0a52c13fd0","9c3cc7337f7d70593a1ff8622de3128e1708b5a2","601911f388ba3a8b5d666b31afc61aa6dfd1d433","65b54461a0436e69969b2e2679dcbedcddd40d95","a2ff5117ccd1eb3e42c6a606b8cecb4358d3ec84","0a12a179bebdf4bb69d692a1127795b3f536270b","703c2186db40f01cc7527aa45f5627d877487fe2","b194c07c1b557f665a7460b515231ff36af6218f","090030e0d1aa117008e9e9fa4abdee0a95455f4a","230239fb61d7a6996ac9552706363323b34735f2","1f9d47906319d0a8fac5c5fdbadf98e9da7966f9","476b64be7cc0b985c02d69dd0532965924dd1869","1b535af0d110491eabeedf8323a51327846e55b2","0541d5338adc48276b3b8cd3a141d799e2d40150","1760771f154262c26fd263f71066eca04afe00f5","310ecac3477a51aa303284f0853bd49ae8383ac3","ab756f4ed89c8e17632befe15c3579f0b9f04800"],"s2Url":"https://semanticscholar.org/paper/62c0af943a259c66b91dc932d3a5611afd014a4c","s2PdfUrl":"","id":"62c0af943a259c66b91dc932d3a5611afd014a4c","authors":[{"name":"Hao Wang","ids":["39049654"]},{"name":"Jing Zhang","ids":["2266189"]},{"name":"Da Zhang","ids":["1948380"]},{"name":"Sarunya Pumma","ids":["1938539"]},{"name":"Wu-chun Feng","ids":["1688860"]}],"journalName":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","paperAbstract":"Today, big data applications can generate largescale data sets at an unprecedented rate; and scientists have turned to parallel and distributed systems for data analysis. Although many big data processing systems provide advanced mechanisms to partition data and tackle the computational skew, it is difficult to efficiently implement skew-resistant mechanisms, because the runtime of different partitions not only depends on input data size but also algorithms that will be applied on data. As a result, many research efforts have been undertaken to explore user-defined partitioning methods for different types of applications and algorithms. However, manually writing application-specific partitioning methods requires significant coding effort, and finding the optimal data partitioning strategy is particularly challenging even for developers that have mastered sufficient application knowledge. In this paper, we propose PaPar, a Parallel data Partitioning framework for big data applications, to simplify the implementations of data partitioning algorithms. PaPar provides a set of computational operators and distribution strategies for programmers to describe desired data partitioning methods. Taking an input data configuration file and a workflow configuration file as the input, PaPar can automatically generate the parallel partitioning codes by formalizing the user-defined workflow as a sequence of key-value operations and matrixvector multiplications, and efficiently mapping to the parallel implementations with MPI and MapReduce. We apply our approach on two applications: muBLAST, a MPI implementation of BLAST algorithms for biological sequence search; and PowerLyra, a computation and partitioning method for skewed graphs. The experimental results show that compared to the partitioning methods of applications, the codes generated by PaPar can produce the same data partitions with comparable or less partitioning time.","inCitations":["310ecac3477a51aa303284f0853bd49ae8383ac3"],"pdfUrls":["http://synergy.cs.vt.edu/pubs/papers/wang-papar-ipdps17.pdf","https://doi.org/10.1109/IPDPS.2017.119"],"title":"PaPar: A Parallel Data Partitioning Framework for Big Data Applications","doi":"10.1109/IPDPS.2017.119","sources":["DBLP"],"doiUrl":"https://doi.org/10.1109/IPDPS.2017.119","venue":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)"},
{"entities":["Activation function","Anatomy, Regional","Artificial neural network","Artificial neuron","Biological Neural Networks","Boosting (machine learning)","Byzantine fault tolerance","Computation","Distributed computing","Failure rate","Fault tolerance","Neural Network Simulation","Neural Networks","Neural Tube Defects","Neuron","Neurons","Propagation of uncertainty","Quantity","Solutions","Synapses","Synaptic Package Manager","Synaptic Transmission","Synaptic weight"],"journalVolume":"","journalPages":"","pmid":"","year":2016,"outCitations":["58ceeb151558c1f322b9f6273b47e90e9c04e6b1","171ef6765ddf9d22806146d8327ba082028ec32f","11951e035379c1dc039af868fbaba8d9737702c3","3fde9f175dd91ebd5accbb337c9abdefef31c81b","563e821bb5ea825efb56b77484f5287f08cf3753","01d47976b70be92ed2a5e06f8bf19a3af5439aad","0122e063ca5f0f9fb9d144d44d41421503252010","34f25a8704614163c4095b3ee2fc969b60de4698","443044b5873706aadaeeb87c8594528d83687462","f012a526043812e53007312ef6876bd8015d73ca","64192e5b1ce902b9dc397c53641d876a8a5506cd","251b6726fcad6b853de3d22939a4b2e36ef802ea","04113e8974341f97258800126d05fd8df2751b7e","1766e97c83f698ce3a292bc851a3bdee8179fba6","f4416d7035103e9d39622178b65f229d68bb6a96","766b21e4984729a17d2d826691affa25f855a38b","3e0080a34eca4eabb9b371c2b3c369dc4dc90112","d12d1289d2384c2ce642f01855637b9f0519e189","1a5ad04c3365a8435317044d5e1c14071a92b4b2","1d53f73859bc89c3ae1880326d4b01738d07abea","222e13bfa2eddcfe9a3eb6895f05186a3bd05b22","38ad1e2d6e85d2e68901fe20c45769fa343726d1","43cab718dbfdb9e9b0a515e897f8e26f3e0ac935","f74ded11f72099d16591a1191d72262ae6b5f14a","891e61c55b49dc55e95c4ed1803cd0801df02d00","b05b2d3a93cf01229cfbe124475d70674d91134d","e637d66ac3c76f805b4873095a748d1035f1f435","04105898efe96c7f2d876e6bcb9e19afd3e23635"],"s2Url":"https://semanticscholar.org/paper/3bcba9b71275c764c95ccf6b202cfdec1b233dd7","s2PdfUrl":"http://pdfs.semanticscholar.org/3bcb/a9b71275c764c95ccf6b202cfdec1b233dd7.pdf","id":"3bcba9b71275c764c95ccf6b202cfdec1b233dd7","authors":[{"name":"Mahdi El Mhamdi","ids":[]},{"name":"Rachid Guerraoui","ids":["1727558"]}],"journalName":"","paperAbstract":"Neural networks have been traditionally considered robust in the sense that their precision degrades gracefully with the failure of neurons and can be compensated by additional learning phases. Nevertheless, critical applications for which neural networks are now appealing solutions, cannot afford any additional learning at run-time. In this paper, we view a multilayer neural network as a distributed system of which neurons can fail independently, and we evaluate its robustness in the absence of any (recovery) learning phase. We give tight bounds on the number of neurons that can fail without harming the result of a computation. To determine our bounds, we leverage the fact that neural activation functions are Lipschitz-continuous. Our bound is on a quantity, we call the Forward Error Propagation, capturing how much error is propagated by a neural network when a given number of components is failing, computing this quantity only requires looking at the topology of the network, while experimentally assessing the robustness of a network requires the costly experiment of looking at all the possible inputs and testing all the possible configurations of the network corresponding to different failure situations, facing a discouraging combinatorial explosion. We distinguish the case of neurons that can fail and stop their activity (crashed neurons) from the case of neurons that can fail by transmitting arbitrary values (Byzantine neurons). In the crash case, our bound involves the number of neurons per layer, the Lipschitz constant of the neural activation function, the number of failing neurons, the synaptic weights and the depth of the layer where the failure occurred. In the case of Byzantine failures, our bound involves, in addition, the synaptic transmission capacity. Interestingly, as we show in the paper, our bound can easily be extended to the case where synapses can fail. We present three applications of our results. The first is a quantification of the effect of memory cost reduction on the accuracy of a neural network. The second is a quantification of the amount of information any neuron needs from its preceding layer, enabling thereby a boosting scheme that prevents neurons from waiting for unnecessary signals. Our third application is a quantification of the trade-off between neural networks robustness and learning cost.","inCitations":["3830110d2903378055b7284b88de7e3260ba7b6f","18f2a39119a83297bfd2d86bc78fb25e5dcc736f","5b5cb09fde38cd61a1576d58ce4b5287ff871afc","6b25106ad8f0a8167516921c3d3966c89f639d13","3c93f07a402dd865c86604628c3b019de2f484d9"],"pdfUrls":["https://infoscience.epfl.ch/record/217561/files/When_Neurons_Technical_Report.pdf","https://infoscience.epfl.ch/record/217561/files/When_Neurons_Fail_Tech%20Report.pdf?version=1"],"title":"When Neurons Fail Technical Report","doi":"","sources":[],"doiUrl":"","venue":""},
{"entities":["CPU cache","Dynamic random-access memory","Graphics processing unit","Memory bandwidth","Multiprocessing","Parallel computing","Replication (computing)","Scheduling (computing)","Task parallelism"],"journalVolume":"","journalPages":"698-707","pmid":"","year":2017,"outCitations":["1a850fbc5d86a91d882eec88290425fbdff57cf6","5d79e0c5e4b531f26de469688668c50f8c1069b2","0612811b3ed9fc7ef8300e65cb70360613dab01d","1eeb50d5f7937f65a910203ae61430ff8b969012","5f3cce1bc739ebfc03e003010d3438bb318efc14","70b1f5b927e03b1800f71c58198b5547dd0f83a4","72ba3d541afe35920aad310c32a5e7acd1347022","ac2c02a5073fb36701af2ecacc596c18db96e2da","2d6f002477015469075954c6748a1a85af352c94","da620c71ca24a493ebd9a96ab05ca116d72eb46e","67bf737ceccf387cdd05c379487da8301f55e93d","14505c2bdd3822d7a62385121d28ba3eb36fea1d","28cc7453c5f3f9ecb9415e631b0829ec9af8a4c3","2dc38b527e91f8cfee6f6c7ba4d079087c293471","7ea15c138cc72588fa376ff819f4bb8ca0b324da","03d832219a7cf933db0ef1f686fec730c09acd55"],"s2Url":"https://semanticscholar.org/paper/78b2292abc46172d621b5303f3f8b166337114a9","s2PdfUrl":"","id":"78b2292abc46172d621b5303f3f8b166337114a9","authors":[{"name":"Abdulaziz Tabbakh","ids":["12199006"]},{"name":"Murali Annavaram","ids":["1789661"]},{"name":"Xuehai Qian","ids":["2064331"]}],"journalName":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","paperAbstract":"The power consumed by memory system in GPUs is a significant fraction of the total chip power. As thread level parallelism increases, GPUs are likely to stress cache and memory bandwidth even more, thereby exacerbating power consumption. We observe that neighboring concurrent thread arrays (CTAs) within GPU applications share considerable amount of data. However, the default GPU scheduling policy spreads these CTAs to different streaming multiprocessor cores (SM) in a round-robin fashion. Since each SM has a private L1 cache, the shared data among CTAs are replicated across L1 caches of different SMs. Data replication reduces the effective L1 cache size which in turn increases the data movement and power consumption. The goal of this paper is to reduce data movement and increase effective cache space in GPUs. We propose a sharing-aware CTA scheduler that attempts to assign CTAs with data sharing to the same SM to reduce redundant storage of data in private L1 caches across SMs. We further enhance the scheduler with a sharing-aware cache allocation and replacement policy. The sharing-aware cache management approach dynamically classifies private and shared data. Private blocks are given higher priority to stay longer in L1 cache, and shared blocks are given higher priority to stay longer in L2 cache. Essentially, this approach increases the lifetime of shared blocks and private blocks in different cache levels. The experimental results show that the proposed scheme reduces the off-chip traffic by 19\\% which translates to an average DRAM power reduction of 10% and performance improvement of 7%.","inCitations":[],"pdfUrls":["http://alchem.usc.edu/portal/static/download/share_aware_gpu.pdf","https://doi.org/10.1109/IPDPS.2017.106"],"title":"Power Efficient Sharing-Aware GPU Data Management","doi":"10.1109/IPDPS.2017.106","sources":["DBLP"],"doiUrl":"https://doi.org/10.1109/IPDPS.2017.106","venue":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)"},
{"entities":["Algorithm","All nearest smaller values","Artificial neural network","Best, worst and average case","Computational biology","Data structure","Distributed memory","Information retrieval","Natural language processing","Parallel algorithm","Shared memory","Suffix tree","Time complexity","Worst-case complexity"],"journalVolume":"","journalPages":"12-21","pmid":"","year":2017,"outCitations":["44fdf50be9c3248b12f83072bd3db6c7914825a4","643b57f4b0421a9efb3bf7397eb24822a2a7b882","a5e766075757d85252b281bf13916fe80d43c32e","a7705a6819179daa39bb717775548328f04e05a8","84e8e291f307c3db9f4aa9da639a88ffd200f017","c87671177ebdc6b8f955a82d1e3e60dbaef22b3f","51f4bb5673ccddbf4bf27b02e2b16b77a175ac94","d206440c08d9d27b7506d282317ba72a3196ca48","a6e02957b814b301c3ac1ffdb5a3c36e3426b3d7","4c67449de5ad4510ea68ad615ab90af613d18bab","798cacfb9a8ca00806a876fcc6397124406a2234","c8e2d72db05ad9ec096058d7a5ed4bcdbf37ec8a","52283dd94ccf049d83f11b8af416be8488886df3","4094d9add285c020f7b7639dace66eaabc9d7394","8dfc1a49894632a27a88490db18441180a215fe2","73921604081880cf903eab568341ebbd1525713d","377c084a1c6ce806292f27856029b1ef19408b88"],"s2Url":"https://semanticscholar.org/paper/1625054c364a02597f8528c226ad0160ffe299c6","s2PdfUrl":"","id":"1625054c364a02597f8528c226ad0160ffe299c6","authors":[{"name":"Patrick Flick","ids":["34973108"]},{"name":"Srinivas Aluru","ids":["1740375"]}],"journalName":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","paperAbstract":"A Suffix tree is a fundamental and versatile string data structure that is frequently used in important application areas such as text processing, information retrieval, and computational biology. Sequentially, the construction of suffix trees takes linear time, and optimal parallel algorithms exist only for the PRAM model. Recent works mostly target low core-count shared-memory implementations but achieve suboptimal complexity, and prior distributed-memory parallel algorithms have quadratic worst-case complexity. Suffix trees can be constructed from suffix and longest common prefix (LCP) arrays by solving the All-Nearest-Smaller-Values(ANSV) problem. In this paper, we formulate a more generalized version of the ANSV problem, and present a distributed-memory parallel algorithm for solving it in O(n/p +p) time. Our algorithm minimizes the overall and per-node communication volume. Building on this, we present a parallel algorithm for constructing a distributed representation of suffix trees, yielding both superior theoretical complexity and better practical performance compared to previous distributed-memory algorithms. We demonstrate the construction of the suffix tree for the human genome given its suffix and LCP arrays in under 2 seconds on 1024 Intel Xeon cores.","inCitations":[],"pdfUrls":["https://doi.org/10.1109/IPDPS.2017.62"],"title":"Parallel Construction of Suffix Trees and the All-Nearest-Smaller-Values Problem","doi":"10.1109/IPDPS.2017.62","sources":["DBLP"],"doiUrl":"https://doi.org/10.1109/IPDPS.2017.62","venue":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)"},
{"entities":["Component-based software engineering","Kernel (operating system)","Linux","Linux","Load balancing (computing)","Memory hierarchy","Memory management","Memory module","Non-volatile random-access memory","Operating system","Operating-system-level virtualization","Out of the box (feature)","PCI Express","Persistent memory","Programmer","Scheduling (computing)","Simulation","User interface","User space"],"journalVolume":"","journalPages":"153-162","pmid":"","year":2017,"outCitations":["2f2614d54cae5e9367a061bc7455a870d9b9a85d","55c968c46b45f4b0955b31f36494ca6a87b8ddb4","77d97e17c7129a810d14fb8dfd17fa4ca07e18bc","24bb5f66906421f42aff2d64dfa35b4beb3ead7a","e040885cd33f933f356ff8f60783acfd3ed1a7c5","0e0ece84cd0a6c7d8bbe8e06da1341dcbd87d225","5232bdf468e907010a0886a63343e54b448780c5","19bd720b75e5c24dd8c702908222514b8df4d0da","0d6787f19c7a521784a38d31420dd8da7bd490ef","8fe10a1189cbd7644f38a2f65df509d9f84893fd","2fa3b8370363eb07f49ad864c932a2ca3c019a87","7070f6e7be0ea5c8b80a6d3f9986fcf743860443","2d846b715e2e6bde38a4c70861a18a4024e11412","34a86b7b24d4a93dbb249fc05f0b7c0f48f90aff","4fe2bf624e18d71d87ae36824606c42c64446562","1e2207b8f1b2ce2f09d2d8bfa5e01324a5d4effa","1274edf4e40fc3b012ace2a2b74217aa52e6d222","166eebd00f73b599be246d1897a87d715509d431","bc382e961d061afb334f015c6293704c58afcc89","145ad8fff2a7023e68645b1b3fe71fea5edf771e","273fcf24c3c9c07cde1cc68b23786ff7910e0d47","04ee1aee71c420c153893e26408a9e7b638ca763","3b7e2038ec22cf637df70c833d473b0f3b43713a","d36f0cf5375345732339abe77255f024d5a9d05a","701c90f0593e5675d62fc3882bd5da9b7c296394","8f1e54c98327edb317225d60464837b6557b247d","c5d0d547b6a3fa470dcc77f558f6c7c5768edabd","be337425916d4e61442269a9bc1cf69169cedb8d","2073266dfb3f034d55cd5a3fca62d230832afd43","cf60a588acc40f7fbb1a61612cdfd380d6faae6d","5b5dfbfffeade87035fca8fadca1a7f27f8a72fe","227e529c08f821d134dd15fb9296419250ab9301","771156b34f7f4f539ef7289027e2205692206aed","050b6a5f0e650a12223c27fb133eb5e398df8480","44da4713fcf0a4ee7a8323737e678b3faec42d2e","9254ac5860329c41457c1a0f03c7c5f216c56318"],"s2Url":"https://semanticscholar.org/paper/6b7c74e9d7d334613bf22032ad1184a92c66b643","s2PdfUrl":"","id":"6b7c74e9d7d334613bf22032ad1184a92c66b643","authors":[{"name":"Swann Perarnau","ids":["2042271"]},{"name":"Judicael A. Zounmevo","ids":["2381933"]},{"name":"Matthieu Dreher","ids":["40293316"]},{"name":"Brian C. Van Essen","ids":["19244667"]},{"name":"Roberto Gioiosa","ids":["1695375"]},{"name":"Kamil Iskra","ids":["1700811"]},{"name":"Maya Gokhale","ids":["1695549"]},{"name":"Kazutomo Yoshii","ids":["1696668"]},{"name":"Peter H. Beckman","ids":["1709765"]}],"journalName":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","paperAbstract":"Exascale systems are expected to feature hundreds of thousands of compute nodes with hundreds of hardware threads and complex memory hierarchies with a mix of on-package and persistent memory modules. In this context, the Argo project is developing a new operating system for exascale machines. Targeting production workloads using workflows or coupled codes, we improve the Linux kernel on several fronts. We extendthe memory management of Linux to be able to subdivide NUMA memory nodes, allowing better resource partitioning among processes running on the same node. We also add support for memory-mapped access tonode-local, PCIe-attached NVRAM devices and introduce a new scheduling class targeted at parallel runtimes supporting user-level load balancing. These features are unified into compute containers, a containerization approach focused on providing modern HPC applications with dynamic control over a wide range of kernel interfaces. To keep our approach compatible with industrial containerization products, we also identifycontentions points for the adoption of containers in HPC settings. Each NodeOS feature is evaluated by using a set of parallel benchmarks, miniapps, and coupled applications consisting of simulation and data analysis components, running on a modern NUMA platform. We observe out-of-the-box performance improvements easily matching, and often exceeding, those observed with expert-optimized configurations on standard OS kernels. Our lightweight approach to resource management retains the many benefits of a full OS kernel that application programmers have learned to depend on, at the same time providing a set of extensions that can be freely mixed and matched to best benefit particular application components.","inCitations":[],"pdfUrls":["http://www.mcs.anl.gov/papers/P7010-0217.pdf","https://doi.org/10.1109/IPDPS.2017.25"],"title":"Argo NodeOS: Toward Unified Resource Management for Exascale","doi":"10.1109/IPDPS.2017.25","sources":["DBLP"],"doiUrl":"https://doi.org/10.1109/IPDPS.2017.25","venue":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)"},
{"entities":["Algorithm","Approximation","Central processing unit","Computation","Graphics processing unit","List scheduling","Runtime system","Scheduling (computing)"],"journalVolume":"","journalPages":"768-777","pmid":"","year":2017,"outCitations":["91c8d42a946110db6ba10587c1d40d10c12661f9","04717b51d7c7dd598d732fb8a293d69d45c0fa9f","2bdb4b4dfec4bcf6d0c26c7ba5d540221a854c00","43b9e9e2e566a30b0b3ade63616ec923b93ff9a5","2e7f407e359890219570e29f8e4d842c4c977c11","ff764a8113534931e3f607cbbda60a2f3f57ad37","a547c92c55a7774fc3416831ecbf04227e444420","7059ceba09965f0950dbd71a825a3e78915edf11","5bfdff20904b5a7d2893a30ca8ffa383d54cbf99","6d208bd8d02f3a0eced5575433733f36eae8b684","877c9c040821cefc2cbe87aa2e42c8e197bb553d","2726565619d893410d936f789418f06ea3ca6287","4b0b1fd123ec9c43e82bc60d7fa0b9254d60a28d","2730606a9d29bb52bcc42124393460503f736d74","bce56b023105e6fadf8f05e450c872a3840a34db"],"s2Url":"https://semanticscholar.org/paper/8cb66bdb8e523c4a047e206571f535bb3268b1d1","s2PdfUrl":"","id":"8cb66bdb8e523c4a047e206571f535bb3268b1d1","authors":[{"name":"Olivier Beaumont","ids":["1731715"]},{"name":"Lionel Eyraud-Dubois","ids":["2380259"]},{"name":"Suraj Kumar","ids":["39703729"]}],"journalName":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","paperAbstract":"In High Performance Computing, heterogeneity is now the normwith specialized accelerators like GPUs providing efficientcomputational power. The added complexity has led to the developmentof task-based runtime systems, which allow complex computations to beexpressed as task graphs, and rely on scheduling algorithms to performload balancing between all resources of the platforms. Developing goodscheduling algorithms, even on a single node, and analyzing them canthus have a very high impact on the performance of current HPCsystems. The special case of two types of resources (namely CPUs andGPUs) is of practical interest. HeteroPrio is such an algorithm whichhas been proposed in the context of fast multipole computations, andthen extended to general task graphs with very interesting results. Inthis paper, we provide a theoretical insight on the performance ofHeteroPrio, by proving approximation bounds compared to the optimalschedule in the case where all tasks are independent and for differentplatform sizes. Interestingly, this shows that spoliation allows toprove approximation ratios for a list scheduling algorithm on twounrelated resources, which is not possible otherwise. We also establishthat almost all our bounds are tight. Additionally, we provide anexperimental evaluation of HeteroPrio on real task graphs from denselinear algebra computation, which highlights the reasons explainingits good practical performance.","inCitations":["e9687c7f101aab7488d40174a14210dc0bc70e60","c2b05dad04399ee2532b490d496ba2aabab00c9c"],"pdfUrls":["https://doi.org/10.1109/IPDPS.2017.71"],"title":"Approximation Proofs of a Fast and Efficient List Scheduling Algorithm for Task-Based Runtime Systems on Multicores and GPUs","doi":"10.1109/IPDPS.2017.71","sources":["DBLP"],"doiUrl":"https://doi.org/10.1109/IPDPS.2017.71","venue":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)"},
{"entities":["Algorithm","Divergence (computer science)","Leader election"],"journalVolume":"","journalPages":"182-191","pmid":"","year":2017,"outCitations":["4ad3d4c280431689ef3a78fd6757c99d93a45664","7b892aef86782f5f63a064b2edb0a21629753e71","2f1f27c95a68d5a3f1fb93b8c924bc06c02d936a","b8031ff197d3e3894f3484318d65aced7054138d","89e03249c2e8e0bc1948f27758da2fd139c7e950","28d40408e35cbbc6ca18df5d767a80be9da23e6a","2ca1408dedeb67edd181ea7aa8988d51c732e4d0","eb9bfe0367605b8221e0266029719891a0ad9807","a10691f71990e7f3f486ff9f28b43e7ac6d46934"],"s2Url":"https://semanticscholar.org/paper/17108842b3ccfa88ad436949549d33e6076e943e","s2PdfUrl":"","id":"17108842b3ccfa88ad436949549d33e6076e943e","authors":[{"name":"Karine Altisen","ids":["1735891"]},{"name":"Ajoy K. Datta","ids":["27396610"]},{"name":"Stéphane Devismes","ids":["2996049"]},{"name":"Anaïs Durand","ids":["38435293"]},{"name":"Lawrence L. Larmore","ids":["2355852"]}],"journalName":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","paperAbstract":"We study (deterministic) leader election in unidirectional rings of homonym processes that have no a priori knowledge on the number of processes. In this context, we show that there is no algorithm that solves process-terminating leader election for the class of asymmetric labeled rings. In particular, there is no process-terminating leader election algorithm in rings in which at least one label is unique. However, we show that process-terminating leader election is possible for the subclass of asymmetric rings, where multiplicity is bounded. We confirm this positive results by proposing two algorithms, which achieve the classical trade-off between time and space.","inCitations":[],"pdfUrls":["https://doi.org/10.1109/IPDPS.2017.23","http://www-verimag.imag.fr/~adurand/docs/slidesIPDPS17.pdf"],"title":"Leader Election in Asymmetric Labeled Unidirectional Rings","doi":"10.1109/IPDPS.2017.23","sources":["DBLP"],"doiUrl":"https://doi.org/10.1109/IPDPS.2017.23","venue":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)"},
{"entities":["Algorithm","Computation","Double-precision floating-point format","Experiment","FLOPS","Full scale","IBM WebSphere eXtreme Scale","Inter-process communication","Locality of reference","Manycore processor","Memory bound function","Parallel computing","Program optimization","Scalability","Simulation","Speedup","Stencil (numerical analysis)","Stration","Sunway","Sunway TaihuLight","Supercomputer","Throughput"],"journalVolume":"","journalPages":"535-544","pmid":"","year":2017,"outCitations":["b76269bf962989ce271bef7ea863ff4adf9c9de6","019ebe0205a759f8dab80b617f9f8ccd179c5c62","3db50ba6e67fe4df04352c4203dfeb86690158a5","0ebb8ef3ef660ea8484202e74e2e3df7b3c59cc6","34b44a9e55184b48c94a15f29f052941b342e8bf","0692f43523ebd6394a4ee76e3224f3c01cc2c4eb","9c7da99a0a046dcd1b736b19bf694137dec5e1bc","de3679256c7a30fbeb461086fceaf763e209d9fc","57f635f67fa7a1f742bb1c1f1da3e400c954440f","def34f422d6930bd23d5c58de78be98804e44e97","fe7bd2137955540edc81e84c5051ae32daf1703d","0382955dcc73511c3ae9b5327e0213272a1b4152","7c458085fababbaafcbb96c86e0b6482556b841f","1deeb53a514d9a54ad690626c5199bb0d117f9a2","14a477cf712ad5647180e6233dd0638c6c269fdd","eedb46c68a9c71ccb38de3933e5f7e1dd9a789c7","092217c2267f6e0673590aa151d811e579ff7760","3230131b14559a11c8ee9ab9beccf725dfb437de","7b69e7c3dd0ede0eacb2c42c82559367c8f194d4","27d2ac18ef4504df1460460c9711e69d166cc11e","273d591af0bdcbefe37d7dd9150e2f612ca7121d","2d21ca41ebdfb2b4e7a145e36dc8321386627e94","db9a6214180872b8f85c08947da4cd653f2e481e","11f2a5d947f5899d6060009462feb6888a07fb1c","791370da29ba96d355c2fad1ecd06b8e709f8755","408e61c117816833cdd807b5d8c9258f1c2022ab","d40be7ee92f0281425a482f9837132fa5e34bfe1","1ac8bf57669da9afce7a19d9a09dca0d6d4a9784","17c44c4654cc53e792c3aafe3b01df9829fe8e90","0b47e159ed9a3e5db1adc135620e7526d93abd87","009e4da527a3518c29c95970efb79733a67979fb","b8932adc9d9a80de33f891c3e94277b01d100c97"],"s2Url":"https://semanticscholar.org/paper/0c56c62813329bbf616f21eca675f7df796d025b","s2PdfUrl":"","id":"0c56c62813329bbf616f21eca675f7df796d025b","authors":[{"name":"Yulong Ao","ids":["3341167"]},{"name":"Chao Yang","ids":["39408447"]},{"name":"Xinliang Wang","ids":["3303339"]},{"name":"Wei Xue","ids":["1712301"]},{"name":"Haohuan Fu","ids":["1711877"]},{"name":"Fangfang Liu","ids":["1702887"]},{"name":"Lin Gan","ids":["38512584"]},{"name":"Ping Xu","ids":["1724368"]},{"name":"Wenjing Ma","ids":["33955807"]}],"journalName":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","paperAbstract":"Stencil computation arises from a broad set of scientific and engineering applications and often plays a critical role in the performance of extreme-scale simulations. Due to the memory bound nature, it is a challenging task to opti- mize stencil computation kernels on modern supercomputers with relatively high computing throughput whilst relatively low data-moving capability. This work serves as a demon- stration on the details of the algorithms, implementations and optimizations of a real-world stencil computation in 3D nonhydrostatic atmospheric modeling on the newly announced Sunway TaihuLight supercomputer. At the algorithm level, we present a computation-communication overlapping technique to reduce the inter-process communication overhead, a locality- aware blocking method to fully exploit on-chip parallelism with enhanced data locality, and a collaborative data accessing scheme for sharing data among different threads. In addition, a variety of effective hardware specific implementation and optimization strategies on both the process- and thread-level, from the fine-grained data management to the data layout transformation, are developed to further improve the per- formance. Our experiments demonstrate that a single-process many-core speedup of as high as 170x can be achieved by using the proposed algorithm and optimization strategies. The code scales well to millions of cores in terms of strong scalability. And for the weak-scaling tests, the code can scale in a nearly ideal way to the full system scale of more than 10 million cores, sustaining 25.96 PFLOPS in double precision, which is 20% of the peak performance.","inCitations":["e45dea6588d1de0a23618e019031e67eedeeee26"],"pdfUrls":["https://doi.org/10.1109/IPDPS.2017.9"],"title":"26 PFLOPS Stencil Computations for Atmospheric Modeling on Sunway TaihuLight","doi":"10.1109/IPDPS.2017.9","sources":["DBLP"],"doiUrl":"https://doi.org/10.1109/IPDPS.2017.9","venue":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)"},
{"entities":["Algorithm","Bitonic sorter","Central processing unit","Complex system","Dynamic random-access memory","Manycore processor","Mesh networking","Message Passing Interface","Multi-core processor","OpenMP","Program optimization","Xeon Phi"],"journalVolume":"","journalPages":"297-306","pmid":"","year":2017,"outCitations":["03514ed26ee815192ec46eb23a148e0b7b897775","23177452df15b652dd54a59324502b92c99687a7","68bb9a5919e5ae1b45c5923259ceeb8f67acb2ae","645d9e7e5e3c5496f11e0e303dc4cc1395109773","cf0591e00458d5d7ca20fbc82da70f783f57bfd6","d2c16be2bf76ac16d3ffd0fc2aff8a1c6e3c3dc8","32d355a7a20f92ccda0608f83d7456870231c570","14a4369f0fd45b3ae2323dd71eac8980b1556f0d","888d4ade3a7552ebafe997988a82cdd16128961e","034c374b2d973a3ae6e5d80f8ba88b59e5215aca","3f750233c3e20da134b4427eb6645f877ac0a503","2ae65bebbd8c7f943811eb3417b1004870a88483","67cf1189c859d66bac309f9438df434fb651f97a","f3325ace129dec914966f9894d9f412e5e04bdc2"],"s2Url":"https://semanticscholar.org/paper/13387166efd4f6d66b9ab19828855090586b16fd","s2PdfUrl":"","id":"13387166efd4f6d66b9ab19828855090586b16fd","authors":[{"name":"Sabela Ramos","ids":["25440110"]},{"name":"Torsten Hoefler","ids":["1713648"]}],"journalName":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","paperAbstract":"Increasingly complex memory systems and onchip interconnects are developed to mitigate the data movement bottlenecks in manycore processors. One example of such a complex system is the Xeon Phi KNL CPU with three different types of memory, fifteen memory configuration options, and a complex on-chip mesh network connecting up to 72 cores. Users require a detailed understanding of the performance characteristics of the different options to utilize the system efficiently. Unfortunately, peak performance is rarely achievable and achievable performance is hardly documented. We address this with capability models of the memory subsystem, derived by systematic measurements, to guide users to navigate the complex optimization space. As a case study, we provide an extensive model of all memory configuration options for Xeon Phi KNL. We demonstrate how our capability model can be used to automatically derive new close-to-optimal algorithms for various communication functions yielding improvements 5x and 24x over Intel&#x2019;s tuned OpenMP and MPI implementations, respectively. Furthermore, we demonstrate how to use the models to assess how efficiently a bitonic sort application utilizes the memory resources. Interestingly, our capability models predict and explain that the high bandwidthMCDRAM does not improve the bitonic sort performance over DRAM.","inCitations":["53ee65bfc69cd55d81196537086137ef8efb2108","48e6c7035b35a3ee8b8c2e430c158bfd7102a2fe"],"pdfUrls":["https://doi.org/10.1109/IPDPS.2017.30"],"title":"Capability Models for Manycore Memory Systems: A Case-Study with Xeon Phi KNL","doi":"10.1109/IPDPS.2017.30","sources":["DBLP"],"doiUrl":"https://doi.org/10.1109/IPDPS.2017.30","venue":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)"},
{"entities":["Android","Booting","CPU cache","Cloud computing","Computation","Computation offloading","Computer data storage","FUJITSU Cloud IaaS Trusted Public S5","Memory footprint","Mobile app","Mobile cloud computing","Mobile device","Operating system","Operating-system-level virtualization","Runtime system","Smartphone","Speedup","Virtual machine"],"journalVolume":"","journalPages":"123-132","pmid":"","year":2017,"outCitations":["a36bcf48132a1e115020822740e5077d5dcf73e4","6a656a567097c53a49b1dbeb9e1e77bebf7524ec","4e27f44ade4545931a99eee2dc8011b44f5db4b6","24459d006abcf0afd80a33b32884da80236d7a84","59d45d685e35f5a84768c029ea09b9c48765251e","5892b9314971e90e32d8bf81ca4e7dcbecb5ef8f","e65305a3a0bda61bff11d7f70ccc9057039f26b3","1914d07f940dcae3d82642513718858925a26fc3","1e126cee4c1bddbfdd4e36bf91b8b1c2fe8d44c2","594710511ce2177ff7dbbc62fa75dbf14fc7ca26","08b11d0812f6cc3c9b954c116d36bd983ead6241","06706cd6c46bc413bb9c272d6c1c034313ff2742","31d8a5c682486e27eae512fce092d0d458e05760","2c2cfbec94307fc92192e5a4be0d0731799f9bf9","3760de53eec6bff96f2cca1365271b97e1118179","0a289fd7b14345822b1acda6d82750b15d59663e","38208e29960623a60d0f34079c2641319ac21bc9","14d644c617b1bdf57d626d4d14f7210f99d140e6","99b2348bc0a4425294dedba612de72cef0b63402"],"s2Url":"https://semanticscholar.org/paper/b359adacfa2dd6ab15c8fe930f99ac0db0be30a8","s2PdfUrl":"","id":"b359adacfa2dd6ab15c8fe930f99ac0db0be30a8","authors":[{"name":"Song Wu","ids":["1685757"]},{"name":"Chao Niu","ids":["1714812"]},{"name":"Jia Rao","ids":["1786877"]},{"name":"Hai Jin","ids":["2156156"]},{"name":"Xiaohai Dai","ids":["19208996"]}],"journalName":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","paperAbstract":"With the explosive growth of smartphones and cloud computing, mobile cloud, which leverages cloud resource to boost the performance of mobile applications, becomes attrac- tive. Many efforts have been made to improve the performance and reduce energy consumption of mobile devices by offloading computational codes to the cloud. However, the offloading cost caused by the cloud platform has been ignored for many years. In this paper, we propose Rattrap, a lightweight cloud platform which improves the offloading performance from cloud side. To achieve such goals, we analyze the characteristics of typical of- floading workloads and design our platform solution accordingly. Rattrap develops a new runtime environment, Cloud Android Container, for mobile computation offloading, replacing heavy- weight virtual machines (VMs). Our design exploits the idea of running operating systems with differential kernel features inside containers with driver extensions, which partially breaks the limitation of OS-level virtualization. With proposed resource sharing and code cache mechanism, Rattrap fundamentally improves the offloading performance. Our evaluation shows that Rattrap not only reduces the startup time of runtime environments and shows an average speedup of 16x, but also saves a large amount of system resources such as 75% memory footprint and at least 79% disk capacity. Moreover, Rattrap improves offloading response by as high as 63% over the cloud platform based on VM, and thus saving the battery life.","inCitations":[],"pdfUrls":["https://doi.org/10.1109/IPDPS.2017.47"],"title":"Container-Based Cloud Platform for Mobile Computation Offloading","doi":"10.1109/IPDPS.2017.47","sources":["DBLP"],"doiUrl":"https://doi.org/10.1109/IPDPS.2017.47","venue":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)"},
{"entities":["Algorithm","Blue Gene","Experiment","Graphics processing unit","Load balancing (computing)","Louvain Modularity","Multidimensional scaling","Parallel computing","Sequential algorithm","Shared memory","Supercomputer"],"journalVolume":"","journalPages":"625-634","pmid":"","year":2017,"outCitations":["0af803edccda82003b909c630a074c3e1061b0ab","0971a05d435ebc79711e1e74e029416b9d29b05d","38d925c33a8433d36b9409b72d049d035fdd31fd","20983b8a3ab571510697494d168f9685f5596245","7e36674b63ab1c05579b26af6f30c6b0aa17e057","95b251202e1f5ff7d7f41dda553a38e395ecf555","f5ee4b5287bee836f1de23c76ab4e1fa0a58752d","740f4097ca7011a542766f35f2e9bd8064ca30b5","31181e73befea410e25de462eccd0e74ba8fea0b","1ef7f02bce931c8e9ef529e095b274132ce4011a","453d73995c98d6677a77bf547fe569ab7b1b02a8","9e704f358d979d28a091f19adb7aacdbf4b6d83f","2145058a9c15c0e9468638a6b56891271526df1c","3095db6f07b089ebfd07685e8a98b72445b9e73c","73f91eceee057aedba214e27c6dab9d9b081deaf","40a2a398862f5c62555ffaf6d8421dea9f1bbcd3","769d75e9cb010b76ba412d9654cf43c4edf15076","6c2a93d49e4082ab4b5ff4e8c67554654f79468c","31b255ae31ea46bea5a9f9dad19c5cc1ca4c2db7","f38510810eb9a34d10e729112541bda9d46e79d4","cc37e7bacd29b26056163a79c411471d22bf8b0b","de89a54c93e05c24e335c6e0a5f4855c5a06a73a","5b72cf570bfcc84cb03a9e310e680363373565cf","2440a3bce01e9a91f255d2d03447e5c1c53574da","5e2fb453613a697f3aca6cea598d272c4c5536a5"],"s2Url":"https://semanticscholar.org/paper/1e6043554b903ff5c6d0f43eb0785a8a57090da9","s2PdfUrl":"","id":"1e6043554b903ff5c6d0f43eb0785a8a57090da9","authors":[{"name":"Md. Naim","ids":["36197595"]},{"name":"Fredrik Manne","ids":["2296505"]},{"name":"Mahantesh Halappanavar","ids":["3285377"]},{"name":"Antonino Tumeo","ids":["2606269"]}],"journalName":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","paperAbstract":"We present and evaluate a new GPU algorithm based on the Louvain method for community detection. Our algorithm is the first for this problem that parallelizes the access to individual edges. In this way we can fine tune the load balance when processing networks with nodes of highly varying degrees. This is achieved by scaling the number of threads assigned to each node according to its degree. Extensive experiments show that we obtain speedups up to a factor of 270 compared to the sequential algorithm. The algorithm consistently outperforms other recent shared memory implementationsand is only one order of magnitude slower than the current fastest parallel Louvain method running on a Blue Gene/Q supercomputer using more than 500K threads.","inCitations":["94d77111eee3b6c24f19f7397200cee33eb385d4"],"pdfUrls":["https://doi.org/10.1109/IPDPS.2017.16"],"title":"Community Detection on the GPU","doi":"10.1109/IPDPS.2017.16","sources":["DBLP"],"doiUrl":"https://doi.org/10.1109/IPDPS.2017.16","venue":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)"},
{"entities":["Algorithm","Graph (abstract data type)","Graphics processing unit","Memory management","Programmer","Scalability"],"journalVolume":"","journalPages":"479-490","pmid":"","year":2017,"outCitations":["2a17c90ed723d6a14415cc1f677a5c0aa512f501","1186c4a90fb212bdd466159c3a9d45a83189088f","93ee8e1c05d11d63aa3d61653b2c8bae75e0aecd","175d795f44037ef60dd9df341701cd5fdc449f1f","5c874558bfc493260c520a43dd5e29acae68b028","0e570045a764fb1f49c2e33e5ab02b9eba06fbc6","2916fd514c69c1b3141c377c1c97d957bdc86c5e","32c8c7949a6efa2c114e482c830321428ee58d70","b78c04c7f29ddaeaeb208d4eae684ffccd71e04f","56d5d3f3ec4d95d13b0a2d6c08ee46f8704b82dc","259e93de2f10d395a1bdfb2dc6da72b6a3998572","687b65a6e91e429c6c1369aee3b493ffd83c0da0","14edc660cb7db680f2e471460a794f68ba03f295","2ce27845038020ea43afa08e91f916a4ccf19924","191fd33f17c2a79b3825d4cc2105c47a8f16ba44","22c74d8be071084ce8812af19548e7bf2bf0c8b6","7ebb9fad71ce8e08d5284b7644a5452cff6c75b3","80d8fe9fc7b965e1f6289677922a81cd03c54dd5","3ebf3857a60c3e224284bbbe6c7127d0a12c546d","c85c784038d6f4f4845842bdc41877f8581ac796","0d49c615e4a261824677e1e7c08411f745471c79","0074e55e67c74420b725fbb09a8f2f351d6947a9","3c84f22df1948dfa8b1b14bbd4c850baf9c5b632"],"s2Url":"https://semanticscholar.org/paper/3a1f3429bbb163e050188cce42a647a11312260c","s2PdfUrl":"","id":"3a1f3429bbb163e050188cce42a647a11312260c","authors":[{"name":"Yuechao Pan","ids":["2170241"]},{"name":"Yangzihao Wang","ids":["1772360"]},{"name":"Yuduo Wu","ids":["1929145"]},{"name":"Carl Yang","ids":["2695365"]},{"name":"John D. Owens","ids":["1758404"]}],"journalName":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","paperAbstract":"We present a single-node, multi-GPU programmable graph processing library that allows programmers to easily extend single-GPU graph algorithms to achieve scalable performance on large graphs with billions of edges. Directly using the single-GPU implementations, our design only requires programmers to specify a few algorithm-dependent concerns, hiding most multi-GPU related implementation details. We analyze the theoretical and practical limits to scalability in the context of varying graph primitives and datasets. We describe several optimizations, such as direction optimizing traversal, and a just-enough memory allocation scheme, for better performance and smaller memory consumption. Compared to previous work, we achieve best-of-class performance across operations and datasets, including excellent strong and weak scalability on most primitives as we increase the number of GPUs in the system","inCitations":["e2207382768cef76f63a16d91a169078cfdc9b46","46f3bb6751419b87856c4db0193e7a72ef3fa17c","b292326ae0bb4b3192f425ab9928579c6ea8d4f2","db63d47efa261ce1bb1a154e140e4a059f9bb999","0aa83b7afe8d5f7c39b6bd97c3b9394c4b6e5cdf","896134c7aa767e27cb3c3aa0662b335473923602","6401ed8f6f0b37f30573edcc2743134c6fe7a682"],"pdfUrls":["http://www.idav.ucdavis.edu/~yzhwang/SC2015-poster.pdf","https://arxiv.org/pdf/1504.04804v2.pdf","http://idav.ucdavis.edu/~yzhwang/SC2015-poster.pdf","http://arxiv.org/pdf/1504.04804v2.pdf","https://arxiv.org/pdf/1504.04804v1.pdf","https://arxiv.org/pdf/1504.04804v4.pdf","https://people.csail.mit.edu/jshun/6886-s18/papers/Pan17.pdf","https://arxiv.org/pdf/1504.04804v3.pdf","https://doi.org/10.1109/IPDPS.2017.117","http://arxiv.org/abs/1504.04804"],"title":"Multi-GPU Graph Analytics","doi":"10.1109/IPDPS.2017.117","sources":["DBLP"],"doiUrl":"https://doi.org/10.1109/IPDPS.2017.117","venue":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)"},
{"entities":["Data structure","Dependence analysis","Fault tolerance","Finite-state machine","High availability","Multi-core processor","Scheduling (computing)","State machine replication","Strong consistency","Throughput","Web application"],"journalVolume":"","journalPages":"748-757","pmid":"","year":2017,"outCitations":["1d99b7749a9311d2db24a3d84728e444eff23e4b","0a8d3569e9d3359ab9612e1e55775524242e5532","b77bb6ff9a7018fa4f24893a38c27ac6efbfd4e1","036ebe81fc7bd9000c3edda83fa30bee03fedc1a","01e9ddf1062f9a7d7847bb9bcd2371ce6e0d3e29","3a043714354fe498752b45e4cf429dbae0fb2558","d12d1289d2384c2ce642f01855637b9f0519e189","18a5f443299784479e78d9e77f175af57cb2fa2b","f3018e7589af851341e6b40affb12d0ebdfa7db1","5f3f9223c5c9f896be099bc177929febad508407","738c71d77bf3041e4f051a87b1f314738a05a4d3","60d1301ceaf38d5188c3a3b1b421a0eb77f81433","42142c121b2dbe48d55e81c2ce198a5639645030","0e6f25ca2e9dbcca8a630ac5924470aafa3fbcac","8a0af8ae748210ef571d074362b552af571e6d33","00c181b8b64e824fbe0172339f1e4560b557fab5","05a618847e4f08e5bca29dff732757779722b2e0","a1c704b281e939d343219edffbc84b379ab8a571","00f7b192212078fc8afcbe504cc8caf57d8f73b5","4af63ed343df388b6353b6fc77c7137d27822bf4","33457f49553d918e912c2d8c54b81f4fd8a4c234","06d8aa948ed0ff654f772439c00711dfe7fa3d1a"],"s2Url":"https://semanticscholar.org/paper/e7a4c9088cfc7a5ab552728a280aadbb237e54bf","s2PdfUrl":"","id":"e7a4c9088cfc7a5ab552728a280aadbb237e54bf","authors":[{"name":"Odorico Machado Mendizabal","ids":["2546148"]},{"name":"Ruda S. T. De Moura","ids":["34993374"]},{"name":"Fernando Luís Dotti","ids":["3039851"]},{"name":"Fernando Pedone","ids":["1739265"]}],"journalName":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","paperAbstract":"Many services used in large scale web applications should be able to tolerate faults without impacting their performance. State machine replication is a well-known approach to implementing fault-tolerant services, providing high availability and strong consistency. To boost the performance of state machine replication, recent proposals have introduced parallel execution of commands. In parallel state machine replication, incoming commands may or may not depend on other commands that are waiting for execution. Although dependent commands must be processed in the same relative order at every replica to avoid inconsistencies, independent commands can be executed in parallel and benefit from multi-core architectures. Since many application workloads are mostly composed of independent commands, these parallel models promise high throughput without sacrificing strong consistency. The efficient execution of commands in such environments, however, requires effective scheduling strategies. Existing approaches rely on dependency tracking based on pairwise comparison between commands, which introduces scheduling contention. In this paper, we propose a new and highly efficient scheduler for parallel state machine replication. Our scheduler considers batches of commands, instead of commands individually. Moreover, each batch of commands is augmented with a compact data structure that encodes commands information needed to the dependency analysis. We show, by means of experimental evaluation, that our technique outperforms schedulers for parallel state machine replication by a fairly large margin.","inCitations":["32d185cff26027792ce16194f97a899fd045b0d6"],"pdfUrls":["https://doi.org/10.1109/IPDPS.2017.29","http://www.inf.usi.ch/faculty/pedone/Paper/2017/2017IPDPS.pdf"],"title":"Efficient and Deterministic Scheduling for Parallel State Machine Replication","doi":"10.1109/IPDPS.2017.29","sources":["DBLP"],"doiUrl":"https://doi.org/10.1109/IPDPS.2017.29","venue":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)"},
{"entities":["Barrier (computer science)","Benchmark (computing)","Breadth-first search","Fast Fourier transform","Finite difference","Giga-updates per second","InfiniBand","Interconnection","Network architecture","Simulation","Vortex"],"journalVolume":"","journalPages":"409-418","pmid":"","year":2017,"outCitations":["a470a01cedc4df4d7719f41e174dda7c26a9ddb3","2ee85e62099a2393366e33e794796afe62be75e0","46798a562160a628681f4b69121d45dd1105937d","33715194bf741fe17d6f6b9559af694907c26d2a","5f8991828def57d2f0cda942566afff56740d150","c4dd4dff3b7e03a372eaee00dba074f3ebf4a4dc","6bad177eb5fc0fd7ea223149cec4a76d8567479a","3de8fa07b2513240ed6d678ac1a3634f4ab52398","143504cf0794163b60b93fb17cf61c885d7fd73c","02115b80df6978eaded262ea58accb1e1d1364ed","a9d590711b56bb2e66f98814802f1429c20ee863","6891e0c7ded97a62df6fdc3c2553c07de2822b13","33299bbc74d62c9d83f714f0753fc0f2ecadc645","ff06c045b4d5de3c52504c96f43fe296070a8482","01c1ba712c47112ac1d7970b280064132932c492","304a6bac0bbdf16cb3d32853d891748ffd663ae4","cbf8acf187297b22bf189cd057b9495f90bd973b","57748a6246541a2ceb71de18be3a24165753e0a8","848dd6c85c200325778dea4d2f80b30bef2efc12","2b8184d84bf668ed213e3f0fe1081c2fcf08e14d","c3fbbd9c1fc5e53c6a9e3fe27e1bfce4755c8ef3"],"s2Url":"https://semanticscholar.org/paper/1315ea1bf8f92bc1773556f727af0f85605cb677","s2PdfUrl":"","id":"1315ea1bf8f92bc1773556f727af0f85605cb677","authors":[{"name":"Roberto Gioiosa","ids":["1695375"]},{"name":"Antonino Tumeo","ids":["2606269"]},{"name":"Jian Yin","ids":["1700855"]},{"name":"Thomas Warfel","ids":["2616994"]},{"name":"David J. Haglin","ids":["1774041"]},{"name":"Santiago Betelú","ids":["2706560"]}],"journalName":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","paperAbstract":"Emerging applications for data analytics and knowledge discovery typically have irregular or unpredictable communication patterns that do not scale well on parallel systems designed for traditional bulk-synchronous HPC applications. New network architectures that focus on minimizing (short) message latencies, rather than maximizing (large) transfer bandwidths, are emerging as possible alternatives to better support those applications with irregular communication patterns. We explore a system based upon one such novel network architecture, the Data Vortex interconnection network, and examine how this system performs by running benchmark code written for the Data Vortex network, as well as a reference MPI-over- Infiniband implementation, on the same cluster. Simple communication primitives (ping-pong and barrier synchronization), a few common communication kernels (distributed 1D Fast Fourier Transform, breadth-first search, Giga-Updates Per Second) and three prototype applications (a proxy application for simulating neutron transport-&#x201d;SNAP&#x201d;, a finite difference simulation for computing incompressible fluid flow, and an implementation of the heat equation) were all implemented for both network models. The results were compared and analyzed to determine what characteristics make an application a good candidate for porting to a Data Vortex system, and to what extent applications could potentially benefit from this new architecture.","inCitations":["b11068b7dda0b4470132b868c841cb2fbd5ca879"],"pdfUrls":["https://doi.org/10.1109/IPDPS.2017.121"],"title":"Exploring DataVortex Systems for Irregular Applications","doi":"10.1109/IPDPS.2017.121","sources":["DBLP"],"doiUrl":"https://doi.org/10.1109/IPDPS.2017.121","venue":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)"},
{"entities":["Admissible numbering","Consistency model","High- and low-level","IBM WebSphere eXtreme Scale","Non-volatile memory","Papyrus","Persistent memory","Scalability","Software system","Tcl","Web beacon"],"journalVolume":"","journalPages":"1151-1162","pmid":"","year":2017,"outCitations":["165d99c9d30be5d301b998dc23c1a6a28fd0c425","0494a1ab6f0dd764fb9039772818b8f269ed70b4","7377a53399ebd9ecd523cb7beeba0fc614239897","27bcb72519d77192da2b30eca4e1442c8f3637b1","a578daf478e4555e6e81c63cef4f138d92f93245","61b8a8f5810f6670466f9ea58b7cb390ca1a4a89","10d8afea57c8f159c4eb2664a40c8fb859acefef","2e3bda19a2ed88e8a6e5cc415e27da551653ff1d","94783d113951822195d4ba44599a8fcbdef9d4bf","9183cde02e4306828089fb8adae74736a9df3ceb","59ba9f62728b6231f982ea3b59f9ba7422182f28","0599ba259341963bf8abf2818c874713e570a039","40c5050e470fa0890e85487e4679197e07a91c09","4f0a03bd3c7e148e62eecaeaa6c9ce2a5b2a7d52","098d792d1783b5f6fc098203f71f21f5d053c653","b3bdfa918336a7c18093a1f6a14f32ba77d41991","9a047672d9cad7fe11785682e8249e8a5f75c8f1","6e0ade8e4c0948e47b7e1ad78eacf42e5f9d8d0f","f8f52a402b8833ea1ad8eb34e48f011b25c0d306","3a8c90ab13adb55e3610a020c69f03d72dfae274","1c82d6dd3fde20878f9500c31351a3ceb9c05a46","5c0e86f286972d34036da95b9c8d80581a985819","35ae271bcc515d61dc113c35f8d3dc0300f8faad","6a9a57dddf37adce1eb16c682205de8bf9447f60","da8f5c3e65e2eb398dc5a4866023ef51e4056905"],"s2Url":"https://semanticscholar.org/paper/2cdcb05bad9c38dfa39530b159a4ecc0e94d922f","s2PdfUrl":"","id":"2cdcb05bad9c38dfa39530b159a4ecc0e94d922f","authors":[{"name":"Jungwon Kim","ids":["1697093"]},{"name":"Kittisak Sajjapongse","ids":["3079326"]},{"name":"Seyong Lee","ids":["8568681"]},{"name":"Jeffrey S. Vetter","ids":["7553591"]}],"journalName":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","paperAbstract":"A surprising development in recently announced HPC platforms is the addition of, sometimes massive amounts of, persistent (nonvolatile) memory (NVM) in order to increase memory capacity and compensate for plateauing I/O capabilities. However, there are no portable and scalable programming interfaces using aggregate NVM effectively. This paper introduces Papyrus: a new software system built to exploit emerging capability of NVM in HPC architectures. Papyrus (or Parallel Aggregate Persistent -YRU- Storage) is a novel programming system that provides features for scalable, aggregate, persistent memory in an extreme-scale system for typical HPC usage scenarios. Papyrus mainly consists of Papyrus Virtual File System (VFS) and Papyrus Template Container Library (TCL). Papyrus VFS provides a uniform aggregate NVM storage image across diverse NVM architectures. It enables Papyrus TCL to provide a portable and scalable high-level container programming interface whose data elements are distributed across multiple NVM nodes without requiring the user to handle complex communication, synchronization, replication, and consistency model. We evaluate Papyrus on two HPC systems, including UTK Beacon and NERSC Cori, using real NVM storage devices.","inCitations":["41d01619b4f0d14be5c0135ca35f06fb5fc93b2a"],"pdfUrls":["http://www.csm.ornl.gov/newsite/documents/highlights/Science_Highlight_Papyrus.pdf","https://doi.org/10.1109/IPDPS.2017.72"],"title":"Design and Implementation of Papyrus: Parallel Aggregate Persistent Storage","doi":"10.1109/IPDPS.2017.72","sources":["DBLP"],"doiUrl":"https://doi.org/10.1109/IPDPS.2017.72","venue":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)"},
{"entities":["Adaptive algorithm","Algorithm","Approximate computing","Compiler","Computation","Correctness (computer science)","Linear algebra","Machine learning","Memoization","Run time (program lifecycle phase)","Runtime system","Speedup","Static program analysis","Stencil (numerical analysis)"],"journalVolume":"","journalPages":"1140-1150","pmid":"","year":2017,"outCitations":["68837728232463651283edbb7ef0c93b2f502b2b","38e4f247e907bda2275fd6139bd4fb651f915360","1f2a00758fc38d764b05adb76110500870610bc8","0110c80228683bc32879efb1b2f3931421e52eb6","4b434f94fafc3ffc76e0c440897ccd222eaa38ac","16f425a8c6d42a09c16fa074d3b0d7a87fd9348e","180189c3e8b0f783a8df6a1887a94a5e3f82148b","3186aead0cac0a94a8bf909a5023eae7afa8426b","1a8bfc3d9361dc23544c7bc81f4a5d88497a7b50","cfd34380711f505e58289a524e6d154dc44355a1","237a086708ccae0686c7d1995e0a7017650c5740","15b275f0421c606f5903532e9964b140cbb2f878","4f105edc6d373f41b998871962189ab9b2adb601","785f69fbf3ca670bc082f1e669b9b433100a0596","f6e5e70860080a69e232d14a98bf20128957b9b5","1b015bee767db7c4aba13e0320b8fb93a0817445","1105aa77a66a3dbaa6916c57eff1f161c51affc0","9591a06a102a2c80159f6734753b96d23aae4b50"],"s2Url":"https://semanticscholar.org/paper/179d59854178accdb617e327c1e79636ea781e5a","s2PdfUrl":"","id":"179d59854178accdb617e327c1e79636ea781e5a","authors":[{"name":"Iulian Brumar","ids":["19269471"]},{"name":"Marc Casas","ids":["2020430"]},{"name":"Miquel Moretó","ids":["2703643"]},{"name":"Mateo Valero","ids":["1741016"]},{"name":"Gurindar S. Sohi","ids":["1754655"]}],"journalName":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","paperAbstract":"Redundant computations appear during the execution of real programs. Multiple factors contribute to these unnecessary computations, such as repetitive inputs and patterns, calling functions with the same parameters or bad programming habits. Compilers minimize non useful code with static analysis. However, redundant execution might be dynamic and there are no current approaches to reduce these inefficiencies. Additionally, many algorithms can be computed with different levels of accuracy. Approximate computing exploits this fact to reduce execution time at the cost of slightly less accurate results. In this case, expert developers determine the desired tradeoff between performance and accuracy for each application. In this paper, we present Approximate Task Memoization (ATM), a novel approach in the runtime system that transparently exploits both dynamic redundancy and approximation at the task granularity of a parallel application. Memoization of previous task executions allows predicting the results of future tasks without having to execute them and without losing accuracy. To further increase performance improvements, the runtime system can memoize similar tasks, which leads to task approximate computing. By defining how to measure task similarity and correctness, we present an adaptive algorithm in the runtime system that automatically decides if task approximation is beneficial or not. When evaluated on a real 8-core processor with applications from different domains (financial analysis, stencil-computation, machine-learning and linear-algebra), ATM achieves a 1.4x average speedup when only applying memoization techniques. When adding task approximation, ATM achieves a 2.5x average speedup with an average 0.7% accuracy loss (maximum of 3.2%).","inCitations":[],"pdfUrls":["http://upcommons.upc.edu/bitstream/handle/2117/107646/ATM+Approximate+Task+Memoization+in+the+Runtime+System.pdf;jsessionid=3B78BDEF7E686ABD5AFC1057A0F20D28?sequence=3","https://doi.org/10.1109/IPDPS.2017.49"],"title":"ATM: Approximate Task Memoization in the Runtime System","doi":"10.1109/IPDPS.2017.49","sources":["DBLP"],"doiUrl":"https://doi.org/10.1109/IPDPS.2017.49","venue":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)"},
{"entities":["512-bit","64-bit computing","Algorithm","Automatic vectorization","Breadth-first search","Central processing unit","Graph (abstract data type)","Graph500","Graphics processing unit","Haswell (microarchitecture)","Knights","List of algorithms","Load balancing (computing)","Manycore processor","Multi-core processor","Nvidia Tesla","SIMD","Sparse matrix","Xeon Phi"],"journalVolume":"","journalPages":"32-41","pmid":"","year":2017,"outCitations":["af6d5dd24498e0ce9aa7cbee8a7f6356079f5dfa","0a791a760dd883342c8b8456a3e7cb75fb996ef4","3e426349f0cf3a65b502be05ebca23e693ec03fd","2984638090457cf02d82715d9834314448efa878","0a7bcfcb0ddc167de4b456504600806e18690d02","259e93de2f10d395a1bdfb2dc6da72b6a3998572","175d795f44037ef60dd9df341701cd5fdc449f1f","4c77e5650e2328390995f3219ec44a4efd803b84","0624ec3adb8d9f785935746534d4041c2e0802dc","40eb1f990ac292b14b56ea06e61d9aeb9bfa28c3","ae18b99bfa8940f7a17b7f77eb7177d953a5d9f5","947c6bf534ccd620044f77c3bd6068f633b421fb","5b975248796c2ee3f65b2f4430fd3be4d7e6191e","0c9a56eb4f45d3969943e8cff74593e9c6c5f549","3983fe131eb3902f9923f35060c56546bbdc951e","47a6a274c648aeb5ff02eb09aff7ea310eae122e","5a3cd8c65ffcc25bef346174d1f0bc3f83c5cbbb","1156f60e40548096df49528b1342bb3e88b0f378","477a2e92d2fd2ca56fd989d42de58248f1ce04ae","3ef02548615246e74b88808af811f1557b57fa75","ce8190de5cac2b583667079502c130888783303f","141e35263ab810983c90d47ad62eb4fab5e51717","189f76a7501666386809bd280ffe2f0c3acd7cb0","a5aad5abb32f6b15f31b92312bb3b0f7b6470977","31181e73befea410e25de462eccd0e74ba8fea0b","17ad1361dfabc1c50b506813d0f5d54df159fc36","87ee99d4cc4e0601cbb519f6ddbac85772bdc49e","d7f449c199ce86d3b8039899caabb31b54ced7f2"],"s2Url":"https://semanticscholar.org/paper/05b493cac86ef358ee4990429aaa1095a1315054","s2PdfUrl":"","id":"05b493cac86ef358ee4990429aaa1095a1315054","authors":[{"name":"Maciej Besta","ids":["2919642"]},{"name":"Florian Marending","ids":["19322066"]},{"name":"Edgar Solomonik","ids":["2880213"]},{"name":"Torsten Hoefler","ids":["1713648"]}],"journalName":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","paperAbstract":"Vectorization and GPUs will profoundly change graph processing. Traditional graph algorithms tuned for 32- or 64-bit based memory accesses will be inefficient on architectures with 512-bit wide (or larger) instruction units that are already present in the Intel Knights Landing (KNL) manycore CPU. Anticipating this shift, we propose SlimSell: a vectorizable graph representation to accelerate Breadth-First Search (BFS) based on sparse-matrix dense-vector (SpMV) products. SlimSell extends and combines the state-of-the-art SIMD-friendly Sell-C-&#x03C3; matrix storage format with tropical, real, boolean, and sel-max semiring operations. The resulting design reduces the necessary storage (by up to 50%) and thus pressure on the memory subsystem. We augment SlimSell with the SlimWork and SlimChunk schemes that reduce the amount of work and improve load balance, further accelerating BFS. We evaluate all the schemes on Intel Haswell multicore CPUs, the state-of-the-art Intel Xeon Phi KNL manycore CPUs, and NVIDIA Tesla GPUs. Our experiments indicate which semiring offers highest speedups for BFS and illustrate that SlimSell accelerates a tuned Graph500 BFS code by up to 33%. This work shows that vectorization can secure high-performance in BFS based on SpMV products; the proposed principles and designs can be extended to other graph algorithms.","inCitations":["232e641a8b5f550c436af6336ee63e1cd771e073","37050d37c793f4eee3874840fa60a58ca03c3fb0"],"pdfUrls":["https://doi.org/10.1109/IPDPS.2017.93","https://htor.inf.ethz.ch/publications/img/slimsell.pdf","https://people.csail.mit.edu/jshun/6886-s18/papers/BMSH17.pdf"],"title":"SlimSell: A Vectorizable Graph Representation for Breadth-First Search","doi":"10.1109/IPDPS.2017.93","sources":["DBLP"],"doiUrl":"https://doi.org/10.1109/IPDPS.2017.93","venue":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)"},
{"entities":["AV-TEST","Lookup table","Message Passing Interface","Network address"],"journalVolume":"","journalPages":"1008-1017","pmid":"","year":2017,"outCitations":["46d4192ee0506779b311ecb5a0229737acd1d09e","5b16adaf3a0be648032d0996743575c0d939ad1b","21cf16cec9af3a62d73bf5fb811a528694031e0c","99a1520bc334c111ff84619a1ac376f009d0d3bf","a7ee6fa73b34b7e03808dd06b3d5482d5410fa1f","2faf6e0b2b08be9a3ab46d6e932e2c642b882195","5652436b860413016238f4e54589726bde1e99ad"],"s2Url":"https://semanticscholar.org/paper/309bd4c9b1b9cf81cbf071b8b2ad80e97acf7c60","s2PdfUrl":"","id":"309bd4c9b1b9cf81cbf071b8b2ad80e97acf7c60","authors":[{"name":"Yanfei Guo","ids":["1794267"]},{"name":"Charles J. Archer","ids":["2312753"]},{"name":"Michael Blocksome","ids":["1685720"]},{"name":"Scott Parker","ids":["2032331"]},{"name":"Wesley Bland","ids":["2446648"]},{"name":"Kenneth Raffenetti","ids":["2673895"]},{"name":"Pavan Balaji","ids":["2103230"]}],"journalName":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","paperAbstract":"MPI allows applications to treat processes as a logical collection of integer ranks for each MPI communicator, while internally translating these logical ranks into actual network addresses. In current MPI implementations the management and lookup of such network addresses use memory sizes that are proportional to the number of processes in each communicator. In this paper, we propose a new mechanism, called AV-Rankmap, for managing such translation. AV-Rankmap takes advantage of logical patterns in rank-address mapping that most applications naturally tend to have, and it exploits the fact that some parts of network address structures are naturally more performance critical than others. It uses this information to compress the memory used for network address management. We demonstrate that AV-Rankmap can achieve performance similar to or better than that of other MPI implementations while using significantly less memory.","inCitations":["478c09086fcdb2bdaf5f48542dee3e3267790d0f","6b8cc1c8358b84a55ec2858910adf839928370ef"],"pdfUrls":["http://www.mcs.anl.gov/papers/P6078-1016.pdf","http://www.mcs.anl.gov/~yguo/pubs/ANL-MCS-P6078-1016.pdf","http://www.mcs.anl.gov/papers/P6051-0916.pdf","https://doi.org/10.1109/IPDPS.2017.18"],"title":"Memory Compression Techniques for Network Address Management in MPI","doi":"10.1109/IPDPS.2017.18","sources":["DBLP"],"doiUrl":"https://doi.org/10.1109/IPDPS.2017.18","venue":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)"},
{"entities":["Attribute–value pair","Buffer overflow","Burst mode (computing)","Cluster analysis","Experiment","Key-value database","Scalability","Value (ethics)"],"journalVolume":"","journalPages":"1174-1183","pmid":"","year":2017,"outCitations":["7717cb7fbbf26557238c2ef847d0a48def176d0b","478cd69ab5de77a7dc8d2419e49b14a8cac82e73","0c3a060886185b59322bbd1296e08a63d84d2ad8","589e89d77f689ebfc3f36bc1f76fd518ae4a237c","4fbe8c8ace7546e3a10bfd8e151bc09a41fd3f9a","44607270754f8521d6c4d42297aa881393f4f8e0","483b2f4c7dbc72f7969b60cff0984f2062f02956","28f13ebe8e17fdb4c2500c515759a3ee0c2783ce","0973e45d3eeb9641d3de34d48f8d0432f1113dcf","3492873a8bc6d1d501dcac97e891c43dfecc29c0","0f55217987ec25afa0f815e0aa3957e669b0280e","569de2eececd3adb7219d63eb85e4bdc63486c42","11c136aa1136ccf6ebbb23c3b3e1fbdd8447bb00","5d25b4a77268437aa669e272cc81b56ed184e0b6","3288d37f1929d15a26da7a5c09a89258b5d4366c","aeae0567deda241a5a2aeb992f41e68089e58030","a8b429845ac951b0fe6cdb071ae862c7d305e36e","9028fe4e30d51bfdf494a36d2b8c4bcfb10cfe8a","1fcb1c5595b4518b0e8bab042f32605c367588c2","3609a17555a6c6757f8ff0297fc046e6dc623a57","00fd1e42e5c5ce6ab7fb9c7eb7952ab3ae642de5","04b3aaf58a91557e15c8064660baa1cc5e8db14e","721c5be47c923d9c0303a3eefd3d42a57e0add03","2d2255446fa2c5d5e96c4635ba75ca1741c82f7e","337d5988addf1cf6db2233ef357b19000c7b8616"],"s2Url":"https://semanticscholar.org/paper/8cd63388eb68ede942d27644c7ef629e358764da","s2PdfUrl":"","id":"8cd63388eb68ede942d27644c7ef629e358764da","authors":[{"name":"Teng Wang","ids":["1783874"]},{"name":"Adam Moody","ids":["2216937"]},{"name":"Yue Zhu","ids":["2606966"]},{"name":"Kathryn Mohror","ids":["3270933"]},{"name":"Kento Sato","ids":["2605713"]},{"name":"Tanzima Zerin Islam","ids":["2219526"]},{"name":"Weikuan Yu","ids":["1709886"]}],"journalName":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","paperAbstract":"Distributed burst buffers are a promising storage architecture for handling I/O workloads for exascale computing. Their aggregate storage bandwidth grows linearly with system node count. However, although scientific applications can achieve scalable write bandwidth by having each process write to its node-local burst buffer, metadata challenges remain formidable, especially for files shared across many processes. This is due to the need to track and organize file segments across the distributed burst buffers in a global index. Because this global index can be accessed concurrently by thousands or more processes in a scientific application, the scalability of metadata management is a severe performance-limiting factor. In this paper, we propose MetaKV: a key-value store that provides fast and scalable metadata management for HPC metadata workloads on distributed burst buffers. MetaKV complements the functionality of an existing key-value store with specialized metadata services that efficiently handle bursty and concurrent metadata workloads: compressed storage management, supervised block clustering, and log-ring based collective message reduction. Our experiments demonstrate that MetaKV outperforms the state-of-the-art key-value stores by a significant margin. It improves put and get metadata operations by as much as 2.66&#xd7; and 6.29&#xd7;, respectively, and the benefits of MetaKV increase with increasing metadata workload demand.","inCitations":["788bdfe4dbd2228dca0f7ef48eda469af3cb1347","d09b94ca965f63d1687cff8437da6aae7f7b7005"],"pdfUrls":["https://doi.org/10.1109/IPDPS.2017.39"],"title":"MetaKV: A Key-Value Store for Metadata Management of Distributed Burst Buffers","doi":"10.1109/IPDPS.2017.39","sources":["DBLP"],"doiUrl":"https://doi.org/10.1109/IPDPS.2017.39","venue":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)"},
{"entities":["Algorithm","Computer data storage","Data compression","Data point","Encoder","Lossy compression","Mean squared error","NetBSD Gzip / FreeBSD Gzip","Peak signal-to-noise ratio","Variable-length code"],"journalVolume":"","journalPages":"1129-1139","pmid":"","year":2017,"outCitations":["4d3b26551b351ce690d3ce6d323a9e98c8d039ef","4d0b12bef1f99d117f43d46de35682b320a4b9fe","7777d299e7b4217fc4b80234994b5a68b3031199","6e5ad4686eb73092b7a2d532506fa61b5a796d29","09b71bc8d83e2583319b5bd42838e6c4ffa0bd70","39d2b916719a48d262690f57aab91c56aa2ab72c","44607270754f8521d6c4d42297aa881393f4f8e0","9c0b153ea0b741107301c2a13dc0e0f2f92c863c","f3ee81bed49c66cea802f23bfdac4ba23418a305","81ad3cc01deab05be0bf9add2d08e043f28cd55c","0484496ff92137f89796b87766e19c6378ee3e76","093fc19d440f33247e545ec6c047e0aa0afb0863","49c8f4db70b9446da52a9250db26e53e88fc1605"],"s2Url":"https://semanticscholar.org/paper/84f7ef8aa654ccb46244dde02c2bb705d6abb484","s2PdfUrl":"","id":"84f7ef8aa654ccb46244dde02c2bb705d6abb484","authors":[{"name":"Dingwen Tao","ids":["3058378"]},{"name":"Sheng Di","ids":["1699598"]},{"name":"Zizhong Chen","ids":["1756221"]},{"name":"Franck Cappello","ids":["1721552"]}],"journalName":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","paperAbstract":"Today's HPC applications are producing extremely large amounts of data, such that data storage and analysis are becoming more challenging for scientific research. In this work, we design a new error-controlled lossy compression algorithm for large-scale scientific data. Our key contribution is significantly improving the prediction hitting rate (or prediction accuracy) for each data point based on its nearby data values along multiple dimensions. We derive a series of multilayer prediction formulas and their unified formula in the context of data compression. One serious challenge is that the data prediction has to be performed based on the preceding decompressed values during the compression in order to guarantee the error bounds, which may degrade the prediction accuracy in turn. We explore the best layer for the prediction by considering the impact of compression errors on the prediction accuracy. Moreover, we propose an adaptive error-controlled quantization encoder, which can further improve the prediction hitting rate considerably. The data size can be reduced significantly after performing the variable-length encoding because of the uneven distribution produced by our quantization encoder. We evaluate the new compressor on production scientific data sets and compare it with many other state-of-the-art compressors: GZIP, FPZIP, ZFP, SZ-1.1, and ISABELA. Experiments show that our compressor is the best in class, especially with regard to compression factors (or bit-rates) and compression errors (including RMSE, NRMSE, and PSNR). Our solution is better than the second-best solution by more than a 2x increase in the compression factor and 3.8x reduction in the normalized root mean squared error on average, with reasonable error bounds and user-desired bit-rates.","inCitations":["2f84f0296b1151bd7062431d34673babbe97b1af","038aad8409d8d00d82cc96da6b571de7a93a7f4a","042381199e946fa918bb8354210c68c477fe7bf0","d6f159da21bc00dc57bb6994ac3e70f0df698413","ffa491eb990b98a2bbf444b85229dc83a62647c4","953a64af385e2da82f0af8661c42c0224666b45a","20554f3028a9641e0796506b1501cb57681ef1e0","8ce594615e0f448e9b6b498b56b9e48924bca238"],"pdfUrls":["http://arxiv.org/abs/1706.03791","https://arxiv.org/pdf/1706.03791v1.pdf","https://doi.org/10.1109/IPDPS.2017.115"],"title":"Significantly Improving Lossy Compression for Scientific Data Sets Based on Multidimensional Prediction and Error-Controlled Quantization","doi":"10.1109/IPDPS.2017.115","sources":["DBLP"],"doiUrl":"https://doi.org/10.1109/IPDPS.2017.115","venue":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)"},
{"entities":["Algorithm","BLAS","Code generation (compiler)","Coppersmith–Winograd algorithm","Data parallelism","Fast multipole method","Library","Matrix multiplication","Multi-core processor","Parallel computing","Strassen algorithm","Task parallelism","The Matrix","Workspace"],"journalVolume":"","journalPages":"656-667","pmid":"","year":2017,"outCitations":["74d231ca09c7106bdbf7e1ff2852fbbc7bd96c67","3a953ae2b531db881357fa7066b0b17a35708e3f","012d5e0c031f2fa6db6b77dbc29cabbd95ac2854","a0d3306999eacc7fab93955eb1223eef10312708","40fb5ea197206082b0b77f388c57bca79536c877","2493744d70261082eb6eafd4b13f14e8a8f8eb20","0c3bde4b6d8c763c0ec4083c9cda059eff87308e","70e8a6ff0cdad8a798ae552c6259a1fce06ea717","3ac8d40c90e52c96c31e2ab6a485eb6a06070f9f","114f158f3b7b37614d5d83efe33c1e73c051c7c1","d01e00939c1773366237e744ff0047fc55a53453","004eda59c0ffceb2417bee87c95539eae4bdf0cd","34eb32537b3f9dacbbd1567e1ce620c66e51d3c6","05dcdb5f4876b07c20c2c46df156c248f6779a11","9e253ee24f32eb6dfb918156adaa45622aad88b9","3269b04305dd2bd4ecbb2daea4429eeb523cc164","8eaa45df0a85bf7fda455cf7f1699cdfe0de1288","6b2a23349099f95c1c4850bb1d4731612a7046e6","355e35184d084abc712c5bfcceafc0fdfe78ceef"],"s2Url":"https://semanticscholar.org/paper/56e8215d410f575561778299765eab5227a9bfd6","s2PdfUrl":"","id":"56e8215d410f575561778299765eab5227a9bfd6","authors":[{"name":"Jianyu Huang","ids":["1914477"]},{"name":"Leslie Rice","ids":["39967103"]},{"name":"Devin A. Matthews","ids":["1849855"]},{"name":"Robert A. van de Geijn","ids":["9151878"]}],"journalName":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","paperAbstract":"Matrix multiplication (GEMM) is a core operation to numerous scientific applications. Traditional implementations of Strassen-like fast matrix multiplication (FMM) algorithms often do not perform well except for very large matrix sizes, due to the increased cost of memory movement, which is particularly noticeable for non-square matrices. Such implementations also require considerable workspace and modifications to the standard BLAS interface. We propose a code generator framework to automatically implement a large family of FMM algorithms suitable for multiplications of arbitrary matrix sizes and shapes. By representing FMM with a triple of matrices [U, V, W] that capture the linear combinations of submatrices that are formed, we can use the Kronecker product to define a multi-level representation of Strassen-like algorithms. Incorporating the matrix additions that must be performed for Strassen-like algorithms into the inherent packing and micro-kernel operations inside GEMM avoids extra workspace and reduces the cost of memory movement. Adopting the same loop structures as high-performance GEMM implementations allows parallelization of all FMM algorithms with simple but efficient data parallelism without the overhead of task parallelism. We present a simple performance model for general FMM algorithms and compare actual performance of 20+ FMM algorithms to modeled predictions. Our implementations demonstrate a performance benefit over conventional GEMM on single core and multi-core systems. This study shows that Strassen-like fast matrix multiplication can be incorporated into libraries for practical use.","inCitations":["73c2882e65286f209521f75ab4410c22e220e564","1cc406e388203f841adfd712df0be059edda274e","2ef3fb64a368db97e0da4b670cf3fab95b895fde"],"pdfUrls":["http://www.cs.utexas.edu/~jianyu/presentations/fmm_ipdps17.pdf","https://arxiv.org/pdf/1611.01120v1.pdf","http://www.cs.utexas.edu/~jianyu/papers/ipdps17.pdf","https://doi.org/10.1109/IPDPS.2017.56","https://apps.cs.utexas.edu/apps/sites/default/files/tech_reports/FMM.pdf","http://arxiv.org/abs/1611.01120"],"title":"Generating Families of Practical Fast Matrix Multiplication Algorithms","doi":"10.1109/IPDPS.2017.56","sources":["DBLP"],"doiUrl":"https://doi.org/10.1109/IPDPS.2017.56","venue":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)"},
{"entities":["Apache Hadoop","Apache Spark","Sampling (signal processing)","Simulation","Social media","Social simulation"],"journalVolume":"","journalPages":"595-604","pmid":"","year":2017,"outCitations":["43e2dd9cacfc25761d95fa9fb2a0f28aa768efc8","5d405f87571807066aed687baed4d4a3c2a85172","6b406760f69fc0e4b8412ea4b864c345a9540b3b","24281c886cd9339fe2fc5881faf5ed72b731a03e","0e968f665c764a5bb636bb817e0a8b85762ba206","17d72a792b35fa9e65499b3e8f9c15a1f6f8b7b6","6b1146e51b2c4d5b8433d4d9c6dbf87c6c484196","79ff6d26643770fecefe08d7bf1ec504ae465bc8","eecbaaaab769c62e8c24d98b9bc7ca955b41fbda","bb6cedd67b26fce1f0d8eacb0357658c6831586d","a81c73e2e277f290bdf4dc2b0e34a61a2920afc8","82adcea5d233d776b78224998984e7adf2268fe0","8009b1c8cc4af8d3d4b792ac32926487a428172e","1ae0a2b3677b5b99441b4829a94e4577d6786de5","dd61bd4a3a147663df5a6be5ba7e7473dd3fc960","2cdcddb08ae6060e94cba6c9b2b58b87324e686f","38e31e68af9b260c51d5abc03b27041780e81e4b","0c4867f11c9758014d591381d8b397a1d38b04a7","40eb1f990ac292b14b56ea06e61d9aeb9bfa28c3","3ed84f2fbdc4dc6450919ec5b017e66440a5833c","1be5ca1c9a94bd29c64d358e677b699e16c58f55","10aa9ee7caaf9381b6a0468ae899a9729824a6b7","313b6d6a2fe071869507ba7530aef10c91aefe11","2c545dc62362253220285bf521fdf73c0eeba975","f29dac2e26273532c81c933f091c7a60b9480f94"],"s2Url":"https://semanticscholar.org/paper/8346f424b4d07919e81581f405a983cda43a8de5","s2PdfUrl":"","id":"8346f424b4d07919e81581f405a983cda43a8de5","authors":[{"name":"Jen-Cheng Huang","ids":["40429127"]},{"name":"Lifeng Nai","ids":["2144577"]},{"name":"Pranith Kumar","ids":["39708396"]},{"name":"Hyojong Kim","ids":["3194681"]},{"name":"Hyesoon Kim","ids":["8187053"]}],"journalName":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","paperAbstract":"Today, there is a steep rise in the amount of data being collected from diverse applications. Consequently, data analytic workloads are gaining popularity to gain insight that can benefit the application, e.g., financial trading, social media analysis. To study the architectural behavior of the workloads, architectural simulation is one of the most common approaches. However, because of the long-running nature of the workloads, it is not trivial to identify which parts of the analysis to simulate. In the current work, we introduce SimProf, a sampling framework for data analytic workloads. Using this tool, we are able to select representative simulation points based on the phase behavior of the analysis at a method level granularity. This provides a better understanding of the simulation point and also reduces the simulation time for different input sets. We present the framework for Apache Hadoop and Apache Spark frameworks, which can be easily extended to other data analytic workloads.","inCitations":[],"pdfUrls":["https://doi.org/10.1109/IPDPS.2017.118"],"title":"SimProf: A Sampling Framework for Data Analytic Workloads","doi":"10.1109/IPDPS.2017.118","sources":["DBLP"],"doiUrl":"https://doi.org/10.1109/IPDPS.2017.118","venue":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)"},
{"entities":["Apache Hadoop","Data redundancy","Jumpstart Our Business Startups Act","Load balancing (computing)","MapReduce","Name binding"],"journalVolume":"","journalPages":"1078-1087","pmid":"","year":2017,"outCitations":["257adee470c54280da48d448a064b35537d51fbd","3d658c5c758b2a567c7d9150759b2b6d6ddda50b","4a0bb4eece00f3e9445d1a0d933422aa408ce8d1","0ea4380ff8bb30e6bd5fd888268d6f8f38229fb7","a2bf3581cf421211c4cafdcb295610b7aa7e8991","0a12a179bebdf4bb69d692a1127795b3f536270b","c737aa8b2c916fe1f13a6fd4e847fa45da1e5434","67a16b2945c33eabd17eb314c58c8e3eb7d2334d","52bfb3aa30ec06784d839ab431287a657d0d7907","6973083bca583e26a0d8e7709ce7b9888cf3ee69","78f246756811e924825a03909952d2c32c593a52","0541d5338adc48276b3b8cd3a141d799e2d40150","443b8c56d7300f61b825d1dbafe06afdda23c3e1","090030e0d1aa117008e9e9fa4abdee0a95455f4a","70bd563d00fcb402eb7d9f251bea544ecb08f213","ae24289a0ed3152de528f863c96279382b14ae61","2f88dcf1e9abaa0b0f8c63820548c98b2da61220","d65f897b7cea2761f88411e757e9587c0282cb41","24281c886cd9339fe2fc5881faf5ed72b731a03e","1f9d47906319d0a8fac5c5fdbadf98e9da7966f9","0867e38682f5cdbcfb53588ab1315f4cc8595582"],"s2Url":"https://semanticscholar.org/paper/7bbd1c3872e679cf4ea8ec2e57dc17506af8c291","s2PdfUrl":"","id":"7bbd1c3872e679cf4ea8ec2e57dc17506af8c291","authors":[{"name":"Wei Chen","ids":["1728624"]},{"name":"Jia Rao","ids":["1786877"]},{"name":"Xiaobo Zhou","ids":["1718639"]}],"journalName":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","paperAbstract":"MapReduce applications, which require access to a large number of computing nodes, are commonly deployed in heterogeneous environments. The performance discrepancy between individual nodes in a heterogeneous cluster present significant challenges to attain good performance in MapReduce jobs. MapReduce implementations designed and optimized for homogeneous environments perform poorly on heterogeneous clusters. We attribute suboptimal performance in heterogeneous clusters to significant load imbalance between map tasks. We identify two MapReduce designs that hinder load balancing: (1) static binding between mappers and their data makes it difficult to exploit data redundancy for load balancing; (2) uniform map sizes is not optimal for nodes with heterogeneous performance. To address these issues, we propose FlexMap, a user-transparent approach that dynamically provisions map tasks to match distinct machine capacity in heterogeneous environments. We implemented FlexMap in Hadoop-2.6.0. Experimental results show that it reduces job completion time by as much as 40% compared to stock Hadoop and 30% to SkewTune.","inCitations":["98ac6ab21def7f6e1b80c0be62ccfc0b594bfc57"],"pdfUrls":["https://doi.org/10.1109/IPDPS.2017.28"],"title":"Addressing Performance Heterogeneity in MapReduce Clusters with Elastic Tasks","doi":"10.1109/IPDPS.2017.28","sources":["DBLP"],"doiUrl":"https://doi.org/10.1109/IPDPS.2017.28","venue":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)"},
{"entities":["Computational chemistry","Defense Distributed","Electronic structure","Parallel computing","Perturbation theory","Runtime system","Scalability","Sparse matrix"],"journalVolume":"","journalPages":"555-564","pmid":"","year":2017,"outCitations":["e7f981b829c2febd381a7e90b8b8c85a5f5c4777","2706db42926e0e58e35336331f6d3b62f0811cf5","68d80dacf66e2ea556bcdbb4f4792efeeda2122f","2356b24bae9cd3fc3b42686b129d6e1fb5cc48d9","a0f60261be52d1467797e8586aadea1986d4044f","ed632cc68d6087210cdeb6a00317bc5032544e41","19df5d05b6da98f99619ee4584c5177bd02c8a2a","a1dd203c6159a1eddaa2d7cc104d4c06a7ffffa4","0c1ef9519c3dcd4a309650bc24e5d5f906e369ed","a3bec8c3dd2ac915675db13dd1d64f53588e7aca","9eaf2d5af34f9a284902d9fb0c1f8b187b2bc3fc","a814615952a917dcb0b7fb5e22285b62fbe3d6cc"],"s2Url":"https://semanticscholar.org/paper/69f9a690eee4081b07244c251328e7d1ac19d4c7","s2PdfUrl":"","id":"69f9a690eee4081b07244c251328e7d1ac19d4c7","authors":[{"name":"Beverly A. Sanders","ids":["2932353"]},{"name":"Jason N. Byrd","ids":["7301435"]},{"name":"Nakul Jindal","ids":["2712027"]},{"name":"Victor F. Lotrich","ids":["34851048"]},{"name":"Dmitry Lyakh","ids":["31938628"]},{"name":"Ajith Perera","ids":["36824435"]},{"name":"Rodney J. Bartlett","ids":["2774852"]}],"journalName":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","paperAbstract":"Aces4 is a parallel programming platform comprising a DSL for Computational Chemistry and its runtime system. It offers a convenient way to express parallelism together with extensive support for extremely large, possibly sparse, distributed arrays. It aids scientists in the creation of performant, scalable, massively parallel programs that can effectively take advantage of leadership class computing systems to address important scientific questions. Aces4 has enabled the development and implementation of new methods in electronic structure theory which are breaking new ground in their ability to perform highly accurate calculations on ever larger molecular systems. In this paper the design of Aces4, which is based on the the Super Instruction Architecture approach, is described. Experimental scaling results for Molecular Cluster Perturbation Theory, a new method enabled by Aces4, and CCSD, a widely used computational chemistry method are given.","inCitations":[],"pdfUrls":["https://doi.org/10.1109/IPDPS.2017.108"],"title":"Aces4: A Platform for Computational Chemistry Calculations with Extremely Large Block-Sparse Arrays","doi":"10.1109/IPDPS.2017.108","sources":["DBLP"],"doiUrl":"https://doi.org/10.1109/IPDPS.2017.108","venue":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)"},
{"entities":["Address space","Algorithm","Correctness (computer science)","Data access","FastTrack","Intrusion detection system","Line Printer Daemon protocol","Locality of reference","Lock (computer science)","Multi-core processor","Multiprocessing","Principle of locality","Race condition","Symmetric multiprocessing","Thread (computing)"],"journalVolume":"","journalPages":"387-396","pmid":"","year":2017,"outCitations":["0180ce1c6e98fd66362b46bd945c503bf7372aa8","54f3331b575b2d451c2d716f86496cada23d596d","0653e2ed9f683868cb4539eb8718551242834f6b","059697e0824d06a43321a9f9d7450da9cc4dc0a8","a45adba59080ad625e3005c669345c3a96ad3e18","0e578433d4e8bb2a571c87a2d22816074902f009","2346439ece014d5e3ce1564adc2a7ca098a37c8e","8c2b2fb1d4c44d1e1b63be4e5ef3bbb8d37dbfb5","182c0524aa353b6f4f4cb75a88ff3f5fc3bd86e0","d3630df8179b704a516b6ed01cfcad24ae50ade7","8b28b02af1ba77fff5b08d6dea87ba8b043b479d","711b89b078ceb7722406c719a6ac1316ade61daf","5bc7a761cb77abe5aa964191d501385198b7f79d","7905c7e24bbd5c987dca90dc690e8a11ed4d122f","0958a63d9c6238b38377f076b487c413bc8642c1","131672edb4fa10458d1b5d9d047dde18f33d997f","05a618847e4f08e5bca29dff732757779722b2e0","771e3c7146213802ca8c4db0afbde51606293a71","8b662bf3287f779f9f5d367e7b34d2fe1e3efde2","968f8a1d37e7ae479c2534a29d0d9d9225134605","86ed165adcfd254b511ff1bbb912cad65d45f0d6","60a45695845e3f1e5dd8d7a886b23fff89c295bc","44808fd8f2ffd19bb266708b8de835c28f5b8596","5b89866789a58f374a4ae83c555dd20e67e80ac5"],"s2Url":"https://semanticscholar.org/paper/9e1d75ff8ada7ba76b640359590acca7ec762b5d","s2PdfUrl":"","id":"9e1d75ff8ada7ba76b640359590acca7ec762b5d","authors":[{"name":"Young Wn Song","ids":["3278547"]},{"name":"Yann-Hang Lee","ids":["1721119"]}],"journalName":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","paperAbstract":"Detecting data races in multithreaded programs is critical to ensure the correctness of the programs. To discover data races precisely without false alarms, dynamic detection approaches are often applied. However, the overhead of the existing dynamic detection approaches, even with recent innovations, is still substantially high. In this paper, we present a simple but efficient approach to parallelize data race detection in multicore SMP (Symmetric Multiprocessing) machines. In our approach, data access information needed for dynamic detection is collected at application threads and passed to de-tection threads. The access information is distributed in a way that the operation performed by each detection thread is inde-pendent of that of other detection threads. As a consequence, the overhead caused by locking operations in data race detection can be alleviated and multiple cores can be fully utilized to speed up and scale up the detection. Furthermore, each detection thread deals with only its own assigned memory access region rather than the whole address space. The executions of detection threads can exploit the spatial locality of accesses leading to an improved cache performance. We have applied our parallel approach on the FastTrack algorithm and demon-strated the validity of our approach on an Intel Xeon machine. Our experimental results show that the parallel FastTrack detector, on average, runs 2.2 times faster than the original FastTrack detector on the 8 core machine.","inCitations":[],"pdfUrls":["https://doi.org/10.1109/IPDPS.2017.87"],"title":"A Parallel FastTrack Data Race Detector on Multi-core Systems","doi":"10.1109/IPDPS.2017.87","sources":["DBLP"],"doiUrl":"https://doi.org/10.1109/IPDPS.2017.87","venue":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)"},
{"entities":["Byte","Cache (computing)","Hamming distance","Magnetoresistive random-access memory","Multi-core processor","Non-volatile memory","Overhead (computing)","Performance Evaluation","Pseudo-LRU","Random-access memory","Spectral leakage","Volatility"],"journalVolume":"","journalPages":"92-101","pmid":"","year":2017,"outCitations":["bbd677f51628791eb44d64fb9744ea0e610c357b","a95436fb5417f16497d90cd2aeb11a0e2873f55f","69f7a1499a9b24caeaa586de5ff9737c04fd0b89","17a0a008a276daee5ce7a38b60cac964dea57da9","05015f9db9c040d76d026deb4dd2f82ce275cd91","5f852bfcf28e6a84723567bd40f247c0a8e7638e","a2f3bb40653499eeb33babacf69579b5ea9d20e1","df1ed68dba0407cf2d93736af8cfd2dc5cf86918","2960c89331eb7afa86584792e2e11dbf6a125820","61d16d80cd5e7f79f25785a462ee752d24e3b414","5a893d8cab79cf43a1d225f5beaae54cbae13235","12bc20a1963859e9f76afb4b308b90ded1cff1fe","71c2deb5c3b4b0fd1ed68bdda534ec7ea76e845b","7e2a21fb9f63c91c2974ca3d6c74d8c1ee89c228","a1e4f4ae16c5a18896fe1718acfe56a26aeca620","3e74ae88cdaa33bf89136800258bde97ab397ec9","45f92febffdc46540a3cae433a7b4ef48c029a50","7ef0940a5e093a7c8c3c7d243bbbbf513b3c3192","40b65be7d6e7cae7d530910220182df914103a04","0b5e5a2516a49997feea686c434580d9058fd1aa","3871446c86963903b087c1616bb1a0887a63f234","dd4f901d0e692a4cc17741fc3479a661432b2824","4654615ee9187ee1bb784feb6175a47a726d813d","1600c3ed12301b06a1107a68c2de84fb3582a918","7cd29ed1da71593bfb79b553ba6c5ee39ccf7a7b","40eb2f5a97298da40838388700b097f82adff167","5d999f4a5567e6f4a54e46bbcd6006f75ab0cbac","d1f4ff21631dc8ac85dd39516e22d5e187cd9d5e","afd4a9332cb43854b513ebba6ff17a79c388824b"],"s2Url":"https://semanticscholar.org/paper/002fe9a7c5f0522279974faa4eeadc70838eb862","s2PdfUrl":"","id":"002fe9a7c5f0522279974faa4eeadc70838eb862","authors":[{"name":"Qi Zeng","ids":["40163311"]},{"name":"Jih-Kwon Peir","ids":["1759383"]}],"journalName":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","paperAbstract":"Spin-Transfer Torque Magnetoresistive Random-Access Memory (STT-MRAM) is a promising memory technology, which has high density, fast read speed, low leakage power, and non-volatility, and is suitable for multi-core on-chip last-level caches. However, the high write energy and latency, as well as less-than-desirable write endurance of STT-MRAM remain challenges. This paper proposes a new encoded content-aware cache replacement policy to reduce the total switch bits for write, lower the write energy, and improve write endurance. Instead of replacing the LRU block under the conventional pseudo-LRU replacement policy, we select a replacement block near the LRU position, which has the most similar content to the missed block. The selected replacement block can reduce the switch bits without damaging the cache performance. To avoid fetching and comparing the entire block contents, we present a novel content encoding method to encode 64-byte block using just 8 bits, each bit represents 8-byte content. The encoded bit is determined by the presence of a dominant bit value in the 8 bytes. We measure the content similarity using the Hamming distance between the encoded bits of the missed block and the replaced block. Performance evaluation demonstrates that the proposed simple content encoding method is effective with an average of 20.5% reduction in total switch bits, which results in improvement on write endurance and less write energy consumption. These improvements are accomplished with low overhead and minimum impact on the cache performance.","inCitations":["fbb6409694d23f2322738cc9247735c55959626a"],"pdfUrls":["https://doi.org/10.1109/IPDPS.2017.103"],"title":"Content-Aware Non-Volatile Cache Replacement","doi":"10.1109/IPDPS.2017.103","sources":["DBLP"],"doiUrl":"https://doi.org/10.1109/IPDPS.2017.103","venue":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)"},
{"entities":["Algorithm","Application framework","Context-free grammar","FIFO (computing and electronics)","High-throughput computing","Multi-core processor","Non-blocking algorithm","Operating system","Scalability","Thread (computing)","Throughput"],"journalVolume":"","journalPages":"907-916","pmid":"","year":2017,"outCitations":["fae286bb9f154cb9009ef24abfaa7529e079b466","363b85f61630ebdc1194a59816ad950bf305c40a","026846d1c3a79a6ad6067007b0eac8922502550c","045a975c1753724b3a0780673ee92b37b9827be6","13d660826130d12d696b86e79052191c6d3b1a18","6db9bd41b294a7b45792b8f4ac8864f5d178f35e","68df322f2263ba1e32050beea657b108c49de8ae","1c5c8e567439e46feff03981b47fc5ba7ceb44d8","ec79422e0bfdb61d8b6d2a6ec5b2dfbcab970852","523b2e438d43364b6e70bb3a97e395aff5488113","52ac2f1620687a9070c6c3354c30343c3de80671","51a745c63f2e551488924a20650c5179d0332dfd","0a289fd7b14345822b1acda6d82750b15d59663e","bf104ebfbd44924b6b7602e48b0a74e987baaca8","37a1e8411669e29cf8fbf48ec920c97c0066ac7e","2d0fa88a9644cca92730869bf8ba8ce1b86f7dbd","33da45838d0b6c082cc71e603fd802bac4d56713","1c74f84dcfaaa317a82708ad30f395a893dbb9c6"],"s2Url":"https://semanticscholar.org/paper/44b92a386c4c30fa9de99bc30abadd9d04007b0e","s2PdfUrl":"","id":"44b92a386c4c30fa9de99bc30abadd9d04007b0e","authors":[{"name":"Sergei Arnautov","ids":["1873477"]},{"name":"Pascal Felber","ids":["1743906"]},{"name":"Christof Fetzer","ids":["2314032"]},{"name":"Bohdan Trach","ids":["7612177"]}],"journalName":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","paperAbstract":"With the spreading of multi-core architectures, operating systems and applications are becoming increasingly more concurrent and their scalability is often limited by the primitives used to synchronize the different hardware threads. In this paper, we address the problem of how to optimize the throughput of a system with multiple producer and consumer threads. Such applications typically synchronize their threads via multi-producer/multi-consumer FIFO queues, but existing solutions have poor scalability, as we could observe when designing a secure application framework that requires high-throughput communication between many concurrent threads. In our target system, however, the items enqueued by different producers do not necessarily need to be FIFO ordered. Hence, we propose a fast FIFO queue, FFQ, that aims at maximizing throughput by specializing the algorithm for single-producer/multiple-consumer settings: each producer has its own queue from which multiple consumers can concurrently dequeue. Furthermore, while we provide a wait-free interface for producers, we limit ourselves to lock-free consumers to eliminate the need for helping. We also propose a multi-producer variant to show which synchronization operations we were able to remove by focusing on a single producer variant. Our evaluation analyses the performance using micro-benchmarks and compares our results with other state-of-the-art solutions: FFQ exhibits excellent performance and scalability.","inCitations":[],"pdfUrls":["https://doi.org/10.1109/IPDPS.2017.41"],"title":"FFQ: A Fast Single-Producer/Multiple-Consumer Concurrent FIFO Queue","doi":"10.1109/IPDPS.2017.41","sources":["DBLP"],"doiUrl":"https://doi.org/10.1109/IPDPS.2017.41","venue":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)"},
{"entities":["Cloud computing","Computation","Coprocessor","Cryptography","Library","Montgomery modular multiplication","OpenSSL","Public-key cryptography","RSA (cryptosystem)","SIMD","Symmetric-key algorithm","Thread (computing)","Throughput","Transport Layer Security","Xeon Phi"],"journalVolume":"","journalPages":"565-574","pmid":"","year":2017,"outCitations":["79234d33b0d53ad6bd507949806a96acaece3a3b","31c05ee157fb7cd55c48e9363a44c64a298602db","1892133530acab096b4ea4bf281377c3eec760b9","1f527fed31971e07093695c128c10b4f3c20d109","047edaacc9f2f107e747dad735b85b83c19246df","3f4b5703f44970649551c96c6891465339e78ee4","b725016a4e7b7fa0cb4b334f6e185c3479fd3b9b","8da6ba33d9b392024f03b92a2bfe963f7dd402a8","6b9f7f1e8a602ff83126d087c5a08aa9c8c12f16","248e7c9b9f60868f95accdd2fe90053edd84ce6c","9a59ec7a2153a9b87c384f9d32a7b0b1d2d436cf","9a97d2ff61a2bbe6d72e32633d7bdb1750dfe31b","92376650e204612f54ae9023bcb748e38f2852ae","490f3679089cec9b848b49c11e841b41fda9df27","6debd9d773c7aca19f18f3b4640c45f8ae12b254","0f9b2e598ee1ddde4fd5a2f3008a6983367cc22c","22ed33b9108ee432bf2155d5b474f70960fe3d3f","6f7afc7542e18a0956c6387322a79249ff886e22","6b69f7503ae2d1222407fba6b0b9a52ffa9e1ee5","3cb97904fecafcd1394495b1552325b4e9e9ac15"],"s2Url":"https://semanticscholar.org/paper/994f209aa17986a29b13b641ab1c6ee76073707c","s2PdfUrl":"","id":"994f209aa17986a29b13b641ab1c6ee76073707c","authors":[{"name":"Shun Yao","ids":["2964052"]},{"name":"Dantong Yu","ids":["33830021"]}],"journalName":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","paperAbstract":"The Secure Sockets Layer (SSL) is the main protocol used to secure Internet traffic and cloud computing. It relies on the computation-intensive RSA cryptography, which primarily limits the throughput of the handshake process. In this paper, we design and implement an OpenSSL library, termed PhiOpenSSL, which targets the Intel Xeon Phi (KNC) coprocessor, and utilizes Intel Phi's SIMD and multi-threading capability to reduce the SSL computation latency. In particular, PhiOpenSSL vectorizes all big integer multiplications and Montgomery operations involved in RSA calculations and employs theChinese Remainder Theorem and fixed-window exponentiation in its customized library. In an experiment involving the computation of Montgomery exponentiation, PhiOpenSSL was as much as 15.3 times faster than the two other reference libcrypto libraries, one from the Intel Many-core Platform Software Stack (MPSS) and the other from the default OpenSSL. Our RSA private key cryptography routines in PhiOpenSSL are 1.6-5.7 times faster than those in these two reference systems.","inCitations":[],"pdfUrls":["https://doi.org/10.1109/IPDPS.2017.32"],"title":"PhiOpenSSL: Using the Xeon Phi Coprocessor for Efficient Cryptographic Calculations","doi":"10.1109/IPDPS.2017.32","sources":["DBLP"],"doiUrl":"https://doi.org/10.1109/IPDPS.2017.32","venue":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)"},
{"entities":["Algorithm","Automaton","Central processing unit","Computation","Computer vision","Database","Field-programmable gate array","Graphics processing unit","Internet bottleneck","K-nearest neighbors algorithm","Machine learning","Natural language processing","Program optimization","Recommender system","Robotics","Similarity search","Speedup","Throughput"],"journalVolume":"","journalPages":"523-534","pmid":"","year":2017,"outCitations":["d4e445281bd0cffb900a7605872849ac2a2a5e31","086d4ffac8b60821aa05fd14cae101e32eb1e462","1b20afbd2d2a349737ed3dc246e44bbdba203190","630eb0c8cf211e95afc1696a2c627abe9e779bb3","268cd46a06e8e3052bbd64e96fac73d600430281","396514fb219879a4a18762cddfae2a6a607f439f","3ad277770454b1f53a1ae8109c35b1b59a22d33f","19d2b786fec5ded4d6cdca0e21f3c3f5264ecadf","2731046193a1d034ca5544e9cc642957c2d6310d","10911d4e163f7eeca5b53786814a01694643267a","bf70d60fc8d1de5fa53e8220a014fe463de4b7e5","a8ab0fc2f42476d2f76629747ecb981a438ea8ed","01e6176d319e3bfecc3667447c5bbaf2d00b9c7c","10e8ebc9a2397336cd03dda18842ad6e7e7299bb","12d0c11d546d91e776a170898ebf3a38c010695c","99d80987446ecc7fb546826e7bccebb2fdc5fa12","93c25da1b96dba6a83defeb05ebd5bd3c66feb87","06902cb95ede2c305db4000852014f276b25c082","1b68aa68c70af87fc3b712ff7a4a9aa289bf23bf","1fcfbc935db4d3297dc69e96d5b6741b7d151a2b","06c00c5de872edbb24ba5d67992cfbc912ffa7fb","4cd6b5e470b4205cd1560de42cde8108fa42ba4b","3ac6671a0c61544b9dab543b116eccdaccc6469e","1fbbc34d163b42a6cfd14eaff9556359c072e210","8d0bb67313c489aa90116c0c7df367a6ce46616d","6bccf2ba321177023d0f1d83484ae81fba687d97","0bd156b327f14b915a44848b1a0267fe9c30198c","3f3a44e1ef5acb51d6c53099fd296aa7d40355e0","8deafe947207eab416d8791f2e750289bd9ac73b","d4f2f0b971984fa5235ccd76a8bb1441a736bfa5","2d680892a7318ab7eff879054ea7ab6aeeb51fe9","3f1e54ed3bd801766e1897d53a9fc962524dd3c2","179f80848143cf109fa6aebae6c3844da03b062c","9952d4d5717afd4a27157ed8b98b0ee3dcb70d6c","4f86a09f1c3778203807c60f968605b139efe8d3","cd800900fbd9bc89a35ff597a7ddfe74aceefa55","149ad380837451a3903dafbb13f6de3815547852","a888f137b7d821497ad3a1264ae28a93852c0d75","56ef240a30a228ea6a6885d09dd3c60d2b021788","2871f115e7a11c903258491c75d4171fac679344","10b014e882764f5800ecdcbaba1fa08795d0c54d"],"s2Url":"https://semanticscholar.org/paper/3ad895a6e4ce6f07b722325613b27decf7aef4bc","s2PdfUrl":"","id":"3ad895a6e4ce6f07b722325613b27decf7aef4bc","authors":[{"name":"Vincent T. Lee","ids":["31901851"]},{"name":"Justin Kotalik","ids":["3451487"]},{"name":"Carlo C. del Mundo","ids":["2896556"]},{"name":"Armin Alaghi","ids":["1698528"]},{"name":"Luis Ceze","ids":["1717411"]},{"name":"Mark Oskin","ids":["1723213"]}],"journalName":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","paperAbstract":"Similarity search is a critical primitive for a wide variety of applications including natural language processing, content-based search, machine learning, computer vision, databases, robotics, and recommendation systems. At its core, similarity search is implemented using the k-nearest neighbors (kNN) algorithm, where computation consists of highly parallel distance calculations and a global top-k sort. In contemporary von-Neumann architectures, kNN is bottlenecked by data movement which limits throughput and latency. In this paper, we present and evaluate a novel automata-based algorithm for kNN on the Micron Automata Processor (AP), which is a non-von Neumann near-data processing architecture. By employing near-data processing, the AP minimizes the data movement bottleneck and is able to achieve better performance. Unlike prior work in the automata processing space, our work combines temporal encodings with automata design to augment the space of applications for the AP. We evaluate our design's performance on the AP and compare to state-of-the-art CPU, GPU, and FPGA implementations; we show that the current generation of AP hardware can achieve over 50x speedup over CPUs while maintaining competitive energy efficiency gains. We also propose several automata optimization techniques and simple architectural extensions that highlight the potential of the AP hardware.","inCitations":[],"pdfUrls":["https://doi.org/10.1109/IPDPS.2017.12","https://export.arxiv.org/pdf/1608.03175","http://homes.cs.washington.edu/~vlee2/docs/vlee-ipdps17.pdf"],"title":"Similarity Search on Automata Processors","doi":"10.1109/IPDPS.2017.12","sources":["DBLP"],"doiUrl":"https://doi.org/10.1109/IPDPS.2017.12","venue":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)"},
{"entities":["Algorithm","CPU cache","Cache (computing)","Clustering coefficient","Coefficient","Communication-avoiding algorithms","Computation","Floor and ceiling functions","Library","Message Passing Interface","Revolution in Military Affairs","Simulation"],"journalVolume":"","journalPages":"1018-1027","pmid":"","year":2017,"outCitations":["0caf691dc2f25a3eee80b3ac0e2aa72f953e18bc","11f093a54c40d8ea8336d8e575d5ab717e0fbb51","3ef8f56281ea8deb1a38eb95119e54e0da78d4ec","11ff641f673812b3ce78d46b13323a5f84393f60","fce7fd98928ab9bf3e4e919e108c48fc1040f569","bae1f940475f4be4862425582aa84a24e57e0d46","a1b9f637796f7366669f3c68dc7459596d1f7fad","6bad177eb5fc0fd7ea223149cec4a76d8567479a","24dc8d1de7e78ab100d2d83cbdf1390ddb9234c9","0371f9e3efbcd4829b5ffbff585155746ef05284","07745dca3ddfe267ccd7ad30cb6d4877f16389cc","2ef5647f9bd901a6da89cdaa064fa67ce905b38d","057a60fbc431ed0aef4e552199cc0ae0b970bb87","f58b22395f9585c3da65bbc948c67eed3377f701","17ffa6c8c257bf02a23699d226c541ac86af5e48","38a351a8ac273ff60b5fd712eafa49d6b8414009","08937c92f31895e16af48de1c7d18eeceef11f6f","6d1ca1108d9d96e5607571502552ad04464d7f15","69258ba9b1ace027daa767192698c84bf49b9fb6","01c1f0e97ce5c74c714dc7aa43cb064f45cc3b04"],"s2Url":"https://semanticscholar.org/paper/64e3577ff9b99f6c6ca366153d65b042afc058ab","s2PdfUrl":"","id":"64e3577ff9b99f6c6ca366153d65b042afc058ab","authors":[{"name":"Salvatore Di Girolamo","ids":["32042628"]},{"name":"Flavio Vella","ids":["34920674"]},{"name":"Torsten Hoefler","ids":["1713648"]}],"journalName":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","paperAbstract":"The constantly increasing gap between communication and computation performance emphasizes the importance of communication-avoidance techniques. Caching is a well-known concept used to reduce accesses to slow local memories. In this work, we extend the caching idea to MPI-3 Remote Memory Access (RMA) operations. Here, caching can avoid inter-node communications and achieve similar benefits for irregular applications as communication-avoiding algorithms for structured applications. We propose CLaMPI, a caching library layered on top of MPI-3 RMA, to automatically optimize code with minimum user intervention. We demonstrate how cached RMA improves the performance of a Barnes Hut simulation and a Local Clustering Coefficient computation up to a factor of 1.8x and 5x, respectively. Due to the low overheads in the cache miss case and the potential benefits, we expect that our ideas around transparent RMA caching will soon be an integral part of many MPI libraries.","inCitations":[],"pdfUrls":["https://doi.org/10.1109/IPDPS.2017.92"],"title":"Transparent Caching for RMA Systems","doi":"10.1109/IPDPS.2017.92","sources":["DBLP"],"doiUrl":"https://doi.org/10.1109/IPDPS.2017.92","venue":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)"},
{"entities":["Algorithm","Computer","Graph property","Parallel random-access machine","Sorting","Spanning tree"],"journalVolume":"","journalPages":"937-946","pmid":"","year":2017,"outCitations":[],"s2Url":"https://semanticscholar.org/paper/3c4fdf1e1ab3fbac23575d76a6f37deccbee5ad0","s2PdfUrl":"","id":"3c4fdf1e1ab3fbac23575d76a6f37deccbee5ad0","authors":[{"name":"Yujie An","ids":["3434870"]},{"name":"Quentin F. Stout","ids":["3067853"]}],"journalName":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","paperAbstract":"We give efficient algorithms to solve fundamental data movement problems on mesh-connected computers augmented with limited global bandwidth. Adding a small amount of global bandwidth makes a practical design that combines aspects of mesh and fully connected models to achieve the benefits of each. We give algorithms for sorting, finding the median, finding a spanning tree, and determining various graph properties to show that the small amount of global communication can significantly reduce the time, and that concurrent read helps even more. Most of these algorithms are optimal. We also extend our results to mesh-connected computers with row and column buses.","inCitations":[],"pdfUrls":["https://doi.org/10.1109/IPDPS.2017.74"],"title":"Optimal Algorithms for a Mesh-Connected Computer with Limited Additional Global Bandwidth","doi":"10.1109/IPDPS.2017.74","sources":["DBLP"],"doiUrl":"https://doi.org/10.1109/IPDPS.2017.74","venue":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)"},
{"entities":["Algorithm","Cray XC30","Cuthill–McKee algorithm","Data structure","Distributed memory","Locality of reference","Nested dissection","Parallel computing","Reliability-centered maintenance","Scalability","Sparse matrix","Speedup","Supercomputer"],"journalVolume":"","journalPages":"22-31","pmid":"","year":2017,"outCitations":["7ff0fa0958783397fa8db7125205bd6ee65b4c01","e20429fddd00ab8a3679b8cc3e82108ff128295d","0a791a760dd883342c8b8456a3e7cb75fb996ef4","4fa429b8b44bf3c67d2b4ebf6625c9357a0c8e3d","5a3cd8c65ffcc25bef346174d1f0bc3f83c5cbbb","a2ea514fe0486cfc46a0b50f1f8f187c43027d59","5cfeda94aaa59702e57647045de1488b8258abef","64a513b60ad89c4eee81a186e53c8d5c8773acac","ba75e4f7f6356d0c7a98ae813f085ce1a7a0aeec","1ef7f02bce931c8e9ef529e095b274132ce4011a","1e27b9b447cebd5047050e39bb9246fa6364b760","4f3caa5573b4c1ebef7c3ee6b9f7643e689c858e","15942e821dbca09e250a3831aa02c161d181030c","3cf464d61246103b12c64d4f790d8e40c639ffb8","c127da5c51a766bfeafdd02827b3225bf9f50dd4","c04ff62fd8366fa57fb9a039a52e590470066f43"],"s2Url":"https://semanticscholar.org/paper/4f31172aa290766d0cf453d5796186aa5749c60d","s2PdfUrl":"","id":"4f31172aa290766d0cf453d5796186aa5749c60d","authors":[{"name":"Ariful Azad","ids":["3032988"]},{"name":"Mathias Jacquelin","ids":["2955257"]},{"name":"Aydin Buluç","ids":["2238795"]},{"name":"Esmond G. Ng","ids":["40137540"]}],"journalName":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","paperAbstract":"Ordering vertices of a graph is key to minimize fill-in and data structure size in sparse direct solvers, maximize locality in iterative solvers, and improve performance in graph algorithms. Except for naturally parallelizable ordering methods such as nested dissection, many important ordering methods have not been efficiently mapped to distributed-memory architectures. In this paper, we present the first-ever distributed-memory implementation of the reverse Cuthill-McKee (RCM) algorithm for reducing the profile of a sparse matrix. Our parallelization uses a two-dimensional sparse matrix decomposition. We achieve high performance by decomposing the problem into a small number of primitives and utilizing optimized implementations of these primitives. Our implementation attains up to 38x speedup on matrices from various applications on 1024 cores of a Cray XC30 supercomputer and shows strong scaling up to 4096 cores for larger matrices.","inCitations":["d845d46d21df1009c0146ebdd341de9056834e6d"],"pdfUrls":["http://www.eecs.wsu.edu/~assefaw/CSC16/abstracts/azad-CSC16_paper_36.pdf","http://www.eecs.wsu.edu/~assefaw/CSC16/slides/ariful_azad_CSC16.pdf","https://arxiv.org/pdf/1610.08128v1.pdf","https://doi.org/10.1109/IPDPS.2017.85","https://crd.lbl.gov/assets/Uploads/RCM-ipdps17.pdf","http://crd.lbl.gov/assets/Uploads/RCM-ipdps17.pdf","http://arxiv.org/abs/1610.08128"],"title":"The Reverse Cuthill-McKee Algorithm in Distributed-Memory","doi":"10.1109/IPDPS.2017.85","sources":["DBLP"],"doiUrl":"https://doi.org/10.1109/IPDPS.2017.85","venue":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)"},
{"entities":["Adjusted winner procedure","Algorithm","Central processing unit","Graphics","Graphics processing unit","Next-generation network","Numerical analysis","Parallel computing"],"journalVolume":"","journalPages":"545-554","pmid":"","year":2017,"outCitations":["3dad87731664dc767db210b76dfc6db2bf206a39","32bf0e2294bc545dbee875a30d6a391691b0d671","21f3559282697fbe5ac6a75b3078fb74cf3c7d1c","07da7745ea84f3be0957035496f53d2b5a2acc42","bb4cf037d8a5adbb3f08a3405d926d022b8c27c5","d209b629ad39798ad2d1dae140b8541334a12db3","b7d792cded48c43ee69559276699d767d63ac9aa","2b83e301a84b076fbfc6d97034491cbab4ec0290","2516524a25fdba2c54f9a1d80b26300d896f2c9e","9db02ca8d27a6fc7acb7ed38d9af3527f4a01132","8de213c04eff53b819708487c579718180fb36e8","ea448a83c329e82a9592379e02507feacdf5676a","0919fe1248698a8a7b2a174d4fb160594da46ef9","5b1b04d1587ba870c5a0f0bb9a7e5de31b2b745d","092217c2267f6e0673590aa151d811e579ff7760","b5f3cdf1af34060e475ef6aba96059fb347c436c","05918ee9491afa7aef5936034de834f389da01f3"],"s2Url":"https://semanticscholar.org/paper/317e88dfbe42709490e90b3d63c22a6507494908","s2PdfUrl":"","id":"317e88dfbe42709490e90b3d63c22a6507494908","authors":[{"name":"Bram Veenboer","ids":["19252730"]},{"name":"Matthias Petschow","ids":["1776907"]},{"name":"John W. Romein","ids":["1722141"]}],"journalName":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","paperAbstract":"Realizing the next generation of radio telescopes such as the Square Kilometre Array (SKA) requires both more efficient hardware and algorithms than today's technology provides. The recently introduced image-domain gridding (IDG) algorithm is a novel approach towards solving the most compute-intensive parts of creating sky images: gridding and degridding. It avoids the performance bottlenecks of traditional AW-projection gridding by applying instrumental and environmental corrections in the image domain instead of in the Fourier domain. In this paper, we present the first implementations of this new algorithm for CPUs and Graphics Processing Units (GPUs). A thorough performance analysis, in which we apply a modified roofline analysis, shows that our parallelization approaches and optimizations lead to nearly optimal performance on these architectures. The analysis also indicates that, by leveraging dedicated hardware to evaluate trigonometric functions, GPUs are both much faster and more energy efficient than regular CPUs. This makes IDG on GPUs a candidate for meeting the computational and energy efficiency constraints of future telescopes.","inCitations":[],"pdfUrls":["https://doi.org/10.1109/IPDPS.2017.68"],"title":"Image-Domain Gridding on Graphics Processors","doi":"10.1109/IPDPS.2017.68","sources":["DBLP"],"doiUrl":"https://doi.org/10.1109/IPDPS.2017.68","venue":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)"},
{"entities":["Algorithm","Autonomous car","Central processing unit","Control flow","Experiment","Graphics processing unit","Message Passing Interface","Message passing","Network switch","Peer-to-peer","Run time (program lifecycle phase)","Single instruction, multiple threads","Wildcard character"],"journalVolume":"","journalPages":"855-865","pmid":"","year":2017,"outCitations":["1d887b6e2bcdce92f13878e220859e948930734a","cf60b4d7f37cc74ca7345a579201b89a010a67e8","07ecda8ad7075a97baa460ae8b03e2f7fb27c2b9","14d3c4a56abb7680d6523c0bc88d80899b631a09","b78c04c7f29ddaeaeb208d4eae684ffccd71e04f","03c316a2177d112efb7c64fae4fc10377419610b","4849bbb611153b5a7c53894fa1c1314138f5ae89","43498db7de27abf14e5d2903a8318c62b3c4c0e9","284c7fde4bbaf19dd345e3b37d98085d7bfb9a4f","6335be42a352d1d4daa907533854410f57269926","387d5b24317395ae7a86c8ecc9403ac62ed6febe","6f197e5aa64900079d760a397bb6a062df152ea6","8ce244596d60478c4c9c4dd5cf43c57e45fccfa2","401140aefbcefccfcc1dc4e5c5ab913ed9189e6a","63d9562d2e50c57e684faed416801732a37d39fd","28552ecf4eaedb3461edca97304b29082b02fbab","7f2cbf3dd422dec88f5725700913a1d44c6f5beb"],"s2Url":"https://semanticscholar.org/paper/3e1e61d1128dae2038cd9701ba95f348f6d12db1","s2PdfUrl":"","id":"3e1e61d1128dae2038cd9701ba95f348f6d12db1","authors":[{"name":"Benjamin Klenk","ids":["2712837"]},{"name":"Holger Fröning","ids":["1731123"]},{"name":"Hans Eberle","ids":["2298713"]},{"name":"Larry Dennison","ids":["17930267"]}],"journalName":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","paperAbstract":"Accelerators, such as GPUs, have proven to be highly successful in reducing execution time and power consumption of compute-intensive applications. Even though they are already used pervasively, they are typically supervised by general-purpose CPUs, which results in frequent control flow switches and data transfers as CPUs are handling all communication tasks. However, we observe that accelerators are recently being augmented with peer-to-peer communication capabilities that allow for autonomous traffic sourcing and sinking. While appropriate hardware support is becoming available, it seems that the right communication semantics are yet to be identified. Maintaining the semantics of existing communication models, such as the Message Passing Interface (MPI), seems problematic as they have been designed for the CPU&#x2019;s execution model, which inherently differs from such specialized processors. In this paper, we analyze the compatibility of traditional message passing with massively parallel Single Instruction Multiple Thread (SIMT) architectures, as represented by GPUs, and focus on the message matching problem. We begin with a fully MPI-compliant set of guarantees, including tag and source wildcards and message ordering. Based on an analysis of exascale proxy applications, we start relaxing these guarantees to adapt message passing to the GPU&#x2019;s execution model. We present suitable algorithms for message matching on GPUs that can yield matching rates of 60M and 500M matches/s, depending on the constraints that are being relaxed. We discuss our experiments and create an understanding of the mismatch of current message passing protocols and the architecture and execution model of SIMT processors.","inCitations":["63d9562d2e50c57e684faed416801732a37d39fd"],"pdfUrls":["https://doi.org/10.1109/IPDPS.2017.94"],"title":"Relaxations for High-Performance Message Passing on Massively Parallel SIMT Processors","doi":"10.1109/IPDPS.2017.94","sources":["DBLP"],"doiUrl":"https://doi.org/10.1109/IPDPS.2017.94","venue":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)"},
{"entities":["CPU cache","Cache (computing)","Computer data storage","Graphics processing unit","Shared memory"],"journalVolume":"","journalPages":"82-91","pmid":"","year":2017,"outCitations":["1087bbef784e7daecaf13b58bc1480d6dee4929b","ed0190758c03aea4e8f3c10f05851543ade1aea9","3364bc50921a9566d61ef8cb73baa82341725e4b","14d98ecba21e404f80daf024a03effe259cf9b88","0612811b3ed9fc7ef8300e65cb70360613dab01d","5f3cce1bc739ebfc03e003010d3438bb318efc14","7132859e2843f7adb82ec89daf0eb2bdb1da590b","c07ebd47e86f0ece88b28c57d79ed7544f5a30f0","559d122ef5c04a872812f8621df8f181e527b8bb","1a850fbc5d86a91d882eec88290425fbdff57cf6","03d832219a7cf933db0ef1f686fec730c09acd55","14505c2bdd3822d7a62385121d28ba3eb36fea1d","2d6f002477015469075954c6748a1a85af352c94","a1e4f4ae16c5a18896fe1718acfe56a26aeca620","712c92e77a2fd2cf4efec2bd2b3daa4ac7d16283","540a65f5e2176c4000551f1335a24e0f07500f68","28cc7453c5f3f9ecb9415e631b0829ec9af8a4c3","2dc38b527e91f8cfee6f6c7ba4d079087c293471","0ee3a956a67b0d679bf485d60e75abdbdb5d50e7"],"s2Url":"https://semanticscholar.org/paper/e80ccbd999f8f20ef8f06619e99073c6fca995b8","s2PdfUrl":"","id":"e80ccbd999f8f20ef8f06619e99073c6fca995b8","authors":[{"name":"Bingchao Li","ids":["2326844"]},{"name":"Jizhou Sun","ids":["35900806"]},{"name":"Murali Annavaram","ids":["1789661"]},{"name":"Nam Sung Kim","ids":["1686484"]}],"journalName":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","paperAbstract":"GPUs provide high-bandwidth/low-latency on-chip shared memory and L1 cache to efficiently service a large number of concurrent memory requests (to contiguous memory space). To support warp-wide accesses to L1 cache, GPU L1 cache lines are very wide. However, such L1 cache architecture cannot always be efficiently utilized when applications generate many memory requests with irregular access patterns especially due to branch and memory divergences. In this paper, we propose Elastic-Cache that can efficiently support both fine- and coarse-grained L1 cache-line management for applications with both regular and irregular memory access patterns. Specifically, it can store 32- or 64-byte words in non-contiguous memory space to a single 128-byte cache line. Furthermore, it neither requires an extra tag storage structure nor reduces the capacity of L1 cache since it stores auxiliary tags for fine-grained L1 cache-line managements in sharedmemory space that is not fully used in many applications. Our experiment shows that Elastic-Cache improves the geo-mean performance of applications with irregular memory access patterns by 58% without degrading performance of applications with regular memory access patterns.","inCitations":[],"pdfUrls":["https://doi.org/10.1109/IPDPS.2017.81"],"title":"Elastic-Cache: GPU Cache Architecture for Efficient Fine- and Coarse-Grained Cache-Line Management","doi":"10.1109/IPDPS.2017.81","sources":["DBLP"],"doiUrl":"https://doi.org/10.1109/IPDPS.2017.81","venue":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)"},
{"entities":["Block (programming)","C++","Computation","Inline expansion","Intel Edison","Message Passing Interface","Overlap–add method","Programmer","Relocation (computing)","Runtime system"],"journalVolume":"","journalPages":"998-1007","pmid":"","year":2017,"outCitations":["2043b7c6fa9bdd1174513cb0528cf8735c68e819","1b058a94bf00727c3e86155566e1e1b1a0d1b7c5","984250da0a78dbc6f47ce5624a98cfd6a4dc0aae","7945a39eebec1f288264464ff70c5a9a1715f367","383aec58bdf09e4549c4df2c984214838c5cb7f6"],"s2Url":"https://semanticscholar.org/paper/f31796af827c0ee9326d4546121b126ec69c38a2","s2PdfUrl":"http://pdfs.semanticscholar.org/f317/96af827c0ee9326d4546121b126ec69c38a2.pdf","id":"f31796af827c0ee9326d4546121b126ec69c38a2","authors":[{"name":"Sergio M. Martin","ids":["37146060"]},{"name":"Marsha J. Berger","ids":["34768652"]},{"name":"Scott B. Baden","ids":["1777004"]}],"journalName":"","paperAbstract":"We discuss early results with Toucan, a sourceto-source translator that automatically restructures C/C++ MPI applications to overlap communication with computation. We co-designed the translator and runtime system to enable dynamic, dependence-driven execution of MPI applications, and require only a modest amount of programmer annotation. Co-design was essential to realizing overlap through dynamic code block reordering and avoiding the limitations of static code relocation and inlining. We demonstrate that Toucan hides significant communication in four representative applications running on up to 24K cores of NERSC’s Edison platform. Using Toucan, we have hidden from 33% to 85% of the communication overhead, with performance meeting or exceeding that of painstakingly hand-written overlap variants. Keywords-Communication/Computation Overlap; Source-toSource Translator; MPI; Data-Driven.","inCitations":[],"pdfUrls":["https://doi.org/10.1109/IPDPS.2017.44","http://cseweb.ucsd.edu/groups/hpcl/scg/papers/2017/IPDPS17-CameraReady.pdf"],"title":"Toucan - A Translator for Communication Tolerant MPI Applications","doi":"10.1109/IPDPS.2017.44","sources":["DBLP"],"doiUrl":"https://doi.org/10.1109/IPDPS.2017.44","venue":"IPDPS"},
{"entities":["Autonomous car","Autonomous robot","Computation","Crash (computing)","Directed acyclic graph","Fault tolerance","Mobile robot","Non-blocking algorithm","Robot","Shared memory","Snapshot (computer storage)","Vertex (geometry)","Vertex (graph theory)"],"journalVolume":"","journalPages":"493-502","pmid":"","year":2017,"outCitations":["60f8c9937c769f51486baa6b2e3c2faa50d96a43","f3018e7589af851341e6b40affb12d0ebdfa7db1","e9b1ae133ae34f8348c19c1c1066439eddbf9ab0","b7faa8135d1159d0356c0710ea267adf43852dab","50b862c0552c18a72632580807572ea4999624ce","35813f84719c20e315268e12a7edfab38ce90418","bb0d3409e2cc104a4ffe062e1a860c537a9f61fb","410a17b2f1120698005cf16829089cf83fdc688e","4dee28e31d82612438c19639cc3d78957ccc858b","6afc4972cc6e40716480c4bc827225b9da7d3046","908b2bf7b27827343ba1d508489238ea2963e4e7","3c9f845e65a1d996a9cc0df88cd39099437f875b","00e35ff42644e5d068634165d77d2e44e78f8679","7ab9de59820edc16d34429ac2ec77c8ff60b1486","40af88c7982cd15e18f21a66897eb938fd98866b","c7f0190371409c846b430d9ed528a95788cbbf56","214baa0f37921ce21a9705e81514cde8e28e1ce9","daefc7b798a846028f6b7a702f56b20c59f144e2","9ef8edd4009f9bc5766391ea737d0ebac1faea6e","296bf65141caa95e7701f5efc2c030a345b20a53","a71d4757e2194ca691b0464dc5d17f9c1d4ef6d7","00e3756119a91432622f6982b59ecd24a1340fbe","01a100dadf3d776e7f2fee97b42966c47aa65fb8","22bb39eebb11649ab5a55b789d5b677ac97eab49","a937641fa59c41ecc1b6ce25ff0a5695285a28f3","c0705da825e9baa2da4232c6a3f283b1990e4e64","000ff9478eb220d1c520b6a2be733178357da64f"],"s2Url":"https://semanticscholar.org/paper/a90b8faaf327dc06dd1df2bf5cc64b4af234b6bc","s2PdfUrl":"","id":"a90b8faaf327dc06dd1df2bf5cc64b4af234b6bc","authors":[{"name":"Sergio Rajsbaum","ids":["1718101"]},{"name":"Armando Castañeda","ids":["38307043"]},{"name":"David Flores-Peñaloza","ids":["2913070"]},{"name":"Manuel Alcantara","ids":["13121022"]}],"journalName":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","paperAbstract":"The LOOK-COMPUTE-MOVE model for a set of autonomous robots has been thoroughly studied for over two decades. Each robot repeatedly LOOKS at its surroundings and obtains a snapshot containing the positions of all robots; based on this information, the robot COMPUTES a destination and then MOVES to it. Previous work assumed all robots are present at the beginning of the computation. What would be the effect of robots appearing asynchronously? This paper studies thisquestion, for problems of bringing the robots close together, andexposes an intimate connection with combinatorial topology. A central problem in the mobile robots area is the gathering problem. In its discrete version, the robots start at vertices in some graph G known to them, move towards the same vertex and stop. The paper shows that if robots are asynchronous and may crash, then gathering is impossible for any graph G with at least two vertices, even if robots can have unique IDs, remember the past, know the same names for the vertices of G and use an arbitrary number of lights to communicate witheach other. Next, the paper studies two weaker variants of gathering: edge gathering and 1-gathering. For both problems we present possibility and impossibility results. The solvability of edge gathering is fully characterized: it is solvable for three or more robots on a given graph if and only if the graph is acyclic. Finally, general robot tasks in a graph are considered. A combinatorial topology characterization for the solvable tasks is presented, by a reduction of the asynchronous fault-tolerant LOOK-COMPUTE-MOVE model to a wait-free read/write shared-memory computing model, bringing together two areas that have been independently studied for a long time into a common theoretical foundation.","inCitations":["357a8059b8fdabc4c281a7cb2e03dce22c4ef2a4","765731abac777fdd772837c58b43a775c99784d0"],"pdfUrls":["https://doi.org/10.1109/IPDPS.2017.70"],"title":"Fault-Tolerant Robot Gathering Problems on Graphs With Arbitrary Appearing Times","doi":"10.1109/IPDPS.2017.70","sources":["DBLP"],"doiUrl":"https://doi.org/10.1109/IPDPS.2017.70","venue":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)"},
{"entities":["Algorithm","Benchmark (computing)","CPU cache","Cache (computing)","Central processing unit","Computer architecture","Computer data storage","Emulator","Non-volatile memory","Non-volatile random-access memory","Persistence (computer science)","Persistent data structure","Persistent memory","Time complexity"],"journalVolume":"","journalPages":"112-122","pmid":"","year":2016,"outCitations":["37a1e8411669e29cf8fbf48ec920c97c0066ac7e","f8aa33900f552f8112d6186d78bc845d2dfc0007","2e8ab636f544007408884dc6fafafdb00a4cd62a","24f45805614b7e74346e56e5d151fc460432f886","0261afd40eee66cea4ea682fab322a439a28f37d","998376a2b4e53acab8dd12f7137dad992c13a8ea","4209919a9b9618d69a145b15927b5c455f9d05d4","5341ce4822eb9cdb614615d57cea02edc8d33c05","0558aeb941b46e7b588e5d81bdc01c11a53ba45d","47b851237f240831abee3971bca6bb8d2a121eb1","78f75d24f58c69386d3d10c27fe55c9bbccb95dd","2d45779437516ee55e5f9f4e7a7d8803fa795443","c7ae87b4e5952560362e24274a3e9f4e78a666f6","47ccfd0c9dc218f5496783310a28c581730b9ca7","0c0ff71e1f225312bd24a2d78153f0b3f3816285","7765baefdd1d404fc31b372094cdebbcb1b63a9e","a7e1330976e46e7a48986f2648381f8876ac653e","23773ffc679a8d9ebfd73810dec3e6fe6aa278ab","209c2347a28bc0af9f8ace63ebbdf056729f41dc","512a8925693d5f4b8e4cfde32bcd3c846a14b71e","9225ce5b4359748953cb1de088da5b8a63397490","10d8afea57c8f159c4eb2664a40c8fb859acefef","2f1993d2c6c82e6a794adf19d1b5cec9fc593602","0332013fc380ca283d3afc457c430c513d19cc51","60b85b7ee655397a4d2202f9cdf6dd5e3f04f6fd","43a7ea9a745da76f5fcf74d1b2ae4786a9f37664","72734685215d1a65a8b2b0f7e9e8f6c3e89fce3e","b8735a449f0a1f1889c6b744061360aa85afaa6b","2f42558a0b49b56ef706e8435eeb7bb480f58aad","4dd4e6f0806001700b0b310172f8161ac47e9389","ca4564d556b03eeee755fad7a89475072424ea56","0cc19393203cd41fc29e3cb940ee468039cd0158","4a58e3066f12bb86d7aef2776e9d8a2a4e4daf3e"],"s2Url":"https://semanticscholar.org/paper/970364730c038572ee9b7b69eef86d97a5488a2d","s2PdfUrl":"","id":"970364730c038572ee9b7b69eef86d97a5488a2d","authors":[{"name":"Pengcheng Li","ids":["40205336"]},{"name":"Dhruva R. Chakrabarti","ids":["1804605"]},{"name":"Chen Ding","ids":["1716493"]},{"name":"Liang Yuan","ids":["32967785"]}],"journalName":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","paperAbstract":"Non-volatile main memory (NVRAM) enables data persistence in memory. However, the existence of transient CPU caches in modern computer architectures brings a serious performance issue. In particular, cache lines have to be flushed frequently to guarantee consistent persistent program states. Hence, persistence and performance cannot be easily obtained simultaneously. In this paper, we optimize data persistence by proposing a software cache. The software cache first buffers lines that need to be flushed, and then flushes them out at an appropriate later time. The software cache aims to maximize the combination of cache line flushes. We designed a new linear-time algorithm to calculate cache miss ratio curve (MRC) so as to adaptively select the best cache capacity at run-time based on program behavior. We evaluated the software cache on a real-world memory-based database benchmark, the SPLASH2 benchmark suite and four micro-benchmarks. Results indicate that the software cache solution reduces cache write backs to persistent memory by 12&#x00D7; and improves performance over the state-of- the-art methods by 2.1&#x00D7; on average, measured on a real system emulator.","inCitations":["aa0fb8802532106dcb78c62065258b8e4683ec94","e3a66a2e79c2a5f7a52bf4e8089bf522fdc04abe"],"pdfUrls":["https://doi.org/10.1109/IPDPS.2017.83","https://doi.org/10.1007/978-3-319-52709-3_8"],"title":"Adaptive Software Caching for Efficient NVRAM Data Persistence","doi":"10.1109/IPDPS.2017.83","sources":["DBLP"],"doiUrl":"https://doi.org/10.1109/IPDPS.2017.83","venue":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)"},
{"entities":["Central processing unit","Compiler","Cray XC30","Knights","Load balancing (computing)","Manycore processor","Multi-core processor","Particle-in-cell","Performance Evaluation","Product binning","SIMD","Simulation","Sorting","Supercomputer","Thread (computing)","Voxel","Xeon Phi"],"journalVolume":"","journalPages":"202-212","pmid":"","year":2017,"outCitations":["277123d2a8b1a97e97d07330dccf03f50e261dcb","0c01348dae1ba59ab5f2ddd802ff9fa1342743be","52c2a2ed3be3690326ca368691f43be7a6292949","0b56c7d137b6cde03cc4657f8608a39837c49e6a","e601fe611210e2afb0d63040ec991004a1c01458","34468f409ff913f03eb8f3705ea6d042c0500832","6ed87b9a87f700b480db236672336d808c1957b6","95b8b77b02cace4f49db3020306224f224c4d04d","b7455effa0ef5f47eb1ed671a122266c1ba0325d","ac9bcf102038159809981e54ba6dadc4f7050db1","fc1cb36e2f054c7ca734f388955a8394e17f81ee","01fded66bcc34d93cba72b3490cabb87268099ba"],"s2Url":"https://semanticscholar.org/paper/fc25cfe166dc88aafb223aa9bb4d548d80a40f43","s2PdfUrl":"","id":"fc25cfe166dc88aafb223aa9bb4d548d80a40f43","authors":[{"name":"Hiroshi Nakashima","ids":["31344273"]},{"name":"Yoshiki Summura","ids":["19312747"]},{"name":"Keisuke Kikura","ids":["19218543"]},{"name":"Yohei Miyake","ids":["1891277"]}],"journalName":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","paperAbstract":"We are now developing a manycore-aware implementation of multiprocessed PIC (particle-in-cell) simulation code with automatic load balancing. A key issue of the implementation is how to exploit the wide SIMD mechanism of manycore processors such as Intel Xeon Phi. Our solution is &#x0022;particle binning&#x0022; to rank all particles in a cell (voxel) in a chunk of SOA (structure-of-arrays) type one-dimensional arrays so that &#x0022;particle-push&#x0022; and &#x0022;current-scatter&#x0022; operations on them are efficiently SIMD-vectorized by our compiler. In addition, our sophisticated binning mechanism performs sorting of particles according to their positions &#x0022;on-the-fly&#x0022;, efficiently coping with occasional &#x0022;bin overflow&#x0022; in a fully multithreaded manner. Our performance evaluation with up to 64 nodes of Cray XC30 and XC40 supercomputers, equipped with Xeon Phi 5120D (Knights Corner) and 7250 (Knights Landing) respectively, not only exhibited good parallel performance, but also proved the effectiveness of our binning mechanism.","inCitations":["3da3a2f4679c4b77d24d0c22c0f311ca49fc23e2"],"pdfUrls":["https://doi.org/10.1109/IPDPS.2017.65"],"title":"Large Scale Manycore-Aware PIC Simulation with Efficient Particle Binning","doi":"10.1109/IPDPS.2017.65","sources":["DBLP"],"doiUrl":"https://doi.org/10.1109/IPDPS.2017.65","venue":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)"},
{"entities":["Experiment","Extrapolation","Job control (Unix)","Linux","Lossless compression","Mathematical model","Memory-mapped I/O","P system","Signal trace","Simulation","Supercomputer","Synthetic data"],"journalVolume":"","journalPages":"585-594","pmid":"","year":2017,"outCitations":["0d744408b775b228dc6ba5064ee769ee4299f6df","6ecf1ef46e34ddcb3e385743fc07a80b860250e2","59ba9f62728b6231f982ea3b59f9ba7422182f28","59a7ce2144978a4cdb115760eb915f224136a8fc","3fcc786e099ba07cb52744a80b895061b309df7f","884fc7d1c8353a6ca2f0830a9f0f840a985afa7e","48d2d23ca0707556e63f7182737612ed2ce8b14c","b3a8bed5645b36606b29811c0a215eaaa017608d","2a11832bb798de3315838c327bdcec6493cd2a5c","9a7388b992631676c58fc06f891878e88e6d102c","39c0b19b60b8e872230220ea8882488221f01941","d6d6793a7049b810a0b1dbb4f6a4d517e69244d7","6a4105c2e444bf4a164c498126bc35f45e497286","483e6cb001353b145b8a0c73b52526c1fe1b0db7","981f151192b553300daaab96d60c7cbe2cdbb1dd","0fef8efee83bf73d50d29de247b1311d260547f3","10b1552f5b7f7f95ebcc02779fed467ef2a812ca","07afa1ea6934df5d325b07754f9eda290981735d","0bb2a26a0adb2be2ee078df83409db27d76ea322","8f16149eb792ef774487a0c008442cd5df72d9e3","3a69f1592a65a85bab18a00481e98f95849d4d9d","6e0039d62431ec95136f738c5020f6e3d3711168","0c60a639dc9cd8014f685ec986c29bf55a10bb5a","9edab79d681bae0071aa784328b0ce134d909c10","aadb50ca39eb59321af14142e88c3fd293238b15"],"s2Url":"https://semanticscholar.org/paper/187a407c0ddcbb73e227b06c08d2f7d1374014a6","s2PdfUrl":"","id":"187a407c0ddcbb73e227b06c08d2f7d1374014a6","authors":[{"name":"Xiaoqing Luo","ids":["1855827"]},{"name":"Frank Mueller","ids":["1684152"]},{"name":"Philip H. Carns","ids":["2797656"]},{"name":"Jonathan Jenkins","ids":["38538207"]},{"name":"Robert Latham","ids":["1692762"]},{"name":"Robert B. Ross","ids":["40211322"]},{"name":"Shane Snyder","ids":["39683248"]}],"journalName":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","paperAbstract":"Today&#x2019;s rapid development of supercomputers has caused I/O performance to become a major performance bottleneck for many scientific applications. Trace analysis tools have thus become vital for diagnosing root causes of I/O problems. This work contributes an I/O tracing framework with (a) techniques to gather a set of lossless, elastic I/O trace files for small number of nodes, (b) a mathematical model to analyze trace data and extrapolate it to larger number of nodes, and (c) a replay engine for the extrapolated trace file to verify its accuracy. The traces can in principle be extrapolated even beyond the scale of presentday systems and provide a test if applications scale in terms of I/O. We conducted our experiments on three platforms: a commodity Linux cluster, an IBM BG/Q system, and a discrete event simulation of an IBM BG/P system. We investigate a combination of synthetic benchmarks on all platforms as well as a production scientific application on the BG/Q system. The extrapolated I/O trace replays closely resemble the I/O behavior of equivalent applications in all cases.","inCitations":[],"pdfUrls":["https://doi.org/10.1109/IPDPS.2017.45","http://moss.csc.ncsu.edu/~mueller/ftp/pub/mueller/theses/luo-th.pdf","http://moss.csc.ncsu.edu/~mueller/ftp/pub/mueller/papers/ipdps17.pdf"],"title":"ScalaIOExtrap: Elastic I/O Tracing and Extrapolation","doi":"10.1109/IPDPS.2017.45","sources":["DBLP"],"doiUrl":"https://doi.org/10.1109/IPDPS.2017.45","venue":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)"},
{"entities":["Algorithm","Las Vegas algorithm","Monte Carlo","Parallel computing","Parallel programming model","Simulation"],"journalVolume":"","journalPages":"357-366","pmid":"","year":2017,"outCitations":["08c3532ede4089a290e114dfa0e7d29ccac9c1bb","a3dcc1d2d083f6ed8371e6619557cf7ffdc4b410","5b3f43a02fe5bce776833d95d5a2b8afc904b375","b78c04c7f29ddaeaeb208d4eae684ffccd71e04f","3515804fcd5368b4eb8af3c66ba75c01f2c9e871","550285725684e2d286ffd9fa5cebdc52d7c4f860","569de2eececd3adb7219d63eb85e4bdc63486c42","f27070fa68d10ac71d82c6f0184cb7c6fc111f79","f3018e7589af851341e6b40affb12d0ebdfa7db1","6fbd9834cb888b84db1f25756cb6173b3622e4a0","5ce262f8c816009dc859d476d1850fba0c516e12","07b66a85083291d2b702a3bcc30f32854d4a6d29","24f310878b013ce02e9c046fa1bac611d66868a8","0ca1e465dd85b8254bcdd7053032d7eab6e2d4b4","10b44b914a35142eb7c1cff7a33e5527715561ee","1822b56cea223cedf501fa10bd3795767ab80a9e","1861776e08d4ce30ac63bd99b03501a80b98bf87","b6571efa4483aa00d23bbcd36930c4877548ba38","a524c99eab404a707fdffe28ff8a83f865cb3d61","b7c2e68743af169ae14dc2f2a0534e0ea2666ba8"],"s2Url":"https://semanticscholar.org/paper/8b12a237f1845086337926742810259b8e87fed0","s2PdfUrl":"","id":"8b12a237f1845086337926742810259b8e87fed0","authors":[{"name":"Torsten Hoefler","ids":["1713648"]},{"name":"Amnon Barak","ids":["2780481"]},{"name":"Amnon Shiloh","ids":["40399772"]},{"name":"Zvi Drezner","ids":["1682521"]}],"journalName":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","paperAbstract":"Large-scale parallel programming environments and algorithms require efficient group-communication on computing systems with failing nodes. Existing reliable broadcast algorithms either cannot guarantee that all nodes are reached or are very expensive in terms of the number of messages and latency. This paper proposes Corrected-Gossip, a method that combines Monte Carlo style gossiping with a deterministic correction phase, to construct a Las Vegas style reliable broadcast that guarantees reaching all the nodes at low cost. We analyze the performance of this method both analytically and by simulations and show how it reduces the latency and network load compared to existing algorithms. Our method improves the latency by 20% and the network load by 53% compared to the fastest known algorithm on 4,096 nodes. We believe that the principle of corrected-gossip opens an avenue for many other reliable group communication operations.","inCitations":["d0556be65e8564ab8bb3e26b6a0146a62027bc40"],"pdfUrls":["https://doi.org/10.1109/IPDPS.2017.36","http://htor.inf.ethz.ch/publications/img/hoefler-corrected-gossip.pdf"],"title":"Corrected Gossip Algorithms for Fast Reliable Broadcast on Unreliable Systems","doi":"10.1109/IPDPS.2017.36","sources":["DBLP"],"doiUrl":"https://doi.org/10.1109/IPDPS.2017.36","venue":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)"},
{"entities":["Algorithm","Clustered file system","Consistent hashing","Data center","Elasticity (cloud computing)","Scalability","Server (computing)","Server farm"],"journalVolume":"","journalPages":"876-885","pmid":"","year":2017,"outCitations":["1d99b7749a9311d2db24a3d84728e444eff23e4b","52d81096f46be0e75f85e0b7eeda65640c281630","178ef44fe69c2adfcd4a31f99bb2b9a8975e9cd5","438c51040ee6ccf9198e52d105c47e75d615b29c","110b17aede6e6a4fc8aeec50a54fe4dddc2c4779","7a2274412948765bf872b765dafd8139e51000ff","5f3f9223c5c9f896be099bc177929febad508407","2e72178091b2ca445f46200dcba71a53417b69eb","1143a1a595dc305347ff8aba001635c88552b6f7","638c917d981915bc7a00bb0941cdd38111df51de","2da760f90c3d2bf6598becdde9063093f488548c","17ad973d5a839c378db68b05d7939a28fc014935","090599a2caf4591c87699ad850c75554cd712937","61d5c261cfa704085f9d397b298a150bcc07336b","2133e6faa9232e0d0967538e51b3d1fe805952d7","780729b2fd5169c2c7a4df956a38d7df15317ca9","534d57618f4e1657c93c0a0f930ae6270794667c","5f6ae1d342411bcae2a1dbec79a4ad590f327bb2"],"s2Url":"https://semanticscholar.org/paper/4f5782ab1e3dc4bfb31e81d72219310b61337049","s2PdfUrl":"","id":"4f5782ab1e3dc4bfb31e81d72219310b61337049","authors":[{"name":"Wei Xie","ids":["1787506"]},{"name":"Yong Chen","ids":["3519489"]}],"journalName":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","paperAbstract":"Elastic distributed storage systems have been increasingly studied in recent years because power consumption has become a major problem in data centers. Much progress has been made in improving the agility of resizing small- and large-scale distributed storage systems. However, most of these studies focus on metadata based distributed storage systems. On the other hand, emerging consistent hashing based distributed storage systems are considered to allow better scalability and are highly attractive. We identify challenges in achieving elasticity in consistent hashing based distributed storage. These challenges cannot be easily solved by techniques used in current studies. In this paper, we propose an elastic consistent hashing based distributed storage to solve two problems. First, in order to allow a distributed storage to resize quickly, we modify the data placement algorithm using a primary server design and achieve an equal-work data layout. Second, we propose a selective data re-integration technique to reduce the performance impact when resizing a cluster. Our experimental and trace analysis results confirm that our proposed elastic consistent hashing works effectively and allows significantly better elasticity.","inCitations":[],"pdfUrls":["https://doi.org/10.1109/IPDPS.2017.88","http://discl.cs.ttu.edu/lib/exe/fetch.php?media=wiki:papers:main_elasticch.pdf"],"title":"Elastic Consistent Hashing for Distributed Storage Systems","doi":"10.1109/IPDPS.2017.88","sources":["DBLP"],"doiUrl":"https://doi.org/10.1109/IPDPS.2017.88","venue":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)"},
{"entities":["Blocking (computing)","Deadlock","Lossless compression","Network congestion","Routing","Simulation","Stock and flow","Throughput","Virtual channel"],"journalVolume":"","journalPages":"842-854","pmid":"","year":2017,"outCitations":["a4e9124a08dcce799cd7b34655bb0d089f47b7df","eeaf14c39d463ca11a6070450fe47bed91a0ae8f","438584c2ea63887ad6b227ad5d6743aa8ab0b443","b10ff7d0f23149d0b442a38964c737374c2fa4ff","b1209f6ac85768f7fc4bd0159fa390200e3207d6","1b27049bc69641a328a9b3a28f9b6bb8b9afd5f5","943cf22e168a86fec0381ca380474c1da39e509c","97900426468a277603579c1410134a2fef509cf3","5f8991828def57d2f0cda942566afff56740d150","0736d68aad2c198a8f6dda851c27bd180421c2aa","d488b45aaa4ceb4e994562199088c46fdb735925","194a6fc5de629e4f55c00d0720b8279ac8b494de","6e2ae987e8c0efc3bb92df4adfa728dc57f983f7","854fc26b3fe9fb5d4e643be81df063d45415a993","6f73820a6ef96c21ac6ae6f82d42b5b187b34138","38bcb4ec26cf6755c4b2d0b74257ad17a4b99642","8eb1c6d8d479192a22d3b2d3f351083a243e6011","7a7158c463a87f7cd7a13a782645f15e13d649a1","4110d5ad162fbf43a3418f28b4d46609c2a147be","8b0a16e9ab419a2096dbd55d5326607cbc385025","1eeea239e84fa6901e74b6e8552ecec7dd800b11","9c4b6c885bfc6038cdac56763663880e0f2624e6","013a0623848119ad6082bc5f8893e4814ab07ea8","565d6f6518e8b74b608b9e37d4c550f62d2909c8","6bad177eb5fc0fd7ea223149cec4a76d8567479a","4190d69147e34441ab4e4fa4fb4247eea092aa8e","15f1312866a40e516f0e7f128864013ef6eb2df8","5885d3525c1789aaa3aacc1740a3a6b51376f1b8","a15bc58fa496b6cca937713723f19f45380fc2fe","022a0317d5bf2b38847b03f7c9bc3bfa35950199","4654de106f5fd7caf1aab17468fad46a525c9da2","c080810ae3dff3bf47305e56328418cdfab83592","2d086787132666be7d425c5534132b0956c30435","8ad197d6b10ddd3df3a1fd1c3f71773a83cfb9ac","42e5e97272ad8728749f861ed7a920707e698778","40bb02f20fd846424a065fc06c45ae237d8ec13e","2f5e593d29a5eb8b3f7c65e4e5c740b792933757","9d7d1fd7be593ca61a334f9b18eb8ba8e0149450","2b8f7ce8460e7e183de754b09cfc0e624476d7f2","528628b4d20e6a98130ac12083a8c879aa31c7ad"],"s2Url":"https://semanticscholar.org/paper/0ca6a879ad1075869c96ac50b9cd7eaf15dfc666","s2PdfUrl":"","id":"0ca6a879ad1075869c96ac50b9cd7eaf15dfc666","authors":[{"name":"Pablo Fuentes","ids":["38516568"]},{"name":"Enrique Vallejo","ids":["1879134"]},{"name":"Ramón Beivide","ids":["1762103"]},{"name":"Cyriel Minkenberg","ids":["1794299"]},{"name":"Mateo Valero","ids":["1741016"]}],"journalName":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","paperAbstract":"Deadlock avoidance mechanisms for lossless lowdistance networks typically increase the order of virtual channel (VC) index with each hop. This restricts the number of buffer resources depending on the routing mechanism and limits performance due to an inefficient use. Dynamic buffer organizations increase implementation complexity and only provide small gains in this context because a significant amount of buffering needs to be allocated statically to avoid congestion. We introduce FlexVC, a simple buffer management mechanism which permits a more flexible use of VCs. It combines statically partitioned buffers, opportunistic routing and a relaxed distancebased deadlock avoidance policy. FlexVC mitigates Head-of-Line blocking and reduces up to 50% the memory requirements. Simulation results in a Dragonfly network show congestion reduction and up to 37.8% throughput improvement, outperforming more complex dynamic approaches. FlexVC merges different flows of traffic in the same buffers, which in some cases makes more difficult to identify the traffic pattern in order to support nonminimal adaptive routing. An alternative denoted FlexVCminCred improves congestion sensing for adaptive routing by tracking separately packets routed minimally and nonminimally, rising throughput up to 20.4% with 25% savings in buffer area.","inCitations":[],"pdfUrls":["http://upcommons.upc.edu/bitstream/handle/2117/107647/FlexVC+Flexible+Virtual+Channel+Management+in.pdf;jsessionid=D92E427C1FA21BE4080A4FAF055A4EE6?sequence=3","https://doi.org/10.1109/IPDPS.2017.110","http://personales.unican.es/fuentesp/refs/IPDPS2017.pdf"],"title":"FlexVC: Flexible Virtual Channel Management in Low-Diameter Networks","doi":"10.1109/IPDPS.2017.110","sources":["DBLP"],"doiUrl":"https://doi.org/10.1109/IPDPS.2017.110","venue":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)"},
{"entities":["Address space","Benchmark (computing)","Chapel","Computer performance","Correctness (computer science)","Local variable","Locality of reference","Parallel computing","Partitioned global address space","Program optimization"],"journalVolume":"","journalPages":"377-386","pmid":"","year":2017,"outCitations":["398aaf00253e2c29e6238dd0499aa3a75c76914c","2a9968defc94dff7f0f40beaf54941dca1a7d342","0062b2153532a78b3aac817806b8be7a760414f6","3bf4dcdbd8787a7ded95af0fc22ba87f26532c2e","3218bbfd89deae4134d6c6d7f8f3ceb5c3a361f7","1f2ff98f9413bb36c641e9edcfa79f7b33eeb80a","4721ad0db596f3f78ddb31b4305ddbde35f8f181","b1f907cd8c25d0acab070a36a9426519a9e669df","5bc26f5871f4b83006e5262810848aafdad00ef5","178599e5e976e82528e71cb2e1b812d588fa0e44","07780846047d6aa2781868b4cdfe13ae8eda21af","6cef271ffa332f9570fc1d21fcc99e7b35c6a825","d32d4ff33b1b2665d6081194eb6acdc3c7dd6891","c5c9f19531d607699f7f8f1f3c8b0105ad023996","1a8d1bb7364022fbda2e5da92f034f729198ac01","5f1c5e9481893362449da2270f5cf751875e406a"],"s2Url":"https://semanticscholar.org/paper/762a3f63c14b597019b2b42399416f4e71be74aa","s2PdfUrl":"","id":"762a3f63c14b597019b2b42399416f4e71be74aa","authors":[{"name":"Hui Zhang","ids":["1688592"]},{"name":"Jeffrey K. Hollingsworth","ids":["1754659"]}],"journalName":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","paperAbstract":"Chapel is an emerging PGAS (Partitioned Global Address Space) language whose design goal is to make parallel programming more productive and generally accessible. To date, the implementation effort has focused primarily on correctness over performance. We present a performance measurement technique for Chapel and the idea is also applicable to other PGAS models. The unique feature of our tool is that it associates the performance statistics not to the code regions (functions), but to the variables (including the heap allocated, static, and local variables) in the source code. Unlike code-centric methods, this data-centric analysis capability exposes new optimization opportunities that are useful in resolving data locality problems. This paper introduces our idea and implementations of the approach with three benchmarks. We also include a case study optimizing benchmarks based on the information from our tool. The optimized versions improved the performance by a factor of 1.4x for LULESH, 2.3x for MiniMD, and 2.1x for CLOMP with simple modifications to the source code.","inCitations":[],"pdfUrls":["https://doi.org/10.1109/IPDPS.2017.37"],"title":"Data Centric Performance Measurement Techniques for Chapel Programs","doi":"10.1109/IPDPS.2017.37","sources":["DBLP"],"doiUrl":"https://doi.org/10.1109/IPDPS.2017.37","venue":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)"},
{"entities":["Cube","Data cube","Data-intensive computing","Dynamic programming","Graph500","IBM WebSphere eXtreme Scale","Multithreading (computer architecture)","NWChem","OpenMP","Parallel computing","Programmer","Thread (computing)"],"journalVolume":"","journalPages":"799-808","pmid":"","year":2017,"outCitations":["94bc92a2275894b498cfed61fee1d261d4daf708","1134aaa6a93f502cac9ce551b13c00b10ff34feb","153fda7d7963c3bbec36e69909973f96a242d1f7","7ac941fa8c72f8a931ae2a48118a9893d6f1d083","4099889b566e4c1d0f9f90457f77b414cca5cb3c","6a668dfe4fa05408a5f752201ad83e02181ed6e2","59fb33ff0f35170529107f8a1a519cdac8464fd6","004c68d94d2806be41cf40cff60bffcf9d4aff0b"],"s2Url":"https://semanticscholar.org/paper/2f963c50025d607c5009be311bfe737aac12a7bb","s2PdfUrl":"","id":"2f963c50025d607c5009be311bfe737aac12a7bb","authors":[{"name":"Jaime Arteaga Molina","ids":["19169712"]},{"name":"Stéphane Zuckerman","ids":["36609334"]},{"name":"Guang R. Gao","ids":["1745279"]}],"journalName":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","paperAbstract":"The overwhelming wealth of parallelism exposed by Extreme-scale computing is rekindling the interest for finegrain multithreading, particularly at the intranode level. Indeed, popular parallel programming models, such as OpenMP, are integrating fine-grain tasking in their newest standards. Yet, classical coarse-grain constructs are still largely preferred, as they are considered simpler to express parallelism. In this paper, we present a Multigrain Parallel Programming environment that allows programmers to use these well-known coarse-grain constructs to generate a fine-grain multithreaded application to be run on top of a fine-grain event-driven program execution model. Experimental results with four scientific benchmarks (Graph500, NAS Data Cube, NWChem-SCF, and ExMatEx's CoMD) show that fine-grain applications generated by and run on our environment are competitive and even outperform their OpenMP counterparts, especially for data-intensive workloads with irregular and dynamic parallelism, reaching speedups as high as 2.6x for Graph500 and 50x for NAS Data Cube.","inCitations":["3d7de57b5dae7e7a5d8a2424c79e46f73c184938"],"pdfUrls":["https://doi.org/10.1109/IPDPS.2017.63"],"title":"Multigrain Parallelism: Bridging Coarse-Grain Parallel Programs and Fine-Grain Event-Driven Multithreading","doi":"10.1109/IPDPS.2017.63","sources":["DBLP"],"doiUrl":"https://doi.org/10.1109/IPDPS.2017.63","venue":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)"},
{"entities":["Algorithm","Application domain","CUDA","Computation","Graphics processing unit","Hidden Markov model","Inter-process communication","Kepler (microarchitecture)","Mathematical optimization","Program optimization","Programmer","Sequence alignment","Shared memory","Smith–Waterman algorithm","String searching algorithm","Whole genome sequencing"],"journalVolume":"","journalPages":"72-81","pmid":"","year":2017,"outCitations":["2ed5d6b35f8971fb9d7434a2683922c3bfcc058e","755e4ad5468747b31b9d6994885b17ad957dc9d7","034b5cb0eb2506096ae6f30790834b4af0da9158","4426da11616bc819b90f8e2413e6850c69cd02a6","d1eabca6c7e931132e74148f5b0f58e2f8e702e2","40c5441aad96b366996e6af163ca9473a19bb9ad","69f1bdb0d46a1597eff3e8a4b30b1a87b0e58c06","950bca8374bf36421957b416e4f58425e9d43095","f455bf8a9ab7ef837dc97d2fe55b92fbc81f04b9","0c7768ed7abec93bd9db840b64dae520b3c368ab","90dc9aa407a46c6c47dc25f21c44fb1d46f21db3"],"s2Url":"https://semanticscholar.org/paper/4f4f814a1cbeebf835464868b7d8cfe394b2632b","s2PdfUrl":"","id":"4f4f814a1cbeebf835464868b7d8cfe394b2632b","authors":[{"name":"Jie Wang","ids":["1729576"]},{"name":"Xinfeng Xie","ids":["3190420"]},{"name":"Jason Cong","ids":["2259796"]}],"journalName":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","paperAbstract":"Data movement is increasingly becoming the bottleneck of both performance and energy efficiency in modern computation. Until recently, it was the case that there is limited freedom for communication optimization on GPUs, as conventional GPUs only provide two types of methods for inter-thread communication: using shared memory or global memory. However, a new warp shuffle instruction has been introduced since the Kepler architecture on Nvidia GPUs, which enables threads within the same warp to directly exchange data in registers. This brought new performance optimization opportunities for algorithms with intensive inter-thread communication. In this work, we deploy register shuffle in the application domain of sequence alignment (or similarly, string matching), and conduct a quantitative analysis of the opportunities and limitations of using register shuffle. We select two sequence alignment algorithms, Smith-Waterman (SW) and Pairwise-Hidden-Markov-Model (PairHMM), from the widely used Genome Analysis Toolkit (GATK) as case studies. Compared to implementations using shared memory, we obtain a significant speed-up of 1.2&#x00D7; and 2.1&#x00D7; by using shuffle instructions for SW and PairHMM. Furthermore, we develop a performance model for analyzing the kernel performance based on the measured shuffle latency from a suite of microbenchmarks. Our model provides valuable insights for CUDA programmers into how to best use shuffle instructions for performance optimization.","inCitations":["289f1567dafdadb4209e5302e31d9364e1fab46e"],"pdfUrls":["http://vast.cs.ucla.edu/sites/default/files/publications/ipdps-submission.pdf","https://doi.org/10.1109/IPDPS.2017.79"],"title":"Communication Optimization on GPU: A Case Study of Sequence Alignment Algorithms","doi":"10.1109/IPDPS.2017.79","sources":["DBLP"],"doiUrl":"https://doi.org/10.1109/IPDPS.2017.79","venue":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)"},
{"entities":["Algorithm","Breadth-first search","Central processing unit","Computation","FLOPS","Graph drawing","Graph traversal","Heterogeneous computing","Intel Core (microarchitecture)","Linpack benchmarks","List of algorithms","Manycore processor","Maximum flow problem","Memory hierarchy","Performance per watt","Shortest path problem","Social network","Sunway","Sunway TaihuLight","TOP500","Traversed edges per second"],"journalVolume":"","journalPages":"635-645","pmid":"","year":2017,"outCitations":["569a19031523dbe03a8c9a3dbe6168912f3cc476","3ffe60733f14d9416ab478c1f273390601f987d8","175d795f44037ef60dd9df341701cd5fdc449f1f","4a5872f80d33be4d448abce21d121ec67453f5a2","ba75e4f7f6356d0c7a98ae813f085ce1a7a0aeec","b76269bf962989ce271bef7ea863ff4adf9c9de6","638deeb9efa10f081f74e6c2ee9195716afd2ceb","c3fbbd9c1fc5e53c6a9e3fe27e1bfce4755c8ef3","3c84f22df1948dfa8b1b14bbd4c850baf9c5b632","455ecea199bc83ec6ee3667afc96ef5f58f2b0ce","0a791a760dd883342c8b8456a3e7cb75fb996ef4","5def0f95a08f4c8f2592ba3323f4f92a6c367335","06f75b1b283569baf96f4a65ec7da734b9c840f8","1e27b9b447cebd5047050e39bb9246fa6364b760","a0618a3d620a4e61ba37b691800fc770e0a77a65","0e939cfcf31d94e27a51fd894e32d62737eb00c6","2ce27845038020ea43afa08e91f916a4ccf19924","02a45f2bf6105bf83e605812735ffa8eb8db520e","4a87972b28143b61942a0eb011b60f76be0ebf2e","2724de31317b1b9e026b5f90251829ee02f3fa3f","254ded254065f2d26ca24ec024cefd7604bd74e7","15b61dedce6c53245249a33e096ccce071d52edc","7ebb9fad71ce8e08d5284b7644a5452cff6c75b3","4ad495b07abc0d7080c020dd563d9406e1753d65","259e93de2f10d395a1bdfb2dc6da72b6a3998572"],"s2Url":"https://semanticscholar.org/paper/2a6f1f5779034004de9b53ebdcac4fd57771941f","s2PdfUrl":"","id":"2a6f1f5779034004de9b53ebdcac4fd57771941f","authors":[{"name":"Heng Lin","ids":["2569311"]},{"name":"Xiongchao Tang","ids":["1928916"]},{"name":"Bowen Yu","ids":["38849012"]},{"name":"Youwei Zhuo","ids":["10716503"]},{"name":"Wenguang Chen","ids":["6301522"]},{"name":"Jidong Zhai","ids":["2467444"]},{"name":"Wanwang Yin","ids":["22987925"]},{"name":"Weimin Zheng","ids":["2225511"]}],"journalName":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","paperAbstract":"Interest has recently grown in efficiently analyzing unstructured data such as social network graphs and protein structures. A fundamental graph algorithm for doing such task is the Breadth-First Search (BFS) algorithm, the foundation for many other important graph algorithms such as calculating the shortest path or finding the maximum flow in graphs. In this paper, we share our experience of designing and implementing the BFS algorithm on Sunway TaihuLight, a newly released machine with 40,960 nodes and 10.6 million accelerator cores. It tops the Top500 list of June 2016 with a 93.01 petaflops Linpack performance [1]. Designed for extremely large-scale computation and power efficiency, processors on Sunway TaihuLight employ a unique heterogeneous many-core architecture and memory hierarchy. With its extremely large size, the machine provides both opportunities and challenges for implementing high-performance irregular algorithms, such as BFS. We propose several techniques, including pipelined module mapping, contention-free data shuffling, and group-based message batching, to address the challenges of efficiently utilizing the features of this large scale heterogeneous machine. We ultimately achieved 23755.7 giga-traversed edges per second (GTEPS), which is the best among heterogeneous machines and the second overall in the Graph500s June 2016 list [2].","inCitations":["e45dea6588d1de0a23618e019031e67eedeeee26","896134c7aa767e27cb3c3aa0662b335473923602"],"pdfUrls":["https://doi.org/10.1109/IPDPS.2017.53","http://alchem.usc.edu/~youwei/publications/2017.ipdps.pdf"],"title":"Scalable Graph Traversal on Sunway TaihuLight with Ten Million Cores","doi":"10.1109/IPDPS.2017.53","sources":["DBLP"],"doiUrl":"https://doi.org/10.1109/IPDPS.2017.53","venue":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)"},
{"entities":["Algorithm","Asynchronous system","Central processing unit","Communications protocol","Data deduplication","Message authentication code"],"journalVolume":"","journalPages":"327-336","pmid":"","year":2017,"outCitations":["c99c2423fece8b5c20a78f8b59d3ce88ca4d9bc4","1233c76ac4a09402e88841f31b6ae0eaaf76f615","b3127883529ac69477b684d520a817e85aced533","4c6906ea491a47bd50a18afad6c49797cba904f5","2c93d82fb103f9a82dd3898bcccab07ce2a5a78c","85920e3c65146d0d250ae247ca672c6393a41488","1bb5295a79dc329fd271b5f2cf67509fc9ea3f93","2449f426a3ce215f9c089d7851e53e6a0169a205","54dc2b248271142c47b264b7a83e01523e0f30e4","4a7be5fe08d86fab74160b3ea9ee359f34b7c5d7","d6c68279828d74b5e58d7595a3f25d91e8729643","6e0ea0553929d2399a75efe392d6176b98cb6049","1e63b8495e6ee76dfaf3d86b7de1badd5a05804f","81f3b90935a63baa419af55e95821e7444787007","d9bb9caec3563bc9a9b077a238087e68cba1319c","a90ef843d5e0faae4d686c1261eab5bcb547e36d"],"s2Url":"https://semanticscholar.org/paper/53fe05be951fb9404bf844e089f04aa328d1784a","s2PdfUrl":"","id":"53fe05be951fb9404bf844e089f04aa328d1784a","authors":[{"name":"Keishla D. Ortiz-Lopez","ids":["19232880"]},{"name":"Jennifer L. Welch","ids":["1717914"]}],"journalName":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","paperAbstract":"In the reliable message transmission problem (RMTP) processors communicate by exchanging messages, but the channel that connects two processors is subject to message loss, duplication, and reordering. Previous work focused on proposing protocols in asynchronous systems, where message size is finite and sequence numbers are bounded. However, if the channel can duplicate messages-but not lose them-and arbitrarily reorder the messages, the problem is unsolvable. We consider a strengthening of the asynchronous model in which reordering of messages is bounded. In this model, we develop an efficient protocol to solve the RMTP when messages may be duplicated but not lost. This result is in contrast to the impossibility of such an algorithm when reordering is unbounded. Our protocol has the pleasing property that no messages need to be sent from the receiver to the sender and it works when message loss is allowed with some minimal modifications.","inCitations":[],"pdfUrls":["https://doi.org/10.1109/IPDPS.2017.14"],"title":"Bounded Reordering Allows Efficient Reliable Message Transmission","doi":"10.1109/IPDPS.2017.14","sources":["DBLP"],"doiUrl":"https://doi.org/10.1109/IPDPS.2017.14","venue":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)"},
{"entities":["Blocking (computing)","Computer data storage","Data structure","Insertion sort","Library (computing)","Non-blocking algorithm","Random access","Server (computing)","Thread (computing)","Time complexity"],"journalVolume":"","journalPages":"917-926","pmid":"","year":2017,"outCitations":["045a975c1753724b3a0780673ee92b37b9827be6","6ed3f7f2cb4acc63dc71312afd931d899e24236a","14f2ab7b89c9f508f9e886e4fd5bb702c867a190","135772775121ba60b47b9f2f012e682fe4128761","05c512b4ef4bb2209a302e4bb655c8a1c4cc6716","415e5008232116e6869caf29c349a2dfe390264e","042f443418ff2ff98a1dccbf49df9fa258dab707","57eaf0036c74895a5e965915c6544041623719e0","942f2a6df29234c304b69129872835d60cf5e9e9","6261748cf3c225c89ccaeca15349bec7e5eaca4d","1d1c68d07c4738e321a3db24fede081e95baff2c","4e3304e77dd2fecea4086e132981d1470434cf65","4a418603a5820524987bf82085dcc162fb7f9f2c","68a9005a5ec10daece36ca5ecb9cad7be44770b1","ff5e8b9972294f600b4de1a0fbb85df3a5b1bc31","363b85f61630ebdc1194a59816ad950bf305c40a","42142c121b2dbe48d55e81c2ce198a5639645030","62bd72d7a4160bd1a35191c51137d11cfbe30cf7","6db9bd41b294a7b45792b8f4ac8864f5d178f35e","6808efa6321651d80881e5718b488817100f8b61","0e422bd90c8be636358d4eb75f05276b361d19d4","2e6177199748bf6cec5b80c87c3bf2816706f1f0","30df50d77ef9478a2848626dfe3bf65f3c991991","1cb0679ae82be093268747da0f634281ea6a41df"],"s2Url":"https://semanticscholar.org/paper/51ac7123db308803413c9c408a377dcd4bc19bee","s2PdfUrl":"","id":"51ac7123db308803413c9c408a377dcd4bc19bee","authors":[{"name":"Ivan Walulya","ids":["2607427"]},{"name":"Philippas Tsigas","ids":["1701362"]}],"journalName":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","paperAbstract":"Dynamic vectors are among the most commonly used data structures in programming. They provide constant time random access and resizable data storage. Additionally, they provide constant time insertion (pushback) and deletion (popback) at the end of the sequence. However, in a multithreaded system, concurrent pushback and popback operations attempt to update the same shared object, creating a synchronization bottleneck. In this paper, we present a lock-free vector design that efficiently addresses the synchronization bottlenecks by utilizing a combining technique on pushback operations. Typical combining techniques come with the price of blocking. Our design introduces combining without sacrificing lock-freedom. We evaluate the performance of our design on a dual socket NUMA Intel server. The results show that our design performs comparably at low loads, and out-performs prior concurrent blocking and non-blocking vector implementations at high contention, by as much as 2.7x.","inCitations":[],"pdfUrls":["https://doi.org/10.1109/IPDPS.2017.73"],"title":"Scalable Lock-Free Vector with Combining","doi":"10.1109/IPDPS.2017.73","sources":["DBLP"],"doiUrl":"https://doi.org/10.1109/IPDPS.2017.73","venue":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)"},
{"entities":["Algorithm","Benchmark (computing)","Blocking (computing)","Cache (computing)","Graph drawing","Hardware performance counter","List of algorithms","Locality of reference","Loop nest optimization","PageRank","Principle of locality","Program optimization","Run time (program lifecycle phase)","Sparse matrix","Synthetic data","Vertex (geometry)"],"journalVolume":"","journalPages":"820-831","pmid":"","year":2017,"outCitations":["141e35263ab810983c90d47ad62eb4fab5e51717","b6b6d2504fd57d27a0467654fa62169cc7dedbdd","3182511c054dac8308a08b408b55ed9520650d27","6b6ca1041dbcf0ff44992f02826342e99da54996","4139eedda8717ffd60052f68ed78b996aaebfced","3339acf7d66a3818bf3eaebdb685ea57d6d62e14","0c0800259bd40b1ac96cc437629c5ea0ad729f22","2d7bf91ca184def17e15bf515532651fd5fe5f01","adafc767bbcb5c196bc7a3e6f252aa67489375c0","4dc578364f357b993b5554b9181c90c84aa6b4d1","5f8dbd927b2be3269624f41d13ae10af2245ff7d","0706356c9ab6014d6b04577d38289ea8328291a5","71affe0d9489be0ecba667f568b1a0bcd9ee3af3","b513711621e81d0abd042e0877ca751581a993f5","05c9330f261ed3f5aecbca28004206d9a029656d","3d985a05e4a49be71d497e7a2ff3fcbeb74c4bc8","eb82d3035849cd23578096462ba419b53198a556","2c394b418715072c01309b646f2535ad734d8c3e","1c872c3f1f74a63f2cdca336409e4755e5198ceb","829bb25d7b86a990232a392d468b0f0999c1939b","5c0d56404b4e21d0e485c2e08abda2d12ae7b953","3486aeaf540c48952120fe853d672af984f40a6a","8572f800eeaae01b7faf7be62e041e3d08ea83ec","4f3caa5573b4c1ebef7c3ee6b9f7643e689c858e","5e762186f9710c3e357195d22488b5616d574da6","55b3e22b56599ed8520deb1d7cb9ac460f4fa6bb","1ef7f02bce931c8e9ef529e095b274132ce4011a","2b1ec3fdf5b695de2d7ec17393ec0ad9445ceb61","3b874ce8d1fedd7f1f31a3c5ec495f4907b59da7","26deee037b221bd05ed34461819f5c067b745445","0271252d3044a646734988e02e0257c24ff6dcaa","272550f6745acba4da9a10ab29ba738cb2c19d3b","28e34059176c36934de116e138dd53cf4ee1dff0","074d096a54bf6bb33c59f628206848c7724a7cf3"],"s2Url":"https://semanticscholar.org/paper/652640d1226131fbeb66aba6eab681196c2d5222","s2PdfUrl":"","id":"652640d1226131fbeb66aba6eab681196c2d5222","authors":[{"name":"Scott Beamer","ids":["2734356"]},{"name":"Krste Asanovic","ids":["1760896"]},{"name":"David A. Patterson","ids":["1701130"]}],"journalName":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","paperAbstract":"Reducing communication is an important objective, as it can save energy or improve the performance of a communication-bound application. The graph algorithm PageRank computes the importance of vertices in a graph, and it serves as an important benchmark for graph algorithm performance. If the input graph to PageRank has poor locality, the execution will need to read many cache lines from memory, some of which may not be fully utilized. We present propagation blocking, an optimization to improve spatial locality, and we demonstrate its application to PageRank. In contrast to cache blocking which partitions the graph, we partition the data transfers between vertices (propagations). If the input graph has poor locality, our approach will reduce communication. Our approach reduces communication more than conventional cache blocking if the input graph is sufficiently sparse or if number of vertices is sufficiently large relative to the cache size. To evaluate our approach, we use both simple analytic models to gain insights and precise hardware performance counter measurements to compare implementations on a suite of 8 real-world and synthetic graphs. We demonstrate our parallel implementations substantially outperform prior work in execution time and communication volume. Although we present results for PageRank, propagation blocking could be generalized to SpMV (sparse matrix multiplying dense vector) or other graph programming models.","inCitations":["c18d8148b38862793f3e319e044e8b46ed8ba585","d72293a7858d27058eac1690e6a3739db4d9bd97","854dfd36420497b6aeae18f0178272588497c2b3","348119d77d127dba6058802c12f98f06c8849f3d"],"pdfUrls":["http://www.scottbeamer.net/pubs/beamer-ipdps2017.pdf","https://doi.org/10.1109/IPDPS.2017.112"],"title":"Reducing Pagerank Communication via Propagation Blocking","doi":"10.1109/IPDPS.2017.112","sources":["DBLP"],"doiUrl":"https://doi.org/10.1109/IPDPS.2017.112","venue":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)"},
{"entities":["Algorithm","Leader election","Mobile operating system","Mobile phone","Network topology","Operating system","Polylogarithmic function","Push–pull output","Smartphone","Time complexity"],"journalVolume":"","journalPages":"172-181","pmid":"","year":2017,"outCitations":["718bba0f9b305c9bcbb332d1e12f87949d97cf95","3045ab5550d9a5d1cd30f37a0547b956f570f14c","13dff6a28d24e4fe443161fcb7d96b68a085a3d4","4ef3133817267e0829bbff3e2024af93403dddc7","52e3d54c7fe011413a4f1cb555e6374d10765a22","3006fbb09724024d98a9c59b70bc03ac14ba3193","a9ea13a34553e3c4fce7abf46dd5999f8bc73cd9","10298ef457eb38f0775326b6f2d245f1b8121bb5","5928dd51e1d7d940d528ffc0455cab8248c551bc","0967bd75632d959541ee4afef35a5ef37c805cc7","f0b657949ff1014e0a69b6c985ba9605b76792bd","48adc076d7c2c5a74323f8dd61ffa32be706d982","2ce42a99cf15fffc3babe6aa35e520deb37f212e","6f8c546b574ff16a800d202d51900cc1e56e4e94","19ae27ba71869cc4328fe428eddec223a5cb2a7d","e70f108961a7b250d3a77ca4a16c8e65626e96e6","bb92a3071b138f9e7c21e11e475ee3b3ab715da3","cdfd5de78df6a2b97b05001de962c7112c736a51"],"s2Url":"https://semanticscholar.org/paper/84521e6d27c705ca3f97e20bbcc3e80a29779f78","s2PdfUrl":"","id":"84521e6d27c705ca3f97e20bbcc3e80a29779f78","authors":[{"name":"Calvin C. Newport","ids":["1779678"]}],"journalName":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","paperAbstract":"In this paper, we study the fundamental problem of leader election in the mobile telephone model: a recently introduced variation of the classical telephone model modified to better describe the local peer-to-peercommunication services implemented in many popular smartphone operating systems. In more detail, the mobile telephone model differs from the classical telephone model in three ways: (1) each devicecan participate in at most one connection per round; (2) the network topology can undergo a parameterizedrate of change; and (3) devices can advertise a parameterized number of bits to their neighbors in each round before connection attempts are initiated. We begin by describing and analyzing a new leader election algorithm in this model that works under the harshest possible parameter assumptions: maximum rate of topology changes and no advertising bits. We then apply this result to resolve an open question from [Ghaffari, 2016] on the efficiency of PUSH-PULL rumor spreading under these conditions. We then turn our attention to the slightly easier case where devices can advertise a single bit in each round. We demonstrate a large gap in time complexity between these zero bit and one bit cases. In more detail, we describe and analyze a new algorithm that solves leader election with a time complexitythat includes the parameter bounding topology changes. For all values of this parameter, this algorithm is faster than the previous result, with a gap that grows quickly as the parameter increases (indicating lower rates of change). We conclude by describing and analyzing a modified version of this algorithmthat does not require the assumptionthat all devices start during the same round. This new version has a similar time complexity (the rounds required differ only by a polylogarithmic factor),but now requires slightly larger advertisement tags.","inCitations":["777b13a1efbc4e6fbacdc1bc4c2cf4987880bfa9"],"pdfUrls":["https://doi.org/10.1109/IPDPS.2017.11","http://people.cs.georgetown.edu/~cnewport/pubs/le-IPDPS2017.pdf"],"title":"Leader Election in a Smartphone Peer-to-Peer Network","doi":"10.1109/IPDPS.2017.11","sources":["DBLP"],"doiUrl":"https://doi.org/10.1109/IPDPS.2017.11","venue":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)"},
{"entities":["Algorithm","Autonomous car","Autonomous robot","Latent class model","Mobile robot","Robot","Visibility (geometry)"],"journalVolume":"","journalPages":"513-522","pmid":"","year":2017,"outCitations":["2beabb0950823dbe5f9fddf546626a8c4acea2ed","b7faa8135d1159d0356c0710ea267adf43852dab","7ab9de59820edc16d34429ac2ec77c8ff60b1486","09932fd31e24c867952a7ad4c0277ee9dbdc6da5","2a62bd4483ea6249455ad969bc50b21795e0fe6d","d349cfd12b4c108e8b40983bbe4552d875d3761a","7670a03964a3d658b3e962fc085f5af0738d7d36","3cf87d1cbb7dff05e0bd302bf83f7601310bcb37","078a5c035a62f81b57105dd2e85c0a606a702709","159e32e3bf9f0ddd5a5aa987a5d036345036556f","75116d7c8d8c02eea1be9caf319f1e1f1ab6f715","874381fff381d10cfbe2e795d6c543713798e228","3db6d7915cf6487148244049b8b12c2d66fb9a27","07065b5481b0ee7599965b03df3a4b6e4c990577","38f69e92858fb51ece47c5a2de5390607829af79","0b74247faf95970ad8a0614efc91ec9305a38446","e9b1ae133ae34f8348c19c1c1066439eddbf9ab0","3c089d795f5a8855ce187c7d151b632a663d619b","25e33d2f1876e9a8393345b44960277acf2edee4","5a75bdd5cea284804e220126603805a0e3a0710f"],"s2Url":"https://semanticscholar.org/paper/c25e4858cda0027a5a8b5383b1fd0648a6e68119","s2PdfUrl":"","id":"c25e4858cda0027a5a8b5383b1fd0648a6e68119","authors":[{"name":"Gokarna Sharma","ids":["1749522"]},{"name":"Ramachandran Vaidyanathan","ids":["1725541"]},{"name":"Jerry L. Trahan","ids":["2582238"]},{"name":"Costas Busch","ids":["1932642"]},{"name":"Suresh Rai","ids":["39840790"]}],"journalName":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","paperAbstract":"We consider the distributed setting of N autonomous mobile robots that operate in Look-Compute-Move (LCM) cycles and communicate with other robots using colored lights (the robots with lights model). We study the fundamental problem of repositioning N autonomous robots on a plane sothat each robot is visible to all others (the Complete Visibility problem) on this model; a robot cannot see another robot if a third robot is positioned between them on the straight line connecting them. There exists an O(1) time, O(1) color algorithm for this problem in the semi-synchronous setting. In this paper, we provide the first O(log N) time, O(1) color algorithm for this problem in the asynchronous setting. This is a significant improvement over an O(N)-time translation of the semi-synchronous algorithm to the asynchronous setting. The proposed algorithm is collision-free - robots do not share positions and their paths do not cross.","inCitations":["4cec08b724bad0607de0903401fe640f0eef76be"],"pdfUrls":["https://doi.org/10.1109/IPDPS.2017.51"],"title":"O(log N)-Time Complete Visibility for Asynchronous Robots with Lights","doi":"10.1109/IPDPS.2017.51","sources":["DBLP"],"doiUrl":"https://doi.org/10.1109/IPDPS.2017.51","venue":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)"},
{"entities":["MapReduce","Program optimization","Supercomputer"],"journalVolume":"","journalPages":"1098-1108","pmid":"","year":2017,"outCitations":["0cd87f8454774bf494bf62a58c137ca9b848d0b4","24281c886cd9339fe2fc5881faf5ed72b731a03e","3aa1bc5f67254b4e2d86170b70adfacf937008f6","500b80adc7e25dfffa9a05d25bdffce81b1b0031","2cdcddb08ae6060e94cba6c9b2b58b87324e686f","261acc48d66031bec58d95623e89d298349937a8","1c0a70a3e34b7071a896be66a923ed776c4c3e97","5d5e93cf6e4a595ba6abe97c852ca7298639b6cd","947c6bf534ccd620044f77c3bd6068f633b421fb","436373807a0a9dc8660e7739e018d18cc18dacd7","0541d5338adc48276b3b8cd3a141d799e2d40150","94ff8cd9e59ec747bdad91835f089a33819c0cb5","1f0e31b52967090e265218ae77b7fd332621a627","32ec06c8cd5cea328af79660c188c56c0b01b5b3","56a3854d0d3a60cf10289724ead0c254df6e6836","911bac2a9205ed5d1178460d269ff0ab109635cc","1087bbef784e7daecaf13b58bc1480d6dee4929b","8dd97ace0d9bddaaa7004c7325f30c2145fbe41f","898634f0e693cb521ad2dd4a7432c11381e6df60","9c3cc7337f7d70593a1ff8622de3128e1708b5a2","7a0868597edad12564839bf0fcbf6b8f3ad36818","70bd563d00fcb402eb7d9f251bea544ecb08f213","145088fc0593b2f95168f3ba4693bbc5487e9068","169d5a4c6281ed5cb0b37b51cb80cc730d7731a0"],"s2Url":"https://semanticscholar.org/paper/8fc351a96549280a5df3b3671af1d37ffeb52782","s2PdfUrl":"","id":"8fc351a96549280a5df3b3671af1d37ffeb52782","authors":[{"name":"Tao Gao","ids":["2229502"]},{"name":"Yanfei Guo","ids":["1794267"]},{"name":"Boyu Zhang","ids":["1788289"]},{"name":"Pietro Cicotti","ids":["1758014"]},{"name":"Yutong Lu","ids":["35150586"]},{"name":"Pavan Balaji","ids":["2103230"]},{"name":"Michela Taufer","ids":["1737658"]}],"journalName":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","paperAbstract":"In this paper we present Mimir, a new implementation of MapReduce over MPI. Mimir inherits the core principles of existing MapReduce frameworks, such as MR-MPI, while redesigning the execution model to incorporate a number of sophisticated optimization techniques that achieve similar or better performance with significant reduction in the amount of memory used. Consequently, Mimir allows significantly larger problems to be executed in memory, achieving large performance gains. We evaluate Mimir with three benchmarks on two highend platforms to demonstrate its superiority compared with that of other frameworks.","inCitations":["8ec36ef8d8ddfa6b6f842336b1f0d93d8dd21da0"],"pdfUrls":["https://doi.org/10.1109/IPDPS.2017.31"],"title":"Mimir: Memory-Efficient and Scalable MapReduce for Large Supercomputing Systems","doi":"10.1109/IPDPS.2017.31","sources":["DBLP"],"doiUrl":"https://doi.org/10.1109/IPDPS.2017.31","venue":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)"},
{"entities":["Algorithm","Analysis of parallel algorithms","Bidiagonalization","Critical path method","Distributed shared memory","Experiment","Greedy algorithm","Multi-core processor","QR code","QR decomposition","R language","Shared memory","Tiled web map"],"journalVolume":"abs/1611.06892","journalPages":"","pmid":"","year":2016,"outCitations":["30555c7ea92f59a9b2d3455ea98b1138015dce37","3314470a6c5e3c55391eda0e43d4e7f1adc9a89f","8c280d3ae4c377fed1565132272c3f0a12efe5dd","5c85d83c341357b6d17883cace475b87817aa84c","c1d28d2ec0416de3ffb019c5066fb81090c25827","904e2ac8131bf903f21e8c1fa9938978303db03b","a3014174676cdbd83bce72b0e8fae5a654d68c76","64b3435826a94ddd269b330e6254579f3244f214","35280a0dabe96002c5626249834d89d176d11785","0199bccf87b17291be6d8823152eabad2be4f242","05da43f0b23936c4970d40db13a927b747241f7b","0a5617cf569abe3c669a71f4c604d47ca334ae12","0cea54c9ff794629795ee819681cb0e54dbfec84","8112c4305b88d85199267e9e03d3a0aca4432059","fe8ec8d8f1d43fbc3a2909ffd8a9f849facd849f","96089f345c7436356cd2e48441183d57fe8b1ef3","82292f38366cbe3167c9de2d71ce86c75fba78a9","017ebdc37a3f36236fd9cf3f43b369937e4da3c0","fec9ce47524e65b89c20d4dc1671c5b1a7a0a41d","78c5054ea4414d4d4040c17552b8d52469ddcec2","05064b678d3bb00397f897125da0f6168c8a5290","0952f0c177df8a2dc8bbb3d3145c4f5f086efc1e","f6430121b2af7d55b090a1c260570630e6cf1f41","ebbfb679e478ed7519d1a7108d0efdc9abb99a70","9cb5af4ec44a08510a31d5a6e4856152df89cd63","95dbfb1ccad4fd46daf9e06153aff5a4effaa129","825315415eba86846605512c31d8adaf173e6f8d","72d8587018e9aee30f7656c14e8265ada24bcf83","ad35fd818e6de7fa855f414f73888a9f4a72451b","cc12a7f07f4755ab2ab4c538941fa696b8643837","8c6cd84ad400a69e4e06be985e857888a9413d07","05d31db3f6d6265a30e82b9e89435cacc7618308","fb9416ccd43e5d3241a88c2dce9ab83fc3ab352d","34e9fbff05b850125bc61c04cfe76110bc16c3eb","0fc0098ffa8f513959279fe5bb74c8f450225924","0def25a673a09c6620485c78bbb075176f31062f"],"s2Url":"https://semanticscholar.org/paper/d82f427d3c56b879d6792c40741b5541a41244ab","s2PdfUrl":"http://pdfs.semanticscholar.org/d82f/427d3c56b879d6792c40741b5541a41244ab.pdf","id":"d82f427d3c56b879d6792c40741b5541a41244ab","authors":[{"name":"Mathieu Faverge","ids":["2351024"]},{"name":"Julien Langou","ids":["1786954"]},{"name":"Yves Robert","ids":["1735015"]},{"name":"Jack J. Dongarra","ids":["1708869"]}],"journalName":"CoRR","paperAbstract":"We consider algorithms for going from a “full” matrix to a condensed “band bidiagonal” form using orthogonal transformations. We use the framework of “algorithms by tiles”. Within this framework, we study: (i) the tiled bidiagonalization algorithm BiDiag, which is a tiled version of the standard scalar bidiagonalization algorithm; and (ii) the R-bidiagonalization algorithm R-BiDiag, which is a tiled version of the algorithm which consists in first performing the QR factorization of the initial matrix, then performing the band-bidiagonalization of the R-factor. For both bidiagonalization algorithms BiDiag and R-BiDiag, we use four main types of reduction trees, namely FlatTS, FlatTT, Greedy, and a newly introduced auto-adaptive tree, Auto. We provide a study of critical path lengths for these tiled algorithms, which shows that (i) R-BiDiag has a shorter critical path length than BiDiag for tall and skinny matrices, and (ii) Greedy based schemes are much better than earlier proposed variants with unbounded resources. We provide experiments on a single multicore node, and on a few multicore nodes of a parallel distributed shared-memory system, to show the superiority of the new algorithms on a variety of matrix sizes, matrix shapes and core counts.","inCitations":["e9687c7f101aab7488d40174a14210dc0bc70e60","0930da5316f6f54fd61da83cb3fdc47bb1fca24f"],"pdfUrls":["https://arxiv.org/pdf/1611.06892v1.pdf","http://arxiv.org/abs/1611.06892"],"title":"Bidiagonalization with Parallel Tiled Algorithms","doi":"","sources":["DBLP"],"doiUrl":"","venue":"ArXiv"},
{"entities":["Artificial neural network","Convolution","Convolutional neural network","Deep learning","Double-precision floating-point format","Graphics processing unit","Pipeline (computing)","Program optimization","SW26010","Speedup","Sunway","Sunway TaihuLight","Supercomputer"],"journalVolume":"","journalPages":"615-624","pmid":"","year":2017,"outCitations":["39f63dbdce9207b87878290c0e3983e84cfcecd9","5c3785bc4dc07d7e77deef7e90973bdeeea760a5","4fe1c707a48869cbbdf3eb0384e526d1d294f7e2","c382406fd8db2744b2a609837395e5da05e1d2ed","402da07a0ac4645e26370ff5ac8ab3540257a8ab","81b7dcaef4a53daab41658a4d1e97972d04b3384","1740eb993cc8ca81f1e46ddaadce1f917e8000b5","4788873f23fbfbca24744f0fa0d8e602c9403fba","9952d4d5717afd4a27157ed8b98b0ee3dcb70d6c","c2c10045880d31dc011fb2ff2935f910f9fcd182","14b5e8ba23860f440ea83ed4770e662b2a111119","23d14ab0f18fa881a2ac8ae027be6b9f2c91d74d","9f3cc03b1c9fc9c3e080e42a0ddd34cdd24a20fb","6f4d58486b1c6d710586b1d182ddad7d09a8da11","68837728232463651283edbb7ef0c93b2f502b2b","722fcc35def20cfcca3ada76c8dd7a585d6de386","b76269bf962989ce271bef7ea863ff4adf9c9de6","b4eac8295c90dbfb7d8d22ba560e025621287c58","092217c2267f6e0673590aa151d811e579ff7760","2ffc74bec88d8762a613256589891ff323123e99","1d696a1beb42515ab16f3a9f6f72584a41492a03","31868290adf1c000c611dfc966b514d5a34e8d23","326d65827307862ddc3d39b84ebc662e83ff95b3","061356704ec86334dbbc073985375fe13cd39088"],"s2Url":"https://semanticscholar.org/paper/7dd462e1dcb0d348e6d35d2f76ab22a812b1da34","s2PdfUrl":"","id":"7dd462e1dcb0d348e6d35d2f76ab22a812b1da34","authors":[{"name":"Jiarui Fang","ids":["2338627"]},{"name":"Haohuan Fu","ids":["1711877"]},{"name":"Wenlai Zhao","ids":["2694567"]},{"name":"Bingwei Chen","ids":["6464022"]},{"name":"Weijie Zheng","ids":["2793777"]},{"name":"Guangwen Yang","ids":["1689072"]}],"journalName":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","paperAbstract":"To explore the potential of training complex deep neural networks (DNNs) on other commercial chips rather than GPUs, we report our work on swDNN, which is a highly-efficient library for accelerating deep learning applications on the newly announced world-leading supercomputer, Sunway TaihuLight. Targeting SW26010 processor, we derive a performance model that guides us in the process of identifying the most suitable approach for mapping the convolutional neural networks (CNNs) onto the 260 cores within the chip. By performing a systematic optimization that explores major factors, such as organization of convolution loops, blocking techniques, register data communication schemes, as well as reordering strategies for the two pipelines of instructions, we manage to achieve a double-precision performance over 1.6 Tflops for the convolution kernel, achieving 54% of the theoretical peak. Compared with Tesla K40m with cuDNNv5, swDNN results in 1.91-9.75x performance speedup in an evaluation with over 100 parameter configurations.","inCitations":["2505deb6860ef0af17eab6f5f7d2161f0c9db2d0","e45dea6588d1de0a23618e019031e67eedeeee26"],"pdfUrls":["https://doi.org/10.1109/IPDPS.2017.20"],"title":"swDNN: A Library for Accelerating Deep Learning Applications on Sunway TaihuLight","doi":"10.1109/IPDPS.2017.20","sources":["DBLP"],"doiUrl":"https://doi.org/10.1109/IPDPS.2017.20","venue":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)"},
{"entities":["Apache Hadoop","Apache Hive","Backward compatibility","Cloud computing","Commodity computing","Computational complexity theory","Experiment","Jumpstart Our Business Startups Act","Microsoft Azure","Query language"],"journalVolume":"","journalPages":"459-468","pmid":"","year":2017,"outCitations":["23de558a10458c1be3062412f134500605eada98","86c8f8c0cad85b189feade4b31f36d56ebd9f6c8","47947ed7d4c12855b1b5a4c4ec3123528761d64b","3aed29136db8f1e5c6a89fc22d3ae4b4926a3555","93e67a1f3f0371114055de4db489385ea133ebc3","706cd2c450fb7054c92916b300513a266a207652","0541d5338adc48276b3b8cd3a141d799e2d40150","25233d201be5af4e1e8926d742af678ca5938223","797d93472c6aed26056de317c4a4cae0fd6e65aa","0fa5455a3241fca461be6c14d0f296c394cadd85","090030e0d1aa117008e9e9fa4abdee0a95455f4a","7c3e88b0c762065bd0d974cb3d67a1e61479f647","7207036cfb1e5e0aa74756e395dfc9dd94e46af0","e20b3b988310a436d38a2fc310dbcc3b12f5a54d","d24bbf5ad9b068aa476d30e3bb898c2e99942744","089c89f54c5dd0d2a873fbfc19183667d3be5b66","2f79404e566af175f03e94827383c3d7b43f4e31","029068a33f6e9f9ba0ddfe5498a67e4c0d349d2f","2988e34168fa91398fa397baf823af2063893e9c","16139ba6fa6ad2828c20abdf5d9f34687836f932"],"s2Url":"https://semanticscholar.org/paper/11c57b6e63184c35fd1999a19c7969170f038973","s2PdfUrl":"","id":"11c57b6e63184c35fd1999a19c7969170f038973","authors":[{"name":"Hong Zhang","ids":["1734058"]},{"name":"Hai Huang","ids":["1723513"]},{"name":"Liqiang Wang","ids":["9467302"]}],"journalName":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","paperAbstract":"Data have been generated and collected at an accelerating pace. Hadoop has made analyzing large scale data much simpler to developers/analysts using commodity hardware. Interestingly, it has been shown that most Hadoop jobs have small input size and do not run for long time. For example, higher level query languages, such as Hive and Pig, would handle a complex query by breaking it into smaller adhoc ones. Although Hadoop is designed for handling complex queries with large data sets, we found that it is highly inefficient to operate at small scale data, despite a new Uber mode was introduced specifically to handle jobs with small input size. In this paper, we propose an optimized Hadoop extension called MRapid, which significantly speeds up the execution of short jobs. It is completely backward compatible to Hadoop, and imposes negligible overhead. Our experiments on Microsoft Azure public cloud show that MRapid can improve performance by up to 88% compared to the original Hadoop.","inCitations":["3ebb5abf41521032df5ace422a3fe696ea5f87ef","7222cea092f37acac75488e425a029758677ef71","0a87b1b2e089d8cf80b4b26cbb191f4648937732","411c3c55361f2287b1a66cc52a93df1e7ed94863"],"pdfUrls":["http://www.cs.ucf.edu/~lwang/papers/IPDPS2017.pdf","https://doi.org/10.1109/IPDPS.2017.100"],"title":"MRapid: An Efficient Short Job Optimizer on Hadoop","doi":"10.1109/IPDPS.2017.100","sources":["DBLP"],"doiUrl":"https://doi.org/10.1109/IPDPS.2017.100","venue":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)"},
{"entities":["Algorithm","BLAST","Central processing unit","Computational biology","Data access","Database","Heuristic","Indexed search","Locality of reference","Memory hierarchy","Multi-core processor","Parallel computing","Peptide sequence","Speedup","Thread (computing)"],"journalVolume":"","journalPages":"62-71","pmid":"","year":2017,"outCitations":["dc0a162462fbc2d3be52e8882dfad60364cadccf","2878976835c73ed0906e17a7336b9bf10d491a1a","4520f74dbf413fe6b6480d0f243ee75fba1167a8","5cba0d330a947249af144b0402fe037a0c7166cf","53d1657eef932911c95ed051961c8136d34ba486","649e858552ed8a289b94ae1f33846b1255b3f07d","65b54461a0436e69969b2e2679dcbedcddd40d95","03deb21d07674506d11d46cc44672f1d9dc65fb0","3979cf5a013063e98ad0caf2e7110c2686cf1640","78be8c9517b822afa0ce858ff6d39a7854893e74","4b7f05a35378a0b17a0f9af3180d43cf7970aa15","1845fc6f7874f46777ebb5ecb37ad07eced75770","0cfd90449754c0ecdedf1d676d9094bc11612c28","583a320a9c612124d62da5741fede120495126fc","1915d4717bbb20849e733f711d194d198df45fe5","62c0af943a259c66b91dc932d3a5611afd014a4c","3e1a3b5741a0c4cf84dd6e8742f42e1b7cd5fdab","bcfbbe50b4b2b9b5c24b0628d31b2b03bf6cb274","0e2993ddba78626376651c3ab8d14f0d680f0595","4dc8f5d19d37e82f3ea0335f9b0c0eb914c166be","1b65277f50406900a475a68856df8fe8835c19be","28f33a53302b8ceae33997d7c94ef46ced26511e","102090e6e2363e094439a41ef0439dfac5da0126","6c9c8d72280301fe55bc2b6be3b271a448153bd8","7e39604a4b65b27da14200b23e950d350da649f5","3ad54fb5c05b4336fc291fe00a8acd2009e5afb5","6c0cc0d86586c6992c4eb940136e20af61252a84","ab756f4ed89c8e17632befe15c3579f0b9f04800","21e913af85844937f9862ac216e8f8509a6bd199","32d355a7a20f92ccda0608f83d7456870231c570","148c8255d4083270ad673ddf2619c867ff4c6839"],"s2Url":"https://semanticscholar.org/paper/310ecac3477a51aa303284f0853bd49ae8383ac3","s2PdfUrl":"","id":"310ecac3477a51aa303284f0853bd49ae8383ac3","authors":[{"name":"Jing Zhang","ids":["2266189"]},{"name":"Sanchit Misra","ids":["3215200"]},{"name":"Hao Wang","ids":["34266162"]},{"name":"Wu-chun Feng","ids":["1688860"]}],"journalName":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","paperAbstract":"Finding regions of local similarity between biological sequences is a fundamental task in computational biology. BLAST is the most widely-used tool for this purpose, but it suffers from irregularities due to its heuristic nature. To achieve fast search, recent approaches construct the index from the database instead of the input query. However, database indexing introduces more challenges in the design of index structure and algorithm, especially for data access through the memory hierarchy on modern multicore processors. In this paper, based on existing heuristic algorithms, we design and develop a database indexed BLAST with the identical sensitivity as query indexed BLAST (i.e., NCBI-BLAST). Then, we identify that existing heuristic algorithms of BLAST can result in serious irregularities in database indexed search. To eliminate irregularities in BLAST algorithm, we propose muBLASTP, that uses multiple optimizations to improve data locality and parallel efficiency for multicore architectures and multi-node systems. Experiments on a single node demonstrate up to a 5.1-fold speedup over the multi-threaded NCBI BLAST. For the inter-node parallelism, we achieve nearly linear scaling on up to 128 nodes and gain up to 8.9-fold speedup over mpiBLAST.","inCitations":["62c0af943a259c66b91dc932d3a5611afd014a4c","3b603228bf9419868e7518614c85338b7a132989"],"pdfUrls":["https://doi.org/10.1109/IPDPS.2017.120","http://synergy.cs.vt.edu/pubs/papers/zhang-seq-search-ipdps17.pdf"],"title":"Eliminating Irregularities of Protein Sequence Search on Multicore Architectures","doi":"10.1109/IPDPS.2017.120","sources":["DBLP"],"doiUrl":"https://doi.org/10.1109/IPDPS.2017.120","venue":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)"},
{"entities":["Algorithm","Artificial neural network","Auto-Tune","Coefficient","Computation","Machine learning","Ordinal data","Ordinal regression","Stencil code","Support vector machine","Test set"],"journalVolume":"","journalPages":"287-296","pmid":"","year":2017,"outCitations":["019ebe0205a759f8dab80b617f9f8ccd179c5c62","36ea668bb7617b9c1e6e98aebe96a0aaf90b569e","10d3e0f0648d0a5cfaebb3044ea7b14a52e54466","0064df0d06312711f5163c4440f3d7f099fc8d9e","6005fdb7813e0f07d90d6ed8e7beecd733ef4d04","220f5b0e74c7f1e71d6e23da672dcffbc9e6520a","1ccaac0fdcc5ab37a45d0cc616feeaa67a3d4ca1","677f01e5410d98c669a1d53e64b10f0533911829","7c0c47c6a2ade8f16ac5a00471348a4eb0bfa8ee","075d460a4737d7c0b3fd4b7aa03e315f7256b1af","1be3c6d7eb84bf88161c20f696a87dacd385d028","1b3c86ad6c149941750d97bd72b6b0122c1d8b5e","782d8591afd432a9b2bfe21553a4158a39cb9d1f","1426b32f40126a0a906121984918ff5fbcb0b4b3","64ac13c42f3ccfa987ad928c03cb0502b3baa7fc","998e6eb3d90327c38fdc7f680c75137e4976c679","98ba8f9863d92839b98f854eada60bfe87805526","4a2d7bf9937793a648a43c93029353ade10e64da","032857f750287c77349074587444ecd3166a4c09","427b168f490b56716f22b129ac93aba5425ea08f","14a477cf712ad5647180e6233dd0638c6c269fdd","648fe4e8d720c414e5edf1eb000cf84a9ae5046a","2c361ef5db3231d34656dd86d9b288397f0b929e","44efef85d56e61fb304f27010cc0d1bd80283a69","1214132530d108eff629ff18a9c05464f8003579","482f5e72c0a245a285ef198861e191fae73de481","2fac633f5167d54d94a9fe6a2532d1aef073980a","064f0793b2b7af8e8fccbf62bf39976dc4ff5b7a","1e716b39c74dd4967b16eb3a6a2e7220e4e07c97","dae60807ef1e6fd61a2362c8187b733b08121e1e","3c31999730ef19007df71909f1ae5223825e0ec9","5672ce28f2927b81b01303e4926643c55a4c8133","6472cab2678c39e2273673968c6d7d3cfe2a62c9","09b1520aea25ff0b5852d8a777e48eacf5300fac","2f7fa291bdc6a2f8c7994cf1896868f057a6b0ca","24fcc566953d80ca12d9e0e0315573083374eacf","366d02f9687a33b21079acc6d62ad755189a52f0","52d0aff3e4407302dd49123a8f87151bf94fdb52","06c15f48f0f71cb034936cee635bec0fc4992594","288b3c6605f3e46bf8b56aded52ed5f6c864f72a","0e12eb94aab5d64d08baacf0df36a4b7ed054c46","26d3c0e79adc665e12d848cd896fec6b6f0fed87","0f9080d297fc22dcf24dfd8ffcd3de5cea04c689","518cd72a5f12f050492b246ad300a46de7604af2","d32b7382787a3e969a1b8d3291c3480b8a1da545"],"s2Url":"https://semanticscholar.org/paper/560f706f2af9d300ab487314ed2c6284652d2ab3","s2PdfUrl":"","id":"560f706f2af9d300ab487314ed2c6284652d2ab3","authors":[{"name":"Biagio Cosenza","ids":["2302342"]},{"name":"Juan J. Durillo","ids":["21490411"]},{"name":"Stefano Ermon","ids":["2490652"]},{"name":"Ben H. H. Juurlink","ids":["1717074"]}],"journalName":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","paperAbstract":"Stencil computations expose a large and complex space of equivalent implementations. These computations often rely on autotuning techniques, based on iterative compilation or machine learning (ML), to achieve high performance. Iterative compilation autotuning is a challenging and time-consuming task that may be unaffordable in many scenarios. Meanwhile, traditional ML autotuning approaches exploiting classification algorithms (such as neural networks and support vector machines) face difficulties in capturing all features of large search spaces. This paper proposes a new way of automatically tuning stencil computations based on structural learning. By organizing the training data in a set of partially-sorted samples (i.e., rankings), the problem is formulated as a ranking prediction model, which translates to an ordinal regression problem. Our approach can be coupled with an iterative compilation method or used as a standalone autotuner. We demonstrate its potential by comparing it with state-of-the-art iterative compilation methods on a set of nine stencil codes and by analyzing the quality of the obtained ranking in terms of Kendall rank correlation coefficients.","inCitations":["4f4853e68da9aa09c0a8ebf4612ca725ce5d3521"],"pdfUrls":["http://www.biagiocosenza.com/papers/CosenzaIPDPS17.pdf","https://doi.org/10.1109/IPDPS.2017.102"],"title":"Autotuning Stencil Computations with Structural Ordinal Regression Learning","doi":"10.1109/IPDPS.2017.102","sources":["DBLP"],"doiUrl":"https://doi.org/10.1109/IPDPS.2017.102","venue":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)"},
{"entities":["Benchmark (computing)","Design of experiments","Heuristic","Iterator","Matrix multiplication","Time complexity","Tucker decomposition"],"journalVolume":"","journalPages":"1038-1047","pmid":"","year":2017,"outCitations":["048bfc88b9f54512304433bb2eeb68a3172159a8","53132a1619b13215bcd791cd6b850ff154f4f837","280bbaa66095fd6f89999003b802700935fdf77c","1322c225b4e05dc22bbff7c5b9f5464f3cb7754b","66479c2251088dae51c228341c26164f21250593","608109b7643145d3559c962041c76207a58a3b57","62dd02837c65b9c90de8d80c493f23ce1116cb3d","41cef633b01c5cae5c9dde2ccc06ffc15b93fb8f","07ed71b436b9adf23f0f93c8e4533461b82e769a","8526f7d58b58294521636d4709a08272e6f1f3c8","44ccdebc83766fb6a2016fa58c3c3a337356b79b","1d0f25989452abbbc8feaf00a034ff110fc4b350","0072eb224991ada6fc8a4e2d3465e4a51c0b26bc"],"s2Url":"https://semanticscholar.org/paper/1ac425def5f0de754c0a738cc8a528eaf9ab3381","s2PdfUrl":"","id":"1ac425def5f0de754c0a738cc8a528eaf9ab3381","authors":[{"name":"Venkatesan T. Chakaravarthy","ids":["1696818"]},{"name":"Jee W. Choi","ids":["14673425"]},{"name":"Douglas J. Joseph","ids":["34348330"]},{"name":"Xing Liu","ids":["1740904"]},{"name":"Prakash Murali","ids":["39229329"]},{"name":"Yogish Sabharwal","ids":["1787471"]},{"name":"Dheeraj Sreedhar","ids":["2178450"]}],"journalName":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","paperAbstract":"The Tucker decomposition expresses a given tensor as the product of a small core tensor and a set of factor matrices. Our objective is to develop an efficient distributed implementation for the case of dense tensors. The implementation is based on the HOOI (Higher Order Orthogonal Iterator) procedure, wherein the tensor-times-matrix product forms the core routine. Prior work have proposed heuristics for reducing the computational load and communication volume incurred by the routine. We study the two metrics in a formal and systematic manner, and design strategies that are optimal under the two fundamental metrics. Our experimental evaluation on a large benchmark of tensors shows that the optimal strategies provide significant reduction in load and volume compared to prior heuristics, and provide up to 7x speed-up in the overall running time.","inCitations":["0f0bcf003e7de278514dff084487873762b9ffb3","7c3c5b282948121244d330651e36b05f31c382cb"],"pdfUrls":["https://arxiv.org/pdf/1707.05594v1.pdf","http://arxiv.org/abs/1707.05594","https://doi.org/10.1109/IPDPS.2017.86"],"title":"On Optimizing Distributed Tucker Decomposition for Dense Tensors","doi":"10.1109/IPDPS.2017.86","sources":["DBLP"],"doiUrl":"https://doi.org/10.1109/IPDPS.2017.86","venue":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)"},
{"entities":["Byte","Byte addressing","Computer data storage","Data structure","Non-volatile memory","Nonvolatile BIOS memory","Null (SQL)","Persistence (computer science)","Program optimization","Run time (program lifecycle phase)","Testbed","Undo"],"journalVolume":"","journalPages":"1163-1173","pmid":"","year":2017,"outCitations":["dc2e2b794a784782d7d9860f1358aa107f71c1bf","94783d113951822195d4ba44599a8fcbdef9d4bf","0645f0f88e9a3cd6e9b1d0c21bc24666a7377666","b575f0d8b3eb38bcf0a1b99bad144002e96ffa18","c516d505dcee2faa0eea6b6a456fefa9451af12e","10d8afea57c8f159c4eb2664a40c8fb859acefef","11ef7c142295aeb1a28a0e714c91fc8d610c3047","2e663c1047ff14ddc2416229459922757a20edfb","0858a3b2b393ae083b3dbc4ded61c046d5ee04d2","0204f40221260d00c5ee63646560a40dcd7d97d1","27bcb72519d77192da2b30eca4e1442c8f3637b1","3a751a26108511e43d4130284ebed785e4c440ed","c44b97f870b862f7f6f8aebc9ffde4565dd64380","5dba3105fc05e6ba918106cb3f96d482c1a092f8","f8f52a402b8833ea1ad8eb34e48f011b25c0d306","6e0ade8e4c0948e47b7e1ad78eacf42e5f9d8d0f","243c522b56809292f1f50117a9915053d32bf4fb","05a1357946de5eca42a477b7b268db4944219a2e","165d99c9d30be5d301b998dc23c1a6a28fd0c425","bf5497e15f22233cbc2a4d0c3cc2c36f26738701","da8f5c3e65e2eb398dc5a4866023ef51e4056905","d7203f317b37d565ab54b6a48ef13ded3777eb78","565c290e4aa268619ecbbc27ea584de0f3525020","24724ad8962a9e04eb496fddaefe9708f6960601"],"s2Url":"https://semanticscholar.org/paper/ac265a5fbea3fde88456b5c78496567a2edec8e8","s2PdfUrl":"","id":"ac265a5fbea3fde88456b5c78496567a2edec8e8","authors":[{"name":"Joel Edward Denny","ids":["20645435"]},{"name":"Seyong Lee","ids":["8568681"]},{"name":"Jeffrey S. Vetter","ids":["7553591"]}],"journalName":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","paperAbstract":"Substantial advances in nonvolatile memory (NVM) technologies have motivated wide-spread integration of NVM into mobile, enterprise, and HPC systems. Recently, considerable research has focused on architectural integration of NVM and respective programming systems, exploiting NVM's trait of persistence correctly and efficiently. In this regard, we design several novel language-based optimization techniques for programming NVM and demonstrate them as an extension of our NVL-C system. Specifically, we focus on optimizing the performance of atomic updates to complex data structures residing in NVM. We build on two variants of automatic undo logging: canonical undo logging, and shadow updates. We show these techniques can be implemented transparently and efficiently, using dynamic selection and other logging optimizations. Our empirical results on several applications gathered on an NVM testbed illustrate that our cost-model-based dynamic selection technique can accurately choose the best logging variant across different NVM modes and input sizes. In comparison to statically choosing canonical undo logging, this improvement reduces execution time to as little as 53% for block-addressable NVM and 73% for emulated byte-addressable NVM on a Fusion-io ioScale device.","inCitations":["e3a66a2e79c2a5f7a52bf4e8089bf522fdc04abe","cb2a018979184f87692d423322e367cc42a215d2"],"pdfUrls":["https://doi.org/10.1109/IPDPS.2017.60"],"title":"Language-Based Optimizations for Persistence on Nonvolatile Main Memory Systems","doi":"10.1109/IPDPS.2017.60","sources":["DBLP"],"doiUrl":"https://doi.org/10.1109/IPDPS.2017.60","venue":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)"},
{"entities":["Aerial photography","Algorithm","Approximation","Approximation algorithm","Linear programming","Operations research","Optimal design","Program optimization","Routing","Simulation","Unmanned aerial vehicle","Vehicle routing problem"],"journalVolume":"","journalPages":"503-512","pmid":"","year":2017,"outCitations":["5b1406f4ba9365072172d666bed2ec49b90b9eeb","b2de7e58521b308415865fda246fe5a4245327e9","b525ed657712256c0abf866e7b0b51831078247f","1d902a44476a9a01092341a63a212052b513724b","1a45104c95259533c7176c956138df40f1efb82e","3c0ce781f3e21314a42c436080f69875d8d7cc9f","07d2f2a85a2453591e01651ffb287f4bc0e3ce7f","180af42e1b95259b593f506b74b99f2acb8f10a4","0c38ced8d5f1abcce633bb347e332dce99a6ad16"],"s2Url":"https://semanticscholar.org/paper/503da6ea753d21b8843d0c35e24d23ef46ba7cbf","s2PdfUrl":"","id":"503da6ea753d21b8843d0c35e24d23ef46ba7cbf","authors":[{"name":"Akhil Krishnan","ids":["11960766"]},{"name":"Mikhail Markov","ids":["32847772"]},{"name":"Borzoo Bonakdarpour","ids":["1746401"]}],"journalName":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","paperAbstract":"The classic vehicle routing problem (VRP) is generally concerned with the optimal design of routes by a fleet of vehicles to service a set of customers by minimizing the overall cost, usually the travel distance for the whole set of routes. Although the problem has been extensively studied in the context of operations research and optimization, there is little research on solving the VRP, where distributed vehicles need to compute their respective routes in a decentralized fashion. Our first contribution is a synchronous distributed approximation algorithm that solves the VRP. Using the duality theorem of linear programming, we show that the approximation ratio of our algorithm is O(n &#xb7; (&#x3c1;)1/n log(n + m)), where &#x3c1; is the maximum cost of travel or service in the input VRP instance, n is the size of the graph, and m is the number of vehicles. We report results of simulations and discuss implementation of our algorithm on a real fleet of unmanned aerial systems (UASs) that carry out a set of tasks.","inCitations":[],"pdfUrls":["https://doi.org/10.1109/IPDPS.2017.90"],"title":"Distributed Vehicle Routing Approximation","doi":"10.1109/IPDPS.2017.90","sources":["DBLP"],"doiUrl":"https://doi.org/10.1109/IPDPS.2017.90","venue":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)"},
{"entities":["Algorithm","Centralisation","Graph property","Linear function (calculus)","Local variable","Mathematical optimization","Nonlinear system","Online and offline","Spectral clustering","Vertex (geometry)"],"journalVolume":"","journalPages":"2-11","pmid":"","year":2017,"outCitations":["4716de48821d4669a08f97e94493c5253c907e0f","86fb6d3152a9849444f2301c91ddce5b97ce611b","4f8298d9932393b0fdd73576715c0818b4083292","24259d92169e287c55abd3bd6cc5b2da50a88c4b","3df7d6f56bcf9d4741409b439b418f4217cdcd2c","41d7a4cb6c804945a7c6a0976a3dd85b9fe37677","3ea5e18d3da3c72212aeccca74e28a2c8d9449cc","121e53ccc22cbf6aa453a221fcde294b1fcffe60","4ebc5082dc41cf6fdc80533d44dfc5db35ffa94f","7c6d51677ffff060ac04e0a61ce2cf9cb2437709","011c8ce97e4481e92e3e6cdb989247a8881a7f2f","63d567b512fca70f84aef4a59bc0e2aafaaebb56","0967bd75632d959541ee4afef35a5ef37c805cc7","aa6ad058dffedcaa0b614b23a7508562a4652855","01169e6900a3bb555d45b55ba674fc3b342d31c9","015d2bee5968ceecdbec6cb4a9328ad04c9efe6c","6d248d20660602f34b87b2e9a597dbc3be06cd3a","59dd507247fd03a93437288b015d55e337807247"],"s2Url":"https://semanticscholar.org/paper/0327efc1d7628328c44b81f452f808d4fe05d955","s2PdfUrl":"","id":"0327efc1d7628328c44b81f452f808d4fe05d955","authors":[{"name":"Gal Yehuda","ids":["3393459"]},{"name":"Daniel Keren","ids":["33438590"]},{"name":"Islam Akaria","ids":["2379080"]}],"journalName":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","paperAbstract":"The following is a very common question in numerous theoretical and application-related domains: given a graph G, does it satisfy some given property? For example, is G connected? Is its diameter smaller than a given threshold? Is its average degree larger than a certain threshold? Traditionally, algorithms to quickly answer such questions were developed for static and centralized graphs (i.e. G is stored in a central server and the list of its vertices and edges is static and quickly accessible). Later, as dictated by practical considerations, a great deal of attention was given to on-line algorithms for dynamic graphs (where vertices and edges can be added and deleted); the focus of research was to quickly decide whether the new graph still satisfies the given property. Today, a more difficult version of this problem, referred to as the distributed monitoring problem, is becoming increasingly important: large graphs are not only dynamic, but also distributed, that is, G is partitioned between a few servers, none of which &#x0022;sees&#x0022; G in its entirety. The question is how to define local conditions, such that as long as they hold on the local graphs, it is guaranteed that the desired property holds for the global G. Such local conditions are crucial for avoiding a huge communication overhead. While defining local conditions for linear properties (e.g. average degree) is relatively easy, they are considerably more difficult to derive for non-linear functions over graphs. We propose a solution and a general definition of solution optimality, and demonstrate how to apply it to two important graph properties &#x2013; the spectral gap and the number of triangles. We also define an absolute lower bound on the communication overhead for distributed monitoring, and compare our algorithm to it, with excellent results. Last but not least, performance improves as the graph becomes larger and denser &#x2013; that is, when distributing it is more important.","inCitations":["782079770e60b2ef266dcf3de861a81f97baa985"],"pdfUrls":["http://www.cs.haifa.ac.il/~dkeren/ipdps17.pdf","http://www.weizmann.ac.il/math/printpdf/seminar/monitoring-properties-large-distributed-dynamic-graphs","https://doi.org/10.1109/IPDPS.2017.123"],"title":"Monitoring Properties of Large, Distributed, Dynamic Graphs","doi":"10.1109/IPDPS.2017.123","sources":["DBLP"],"doiUrl":"https://doi.org/10.1109/IPDPS.2017.123","venue":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)"},
{"entities":["Buffer overflow","Central processing unit","Cluster analysis","Computer cluster","Graphics processing unit","Multi-core processor","Redshift","Sparse matrix","Speedup","Throughput","Total electron content"],"journalVolume":"","journalPages":"832-841","pmid":"","year":2017,"outCitations":["30267d0fb3bcdc7a39b2cc8e4ccb6383c8b2892a","8b54c5824b74aba6fddb1729210c3fa8501e42b5","75a4860c9b3b2e95bc3a8056543e7560a1753f2b","dbf36c7a3c5521b93aef699476ba37b3ca15bb61","271f54ab5239b6a33d3b16bb99f1c16d1e8bb0c0","92e96f85b5081ddab57923dde750e939faec9847","cde4efcd58c9b39c8dd7cf0173643851bfedcbb9","3448ff2614a49c16ad6be8b3e363c57e12762f24","82fafb4c5dfb61901751f47b049d0c67ce64f803","a644c02fc217c9fa49b7a91bd6f65def32196a2a","44d3ffc5979aea32f76f137b6a40424e1437be2a","13a375a84a6c414b85477a401541d3e28db1e11a","63eac5f99c547d0ba3660464799d826a879b53fd","30854a901a39404dbaacb1cb5363ab3c0a2e35e1","bc36e6a50ca6fe42daf8041e7ec68abcdd8cc4fa","be212f16400a7db90c14da51fd69600a124492db","1daefd3a54681a127b54fb0fdba215ce790526f4","1d3b776507f1c11bbcdcd1f8c0ea8c48df675904","d94194381f7323c052891f9bbd40d0680ad01269","0226adea5e4f5f739633a83d159ca989045eefe5","9c5882ea02390e3ca93d04aeeb4ec440ae17ff50","31dfabb8d1085ac468b60a83d32af2a558407c95","560f35ad5e6512b5c26d43c275d3dfd7aabd8ca2","12d49ecc6aa2bf20a850100cafe061a237a4874e","0157f142bee7b462897424908cd6c73d84f225cc","7005f2e50c4c9fcd679b8e6d2ddc5a5ae0c1bf15"],"s2Url":"https://semanticscholar.org/paper/20d8cc3f8a81fddd8168ca696e9182543b33dd43","s2PdfUrl":"","id":"20d8cc3f8a81fddd8168ca696e9182543b33dd43","authors":[{"name":"Michael G. Gowanlock","ids":["2753806"]},{"name":"Cody M. Rude","ids":["18772848"]},{"name":"David M. Blair","ids":["33571655"]},{"name":"Justin D. Li","ids":["10660562"]},{"name":"Victor Pankratius","ids":["1682906"]}],"journalName":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)","paperAbstract":"Large datasets in astronomy and geoscience often require clustering and visualizations of phenomena at different densities and scales in order to generate scientific insight. We examine the problem of maximizing clustering throughput for concurrent dataset clustering in spatial dimensions. We introduce a novel hybrid approach that uses GPUs in conjunction with multicore CPUs for algorithmic throughput optimizations. The key idea is to exploit the fast memory on the GPU for index searches and optimize I/O transfers in such a way that the low-bandwidth host-GPU bottleneck does not have a significant negative performance impact. To achieve this, we derive two distinct GPU kernels that exploit grid-based indexing schemes to improve clustering performance. To obviate limited GPU memory and enable large dataset clustering, our method is complemented by an efficient batching scheme for transfers between the host and GPU accelerator. This scheme is robust with respect to both sparse and dense data distributions and intelligently avoids buffer overflows that would otherwise degrade performance, all while minimizing the number of data transfers between the host and GPU. We evaluate our approaches on ionospheric total electron content datasets as well as intermediate-redshift galaxies from the Sloan Digital Sky Survey. Our hybrid approach yields a speedup of up to 50x over the sequential implementation on one of the experimental scenarios, which is respectable for I/O intensive clustering.","inCitations":["1965b8b0a25956488542510c759b0d6e128d1b90"],"pdfUrls":["https://doi.org/10.1109/IPDPS.2017.17"],"title":"Clustering Throughput Optimization on the GPU","doi":"10.1109/IPDPS.2017.17","sources":["DBLP"],"doiUrl":"https://doi.org/10.1109/IPDPS.2017.17","venue":"2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)"},
