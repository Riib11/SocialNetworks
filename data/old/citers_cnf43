{"entities":["Adobe Flash","Algorithm","Application programming interface","Clustered file system","Copy-on-write","Correctness (computer science)","Crash (computing)","Data structure","Distributed computing","Fault tolerance","Flash file system","Flash memory","Flash memory controller","Garbage collection (computer science)","Information management","LU decomposition","Software deployment","Travis CI","Unavailability"],"journalVolume":"13","journalPages":"18:1","pmid":"","year":2017,"outCitations":[],"s2Url":"https://semanticscholar.org/paper/23b29d0febe574586557c628e81846891c6e2512","s2PdfUrl":"","id":"23b29d0febe574586557c628e81846891c6e2512","authors":[{"name":"Geoffrey H. Kuenning","ids":["1738042"]},{"name":"Carl A. Waldspurger","ids":["2642780"]}],"journalName":"TOS","paperAbstract":"This special issue of the ACM Transactions on Storage presents some of the highlights of the 15th USENIXConference on File and Storage Technologies (FAST’17). In the 15 years since its inception, FAST has grown into a damn active community, attracting 116 submissions and over 400 attendees in 2017. FAST’17 continues the tradition of bringing together storage-system researchers and practitioners to explore new directions in the design, implementation, evaluation, and deployment of storage systems. FAST takes a broad view of storage systems, encompassing everything from lowlevel storage devices to information management systems. We selected five high-quality articles for publication in this special issue of ACM Transactions on Storage. The first article, which was also selected as one of the best papers at the conference, is “Application Crash Consistency and Performance with CCFS,” by Thanumalayan Sankaranarayana Pillai, RamnatthanAlagappan, Lanyue Lu, Vijay Chidambaram, Andrea C. Arpaci-Dusseau, and Remzi H. Arpaci-Dusseau. This article introduces a crash-consistent file system that uses a stream abstraction to guarantee that intra-stream operations are committed in program order to improve the correctness of application crash consistency, while removing ordering constraints across streams to maintain high performance. The second article is “Redundancy Does Not Imply Fault Tolerance: Analysis of Distributed Storage Reactions to File-System Faults,” by Aishwarya Ganesan, Ramnatthan Alagappan, Andrea C. Arpaci-Dusseau, and Remzi H. Arpaci-Dusseau. The authors analyze eight popular distributed storage systems and uncover significant problems that can result in data loss, corruption, and unavailability, with implications for the design of future fault-tolerant distributed systems. The third article is “vNFS: Maximizing NFS Performance with Compounds and Vectorized I/O,” by Ming Chen, Geetika Babu Bangera, Dean Hildebrand, Farhaan Jalia, Geoff Kuenning, Henry Nelson, and Erez Zadok. This article leverages the NFSv4 compounding feature, exposing a highlevel vectorized API to support bulk operations, achieving dramatic speedups on a wide range of workloads under various network latency conditions. The fourth article is “Tiny-Tail Flash: Near-Perfect Elimination of Garbage Collection Tail Latencies in NAND SSDs,” by Shiqin Yan, Huaicheng Li, Mingzhe Hao, Michael Hao Tong, Swaminathan Sundararaman, Andrew A. Chien, and Haryadi S. Gunawi. A well-known problem with flash storage is the issue of long, unpredictable delays induced by garbage collection in the flash translation layer. The authors use a combination of novel strategies to optimize garbage collection, achieving tail latencies that approach the ideal of a system without garbage collection. The final article, also selected as one of the best papers at the conference, is “Efficient Free Space Reclamation inWAFL,” by RamKesavan, Rohit Singh, Travis Grusecki, and Yuvraj Patel. The NetAppWAFL file system uses copy-on-write for high-performancewrites and efficient snapshots, increasing the demand for free-space reclamation. The authors present the evolution of algorithms and data structures spanning more than a decade of production deployments.","inCitations":[],"pdfUrls":["http://doi.acm.org/10.1145/3131620"],"title":"Introduction to the Special Issue on USENIX FAST 2017","doi":"10.1145/3131620","sources":["DBLP"],"doiUrl":"https://doi.org/10.1145/3131620","venue":"TOS"},
{"entities":["ACID","Apache HBase","Attribute–value pair","Data access","High availability","Key-value database","Open-source software","Transaction processing","Transactions per second","Value (ethics)"],"journalVolume":"","journalPages":"167-180","pmid":"","year":2017,"outCitations":["9748241beb02ef1e2d0e6dc877c04b354033a838","0599ba259341963bf8abf2818c874713e570a039","43fe3ad9ce1c3dbe4f905068ae2adc7bcb7fc9fb","8d1c0ae7bbe138bc19abf66ca918f46b244b1f5d","4827cc74dba0c39172554cf0116eb111797f0d1b","1220e4a011c46804d4369b5580dc7fb6e387af54","624cb175af600b7749bce00c0932e2a10f72e564","57efc2b9ba2a725af1d66cc43c472d0314190051","208b6e0a2492275c22f0320879b8ed037c08330f","068e59b88a1230d709d99c83a45d3a5b91260810","00f7b192212078fc8afcbe504cc8caf57d8f73b5","24d565a61917967d0fcaf66cb4d8be9fff5a34fc","665a0bcd3e35453702e655f86683417581517b4e","039f09d49bc408db9e0e8429e6bd92be49c5f72e","3a8c90ab13adb55e3610a020c69f03d72dfae274","4b9c92dc611a6e0861d11240a036b39dd9dd4f7b","18a5f443299784479e78d9e77f175af57cb2fa2b","408b8d34b7467c0b25b27fdafa77ee241ce7f4c4","7cd99fed3b3b701af32bfdad561b0900fb510b7c","29a05cde1994548e2e9487822248c679626c6241","4593ae644f04d76f582dedc4cc32d2acd33c9a93","742c641506ac9efc3281af2effb31f2fb31b2dd4","6e90c995cc9caa0f7d9d68d536f5e16e9bcbbcd6","517e239f97f50079bc557cccf1a6b56aa5736d30","095a3cee30d64d3a6f22caadd58c45c5cd0b83e9","07d847f310d5fa9138f461f0a25c5e0024f1c4af"],"s2Url":"https://semanticscholar.org/paper/1ac7667a185cdcbda194e9852c27dcc169fbbabf","s2PdfUrl":"http://pdfs.semanticscholar.org/ef73/d2a1433187418ce044ac81fa41452f728f29.pdf","id":"1ac7667a185cdcbda194e9852c27dcc169fbbabf","authors":[{"name":"Edward Bortnikov","ids":["1685240"]},{"name":"Eshcar Hillel","ids":["2829412"]},{"name":"Idit Keidar","ids":["1777373"]},{"name":"Ivan Kelly","ids":["36805537"]},{"name":"Matthieu Morel","ids":["39388165"]},{"name":"Sameer Paranjpye","ids":["9753168"]},{"name":"Francisco Perez-Sorrosal","ids":["2196178"]},{"name":"Ohad Shacham","ids":["40076877"]}],"journalName":"","paperAbstract":"We present Omid – a transaction processing service that powers web-scale production systems at Yahoo. Omid provides ACID transaction semantics on top of traditional key-value storage; its implementation over Apache HBase is open sourced as part of Apache Incubator. Omid can serve hundreds of thousands of transactions per second on standard mid-range hardware, while incurring minimal impact on the speed of data access in the underlying key-value store. Additionally, as expected from always-on production services, Omid is highly available.","inCitations":[],"pdfUrls":["https://www.usenix.org/sites/default/files/conference/protected-files/fast17_slides_shacham.pdf","http://webee.technion.ac.il/people/idish/ftp/Omid.pdf","http://www.usenix.org./system/files/conference/fast17/fast17-shacham.pdf","http://www.usenix.org./sites/default/files/conference/protected-files/fast17_slides_shacham.pdf","https://www.usenix.org/conference/fast17/technical-sessions/presentation/shacham","https://www.usenix.org/system/files/conference/fast17/fast17-shacham.pdf"],"title":"Omid, Reloaded: Scalable and Highly-Available Transaction Processing","doi":"","sources":["DBLP"],"doiUrl":"","venue":"FAST"},
{"entities":["Apache Hadoop","Database","Distributed File System (Microsoft)","Downtime","Experiment","Failover","In-memory database","NewSQL","Next-generation network","Scalability","Shared nothing architecture","Throughput"],"journalVolume":"","journalPages":"89-104","pmid":"","year":2017,"outCitations":["4af63ed343df388b6353b6fc77c7137d27822bf4","a93d2871166991749ee865f8a1ceb7f3a7fe8bfc","b0d7bfd07752108b53d885c2835004d49ca693c9","e2c6297a9ad5118dc4a6a0dab6a2af2b83545e3d","253d779cc8939c4f5e2d50158bc76586c743417d","37617b02017b7912ad4d977ba420ab3fa232e445","04b3aaf58a91557e15c8064660baa1cc5e8db14e","3aaa142aeb475b0aaef21e3dcfb7951a97e0f19a","059eb34d95c73e37dca8e35b0ac5a2fb0142f3ee","3d3f22ee1797b0e086da07e00d0f59b1aca08bf3","42142c121b2dbe48d55e81c2ce198a5639645030","b7014a268c35e377366634d6b8370a8a7db285a5","499b458ddd5d10dc4be158fd89aeed0e31b6b5cf","d545d252271c228375f6b24dfa9ab3b3d5592357","b054076afd228c71b95f019f63ab1f8b102fcd56","5ff311923cd8f80057b2cfc15cf7ec3ac0a6fdbc","0b35861df3b66533b0a188b411dcc4de6723a5cd","18a5f443299784479e78d9e77f175af57cb2fa2b","2da760f90c3d2bf6598becdde9063093f488548c","26c4c1dd27fdb449fe0267eac595930766917878","07d847f310d5fa9138f461f0a25c5e0024f1c4af","0712c325155f8af65602a08cc448d1e453466a33","33457f49553d918e912c2d8c54b81f4fd8a4c234","afa94b1bd30b3c6c17766117f8eb543dc9f1f7bb","2d7d50dea29a30b82e75187f8b71579de93e53b9","9748241beb02ef1e2d0e6dc877c04b354033a838","2d60d3596490d9999d8433bf41405060779bc11d","e9852418b28b3d1990ce787193ed1deb2cbc406a","1cfee3e6bad11c5c92cd06065064c474a00e2412","0f408cba7605f7a6fd65837dd1c7e6f193d181ef","1d99b7749a9311d2db24a3d84728e444eff23e4b","b55aefc75061cca8d4697f7d789800a0f2d344a0","e9aac4c3925553c6203293fa519fa1d9f56ef8fc","5f3f9223c5c9f896be099bc177929febad508407","bb294f18c25c877a453b14e80b40b56707753592","0eca989d12fe222091ab92da85b812ed5537c06f","41d2c530e0a44ab8b665caa5e17c8e0ca0c39726","26c713c0775ac388370492c26e25d24d9e430e9a","01e9ddf1062f9a7d7847bb9bcd2371ce6e0d3e29","4593ae644f04d76f582dedc4cc32d2acd33c9a93","57efc2b9ba2a725af1d66cc43c472d0314190051","2b25cfea56184fcad5b36f949a50238d4f810201","87d47502bf40a4bfa7a0ded26c3efb2426250808","2289754c17c95e53c982ca2f023af21dec824d29","0973e45d3eeb9641d3de34d48f8d0432f1113dcf","77733a36cb27577a664c1a657fbed51813202ebd","b9fba8f9da216f4430180baea05239ddce648f3a","24281c886cd9339fe2fc5881faf5ed72b731a03e","4fbe8c8ace7546e3a10bfd8e151bc09a41fd3f9a","332f77fd05703c1607e3b57884ad31fb1fad0104","6d3d5d4fbc6bc0fe419d9c6809a00f71a492c171","152435c55c3d2900c90f106b6688dd3844372fdf","0541d5338adc48276b3b8cd3a141d799e2d40150","d5d868887ae05016e15d2bc050b037eeb0b9f111"],"s2Url":"https://semanticscholar.org/paper/19f3c85feddc4c65409cbb73941f63a98d39fec0","s2PdfUrl":"http://pdfs.semanticscholar.org/5dfb/1e4e731a7c197cf5cefcd9f654a72f1690d0.pdf","id":"19f3c85feddc4c65409cbb73941f63a98d39fec0","authors":[{"name":"Salman Niazi","ids":["34792930"]},{"name":"Mahmoud Ismail","ids":["37148494"]},{"name":"Seif Haridi","ids":["1694607"]},{"name":"Jim Dowling","ids":["1684757"]},{"name":"Steffen Grohsschmiedt","ids":["3422632"]},{"name":"Mikael Ronström","ids":["2984804"]}],"journalName":"","paperAbstract":"Recent improvements in both the performance and scalability of shared-nothing, transactional, in-memory NewSQL databases have reopened the research question of whether distributed metadata for hierarchical file systems can be managed using commodity databases. In this paper, we introduce HopsFS, a next generation distribution of the Hadoop Distributed File System (HDFS) that replaces HDFS’ single node in-memory metadata service, with a distributed metadata service built on a NewSQL database. By removing the metadata bottleneck, HopsFS enables an order of magnitude larger and higher throughput clusters compared to HDFS. Metadata capacity has been increased to at least 37 times HDFS’ capacity, and in experiments based on a workload trace from Spotify, we show that HopsFS supports 16 to 37 times the throughput of Apache HDFS. HopsFS also has lower latency for many concurrent clients, and no downtime during failover. Finally, as metadata is now stored in a commodity database, it can be safely extended and easily exported to external systems for online analysis and free-text search.","inCitations":["b28ead3a0ceaa45394b713fd1c8efb0c4d5eb857","9809bc2847bc9274564c6c3545561d920c5e44f3","7de1da8eddd18cb58d1fdab18fc0c922d9a6ef27","41e494275eb24b248bddc19c4f7185c9abba803d","dbc4aeb409f4d10813290ddfc612d9a271925755"],"pdfUrls":["https://www.usenix.org/system/files/conference/fast17/fast17-niazi.pdf","http://arxiv.org/abs/1606.01588","https://arxiv.org/pdf/1606.01588v1.pdf","http://www.usenix.org./system/files/conference/fast17/fast17-niazi.pdf","https://www.usenix.org/conference/fast17/technical-sessions/presentation/niazi","https://arxiv.org/pdf/1606.01588v2.pdf","http://arxiv.org/pdf/1606.01588v1.pdf"],"title":"HopsFS: Scaling Hierarchical File System Metadata Using NewSQL Databases","doi":"","sources":["DBLP"],"doiUrl":"","venue":"FAST"},
{"entities":["Database engine","High- and low-level","Input/output","Linux","Linux","Open-channel SSD","Parallel computing","Principle of abstraction","Solid-state drive","Spatial variability"],"journalVolume":"","journalPages":"359-374","pmid":"","year":2017,"outCitations":["9b90568faad1fd394737b79503571b7f5f0b2f4b","0831a5baf38c9b3d43c755319a602b15fc01c52d","0cd52827be79113f01e2408411e95fa371c52728","2dd70a51d787c1cb6e7854e32bd00c9050bfcac5","d67adb456a315aee244babf4f20e318cc14d13f3","0bba65fd5ac1db9a3293e9ebcfba092cf4ae58ee","26a88fcaf621270af5f5786fdb2df376a2bc00aa","33438b1148a84d6e5bf2cad70bf7754d546ca5d7","2e46f9074bd81ea4ec29ecec7e0231c16fb2e8db","8c06fb59a79b3b47dff8588302d8e6514a7f7a4a","05a1bad1ef2341339e18d636d78594226d4ee8e6","3e8e43f61b3af63c6a8bb981b5d085c8afb1b9e2","61977858b3eea4f5a6d81393301e7298ade7a2d8","72722e7602138e3896e5576d3f3ef730e7b7c4b4","eb6d964a0d7d51533dc27e7905c309e13a0fea54","84564d347d505467dd628e56319bc037b0a1ec28","d58cc242fd70227cff98376a914e0b42b1b79db8","356955d0f190829b7481b8dc39c5f90dfac1b652","389b618c42c0d5d32a569b9cbaa02a7ff77c6be6","726099036bb32c3fbaf1650d5900eeaa2ecc8fd9","1820a34042d6371a9e20484b0c63b698eb522a6c","70ce10f47aafa0994627a9575565b5c98af58d98","131e1e1d163a0f49881d7b5ac092892093391015","30419d0e0fcaebbfbcdb88f702bd01306d14fb15","acd5b766ce2f210a0351059d2dc29977f5d8abf0","1be96030c042ff6b5bbe05bf0fd86f5f9a4d27dc","491715b493fd7780da43249d9f29f4b00b2d535d","5a04b332441e2ff025313bfd303383e13050a274","719aeeaff7353058a152b4eb3ff77a193624a481","05dd6cb44124b8a210ac391f15ec25e68918ef22","199ac28b6bc68bf05c77645ffae7640df114bca5","08ca74190711c5cc1f0e51a4db805187a5c85a56","5aafae8b45a5dc17f589ad6cc74510f657d60cd6","27cb0c2229299a82cf767d19dcc68aa1e5f0f233","12484231f130f2c3d3d8c3bec33ed2830f100b11","663798bc529bb73f2b3ca8640bb4fcbd83ce5c31","800f30aa6b7107b085727ebbb98eacf944657b55","5271d6693ba950c389921ccc21110664f25a83db"],"s2Url":"https://semanticscholar.org/paper/0da73832dee2c9b3d4c0d039d8e714e6ff098e40","s2PdfUrl":"http://pdfs.semanticscholar.org/0da7/3832dee2c9b3d4c0d039d8e714e6ff098e40.pdf","id":"0da73832dee2c9b3d4c0d039d8e714e6ff098e40","authors":[{"name":"Matias Bjørling","ids":["1869737"]},{"name":"Javier González","ids":["1763726"]},{"name":"Philippe Bonnet","ids":["1748177"]}],"journalName":"","paperAbstract":"As Solid-State Drives (SSDs) become commonplace in data-centers and storage arrays, there is a growing demand for predictable latency. Traditional SSDs, serving block I/Os, fail to meet this demand. They offer a high-level of abstraction at the cost of unpredictable performance and suboptimal resource utilization. We propose that SSD management trade-offs should be handled through Open-Channel SSDs, a new class of SSDs, that give hosts control over their internals. We present our experience building LightNVM, the Linux Open-Channel SSD subsystem. We introduce a new Physical Page Address I/O interface that exposes SSD parallelism and storage media characteristics. LightNVM integrates into traditional storage stacks, while also enabling storage engines to take advantage of the new I/O interface. Our experimental results demonstrate that LightNVM has modest host overhead, that it can be tuned to limit read latency variability and that it can be customized to achieve predictable I/O latencies.","inCitations":["44c216b53c1f5a7091618c6b7ba9a32a35323dad","a906694ac9739f9cb8547b494dddd006b5365a92","0bd0819ff873acc26567ebf9a9cb73fea59d6cdd","40f196e21a289394c4354961116587b8accba45e","05a1bad1ef2341339e18d636d78594226d4ee8e6","1858ed4ca900d9afd06d0b8a8430d0dda8f957bc","ad42b4773cd461ba58bda07e1b7b0ff24c4ddba4","3de30c8dafc720bf066e5e3a005d16212dd31149","b8c87f3c5411557e7a21008bbb5db7485f98dbd0","0f4386d4a521e36cb15252b4e908a948a65252ef","19c0716139727ab7377bc73f394c9b99c86db16b","4d1a62de587f05084e85a4168f960af1e48b9697","c49feb5f91c8ba846eb2e90edf1b01c62a25c8d5","4fa7ade25b7bd22ea3357da0516833a318cc72fc","1d08d231ec66645ec56d2210c1a7c6b44c6ff041","ac924589f32d23c0eebe2173a77e1cc732f351b9","0ae03e097cd936f564a60017b864beeb12635b09","614bdb9fce7c3088050520fc769376722eebe8e2","226ca798b529c13605a2aa7fe75d58f4188f850a"],"pdfUrls":["https://www.usenix.org/conference/fast17/technical-sessions/presentation/bjorling","http://www.usenix.org./sites/default/files/conference/protected-files/fast17_slides_bjorling.pdf","http://www.usenix.org./system/files/conference/fast17/fast17-bjorling.pdf","https://www.usenix.org/system/files/conference/fast17/fast17-bjorling.pdf","http://nvmw.ucsd.edu/nvmw18-program/unzip/current/nvmw2018-final62.pdf","http://platformlab.stanford.edu/Seminar%20Talks/Matias_Bj_rling___Javier_Gonzales.pdf","https://www.usenix.org/sites/default/files/conference/protected-files/fast17_slides_bjorling.pdf"],"title":"LightNVM: The Linux Open-Channel SSD Subsystem","doi":"","sources":["DBLP"],"doiUrl":"","venue":"FAST"},
{"entities":["Address space","Dynamic random-access memory","Flash memory","Locality of reference","Principle of locality","Sequential access","Solid-state drive"],"journalVolume":"","journalPages":"271-284","pmid":"","year":2017,"outCitations":["088e3e939ad234b6fdd0e321290fb26937dc2553"],"s2Url":"https://semanticscholar.org/paper/31edf47da8494e9985c55efcda8e178f5b87140c","s2PdfUrl":"http://pdfs.semanticscholar.org/b307/6f29ad699eb7b0373254eab63a42ba6a1dfa.pdf","id":"31edf47da8494e9985c55efcda8e178f5b87140c","authors":[{"name":"Hyukjoong Kim","ids":["2887206"]},{"name":"Dongkun Shin","ids":["3149588"]},{"name":"Yunho Jeong","ids":["2256812"]},{"name":"Kyung Ho Kim","ids":["2041606"]}],"journalName":"","paperAbstract":"Recent advances in flash memory technology have reduced the cost-per-bit of flash storage devices, thereby enabling the development of large-capacity SSDs. However, two major concerns arise in designing SSDs. The first is the poor performance of random writes, and the second is the large size of the internal DRAM of an SSD. Although the previously proposed demand map loading technique can reduce the required DRAM size, the technique aggravates the poor random performance. We propose a novel address reshaping technique called sequentializing in host and randomizing in device (SHRD), which transforms random write requests into sequential write requests by assigning the address space of the reserved log area in the SSD. SHRD can restore the sequentially written data to the original location without requiring explicit copy operations by utilizing the address mapping scheme. We implement SHRD in a real SSD device and demonstrate the improved performance resulting from SHRD for various workloads1.","inCitations":["0f4386d4a521e36cb15252b4e908a948a65252ef","acd1ffefbe465f7cbb2584d32394d54bd5988a39"],"pdfUrls":["http://nvmw.ucsd.edu/nvmw18-program/unzip/current/nvmw2018-final43.pdf","http://www.usenix.org./system/files/conference/fast17/fast17-kim-hyukjoong.pdf","http://www.usenix.org./sites/default/files/conference/protected-files/fast17_slides_kim.pdf","https://www.usenix.org/sites/default/files/conference/protected-files/fast17_slides_kim.pdf","https://www.usenix.org/system/files/conference/fast17/fast17-kim-hyukjoong.pdf","https://www.usenix.org/conference/fast17/technical-sessions/presentation/kim"],"title":"SHRD: Improving Spatial Locality in Flash Storage Accesses by Sequentializing in Host and Randomizing in Device","doi":"","sources":["DBLP"],"doiUrl":"","venue":"FAST"},
{"entities":["Algorithm","Central processing unit","Copy-on-write","Data structure","Snapshot (computer storage)"],"journalVolume":"","journalPages":"1-14","pmid":"","year":2017,"outCitations":["12a0046a1197ae63c3d616c74e367dc583cef196","7062268b78dff4a8819fe3f1e89c6b5344f715a5","06bd4d2d21624c7713d7f10ccb7df61bf6b9ee71","3d1abb7432c9e52758f1bccc5b1e2e60dfbfe91f","2c84daae142c5b0f4ca6a6772ca7e8cac7d7afca","c9ef82a4ad0b1b33296cea86fb2ec7558cf798fb","556f01b6764f866d7bd4a2d955115ca72bd3413f","d0b6d2075a653d60452b6df0fced4ee0ae093dd2","740b2ff66ea305ffc5369ecee4498941c39efaab","4ef1fcc896885d383442b2aff92c2109cd0da9be","012ab4527d6aee2387c243d304c624f3b9cf03f3","088e3e939ad234b6fdd0e321290fb26937dc2553"],"s2Url":"https://semanticscholar.org/paper/20a108587321823ca9cdd93ac84fc316a0400630","s2PdfUrl":"http://pdfs.semanticscholar.org/20a1/08587321823ca9cdd93ac84fc316a0400630.pdf","id":"20a108587321823ca9cdd93ac84fc316a0400630","authors":[{"name":"Ram Kesavan","ids":["2042885"]},{"name":"Rohit Singh","ids":["39454579"]},{"name":"Travis Grusecki","ids":["9762022"]},{"name":"Yuvraj Patel","ids":["40604671"]}],"journalName":"","paperAbstract":"NetApp®WAFL®is a transactional file system that uses the copy-on-write mechanism to support fast write performance and efficient snapshot creation. However, copy-on-write increases the demand on the file system to find free blocks quickly; failure to do so may impede allocations for incoming writes. Efficiency is also important, because the task may consume CPU and other resources. In this paper, we describe the evolution (over more than a decade) of WAFL’s algorithms and data structures for reclaiming space with minimal impact on the overall storage appliance performance.","inCitations":["0fd85ea4f3701f6baebffcaab39b858b7142b0dc","8ee82c0bd80e86c55b56414a602d53164d4fb5c0","67ffec9c10d9594eb9af4afe25b1f0b0bce5f85d","8d555af4ad0bcb45ac5ce62374fbd23ea429121f","ad897b9261a39cdae6e8b0fdcd755e6001e004bc","556f01b6764f866d7bd4a2d955115ca72bd3413f"],"pdfUrls":["https://www.usenix.org/conference/fast17/technical-sessions/presentation/kesavan","http://www.usenix.org./system/files/conference/fast17/fast17-kesavan.pdf","https://www.usenix.org/system/files/conference/fast17/fast17-kesavan.pdf"],"title":"Algorithms and Data Structures for Efficient Free Space Reclamation in WAFL","doi":"","sources":["DBLP"],"doiUrl":"","venue":"FAST"},
{"entities":["Experiment","Spatial variability"],"journalVolume":"","journalPages":"329-344","pmid":"","year":2017,"outCitations":["b4d9a48d77cdc71290a49e81881528be9a45435b","26a88fcaf621270af5f5786fdb2df376a2bc00aa","0d62acaefd4c4f41fd5814c8d9267d7798de9284","0831a5baf38c9b3d43c755319a602b15fc01c52d","31330de2f2bc268ee2bf5ccbe3bed08b52c3e4fa","12a0046a1197ae63c3d616c74e367dc583cef196","8b90229a5f7af5b4b3d602f63de72fc421d55a24","227e529c08f821d134dd15fb9296419250ab9301","36698f71fea78a0ee1a058484c0c0c781e354f61","039124197fac7a16e36611d8beed94524dd5fed5","012ab4527d6aee2387c243d304c624f3b9cf03f3","02d9013e5d370fb79ff1569a59190e18515fa3cd","13c27125584651329f66461981cbb20fa63e9023","2d60d3596490d9999d8433bf41405060779bc11d","3c89345bb88a440096f7a057c28857cc4baf3695","11fe43dfcf43802595c2076c7641aff6f025e1ec","830ee8d87a3f2ef969d34c1dc7224d1b3dca6c1b","14a2ba566f6c8f7f519b299042ccf358361c558f","8301c813277cc59b47a84d25dc1e307eee8ce310","3f7971691970a4924bc62f3766541a4b4294fa0d","2018f3fc13cd38122abdf37bf939b5011cd2e3c9","b53d8516bf83c1b58147e2b4dbc870a8d396e53e","4ba4613eab33cddc53bec9e14e50d03fa66270ca","1a5fc0a7aca4a8e2f831d0edb1e5d160acea19ac","2da760f90c3d2bf6598becdde9063093f488548c","29a1a2a8f34d17c791e98775cdf7f8580b13abf6","27f8ac77b89986f7a24f929b200b6a358b8f7d01","7b6e453e08717cfdcb66349ac184996e43ed85b3","d0b6d2075a653d60452b6df0fced4ee0ae093dd2","239e046347d5075b3eeef5439050e9f2ca760b7b","1b1ea9f3f15f5160b77aa2177e7fdeb6eeed911a","a8d5edc845fe8512e01ddfd4af0d09c397fbcbec"],"s2Url":"https://semanticscholar.org/paper/35aecf2a6ad7f12ad06d9f9e6b7d4935fea840ac","s2PdfUrl":"http://pdfs.semanticscholar.org/6be0/5e89a9df87f3c2675fe4a5b39e0601c68706.pdf","id":"35aecf2a6ad7f12ad06d9f9e6b7d4935fea840ac","authors":[{"name":"Zhen Cao","ids":["2756452"]},{"name":"Vasily Tarasov","ids":["28670096"]},{"name":"Hari Prasath Raman","ids":["9749757"]},{"name":"Dean Hildebrand","ids":["32401480"]},{"name":"Erez Zadok","ids":["1708491"]}],"journalName":"","paperAbstract":"Ensuring stable performance for storage stacks is important, especially with the growth in popularity of hosted services where customers expect QoS guarantees. The same requirement arises from benchmarking settings as well. One would expect that repeated, carefully controlled experiments might yield nearly identical performance results—but we found otherwise. We therefore undertook a study to characterize the amount of variability in benchmarking modern storage stacks. In this paper we report on the techniques used and the results of this study. We conducted many experiments using several popular workloads, file systems, and storage devices—and varied many parameters across the entire storage stack. In over 25% of the sampled configurations, we uncovered variations higher than 10% in storage performance between runs. We analyzed these variations and found that there was no single root cause: it often changed with the workload, hardware, or software configuration in the storage stack. In several of those cases we were able to fix the cause of variation and reduce it to acceptable levels. We believe our observations in benchmarking will also shed some light on addressing stability issues in production systems.","inCitations":["257c1c169dd0ae98e273efd0d0948f2a028d4c3f","b8c87f3c5411557e7a21008bbb5db7485f98dbd0"],"pdfUrls":["http://www.fsl.cs.stonybrook.edu/docs/evos/evos-instability-fast17.pdf","https://www.usenix.org/sites/default/files/conference/protected-files/fast17_slides_cao.pdf","https://www.usenix.org/system/files/conference/fast17/fast17-cao.pdf","http://www.fsl.cs.sunysb.edu/docs/evos/evos-instability-fast17.pdf","https://www.usenix.org/conference/fast17/technical-sessions/presentation/cao","http://www.usenix.org./system/files/conference/fast17/fast17-cao.pdf","http://www.usenix.org./sites/default/files/conference/protected-files/fast17_slides_cao.pdf"],"title":"On the Performance Variation in Modern Storage Stacks","doi":"","sources":["DBLP"],"doiUrl":"","venue":"FAST"},
{"entities":["Computer data storage","Data deduplication","Garbage collection (computer science)","Initialization (programming)","Liveness","Run time (program lifecycle phase)","Sequential access"],"journalVolume":"","journalPages":"29-44","pmid":"","year":2017,"outCitations":["a48f6d8ef6d2831bf0fd2d720c20adb8ca988550","b8337665c72596062c70d5809a691f2f452134c2","2c8dfa703ba8cb907384149820e117d5935d9ae0","71d848f5633852d3c9a54ca733af6c0103cd8364","0ce479229630e55e732597cf9b2aeb5018aae4c2","80fd0f9813d08ef66913aeee95fb674853dbafd7","d4da5fbf10b696fa691501ec08618aee479ba3ea","5bb770af1973f929e8622f17ddf378d439245144","9c046601e01d693c1d36a074c00d226c563c76f2","26a88fcaf621270af5f5786fdb2df376a2bc00aa","bb6bb8da52698fc965f368ee64c68beab59737a8","f8e4c3f5ddc4d208d1c3161a9d9a32c2141e5500","2f6af58c7905fb8367652fe62fbb1f6ec7e28be0","2be8894db1a0c4787f36e22fe37d4d99dcd916c1","4fa3950e7676c7262e62e8d39a4fc9c304e53aa6","1b753f82e08a04d1442f357edbfd03385ef788f7","088e3e939ad234b6fdd0e321290fb26937dc2553","bf6275801e4bac2918f1b8698c2892e1a375808f","b02f480d56b8a4e462457e3d5fcf765ee9dc2494","8502fd5a659150e0635973744c4a80138c4e7ca7","4a106c99bc3dce2c3999211778fe7bde0a3786f0","04bffc7c4b7e6e40815621c8981f94ba5a3fad8a","04d7623afc83282bc8d4e3ef3ad2decc86cc237d","c6d01d9365d7b134ef2efed0063820d1b9be659a","6662d518878d3eee218462ee4d8b389c64e1b6f7","46a574413123beb2ba0572c563e1a4883baec997","185d057d3bce4ea115c4fbe39da65a43b1cc1a0c"],"s2Url":"https://semanticscholar.org/paper/5a37b117426b1ff41411429fb5687fe8d045757a","s2PdfUrl":"http://pdfs.semanticscholar.org/6060/9d159a47e790595af5e62bcbfa1e1996a689.pdf","id":"5a37b117426b1ff41411429fb5687fe8d045757a","authors":[{"name":"Fred Douglis","ids":["1782350"]},{"name":"Abhinav Duggal","ids":["32331661"]},{"name":"Philip Shilane","ids":["1816098"]},{"name":"Tony Wong","ids":["31934385"]},{"name":"Shiqin Yan","ids":["9765570"]},{"name":"Fabiano C. Botelho","ids":["35031086"]}],"journalName":"","paperAbstract":"Most storage systems that write in a log-structured manner need a mechanism for garbage collection (GC), reclaiming and consolidating space by identifying unused areas on disk. In a deduplicating storage system, GC is complicated by the possibility of numerous references to the same underlying data. We describe two variants of garbage collection in a commercial deduplicating storage system, a logical GC that operates on the files containing deduplicated data and a physical GC that performs sequential I/O on the underlying data. The need for the second approach arises from a shift in the underlying workloads, in which exceptionally high duplication ratios or the existence of millions of individual small files result in unacceptably slow GC using the file-level approach. Under such workloads, determining the liveness of chunks becomes a slow phase of logical GC. We find that physical GC decreases the execution time of this phase by up to two orders of magnitude in the case of extreme workloads and improves it by approximately 10–60% in the common case, but only after additional optimizations to compensate for its higher initialization overheads.","inCitations":["f962e285194ec80f584ea25f77dbbe836fa0c63c","18e74df64bdad475e585b165640c8db3dce00125"],"pdfUrls":["https://webcourse.cs.technion.ac.il/236834/Spring2017/ho/WCFiles/FAST17-GarbageCollection.pdf","https://www.usenix.org/sites/default/files/conference/protected-files/fast17_slides_duggal.pdf","https://www.usenix.org/conference/fast17/technical-sessions/presentation/douglis","http://www.usenix.org./system/files/conference/fast17/fast17-douglis.pdf","http://www.usenix.org./sites/default/files/conference/protected-files/fast17_slides_duggal.pdf","https://www.usenix.org/system/files/conference/fast17/fast17-douglis.pdf"],"title":"The Logic of Physical Garbage Collection in Deduplicating Storage","doi":"","sources":["DBLP"],"doiUrl":"","venue":"FAST"},
{"entities":["B+ tree","B-tree","Memcached","Persistent memory","Radix tree","Spatial database","Synthetic data","Tree structure"],"journalVolume":"","journalPages":"257-270","pmid":"","year":2017,"outCitations":["6abf5107efc723c655956f027b4a67565b048799","314919c141024c71cb17d525ecd8016138335002","24724ad8962a9e04eb496fddaefe9708f6960601","fdc8271d01d35673f82dbe8c230b581ca19f6623","81778c0996c46c77a66597e782ec0eb558f054f2","10d8afea57c8f159c4eb2664a40c8fb859acefef","4228d3fee9d28b3b3a7dc8e9585d02652109029f","7efeb43699d31e8ae365b1e4f7e56c066083a159","3afdc9f3ff5ce37bf204a4f92f4ab1a1bd0e7b5f","2e3bda19a2ed88e8a6e5cc415e27da551653ff1d","05a1357946de5eca42a477b7b268db4944219a2e","3d2dfe972be7a60937df97bd309b423726375cb4","642dd27ce62d51b042e134b0d0aec2f2e7cc4d29","9183cde02e4306828089fb8adae74736a9df3ceb","544c1ddf24b90c3dfba7b1934049911b869c99b4","5a49c1c694028fd6ca7acc6af601c0f54efa0700","295521cfe1a56458d53a58613de5fb92c97c5c23","202a362049618c8c485a235536e1540d3ce9265b","bf5497e15f22233cbc2a4d0c3cc2c36f26738701","94783d113951822195d4ba44599a8fcbdef9d4bf"],"s2Url":"https://semanticscholar.org/paper/79d380f14a4e117e1b7f66a4a2b1304717718f61","s2PdfUrl":"http://pdfs.semanticscholar.org/79d3/80f14a4e117e1b7f66a4a2b1304717718f61.pdf","id":"79d380f14a4e117e1b7f66a4a2b1304717718f61","authors":[{"name":"Se Kwon Lee","ids":["8568662"]},{"name":"K. Hyun Lim","ids":["38611538"]},{"name":"Hyunsub Song","ids":["9763760"]},{"name":"Beomseok Nam","ids":["1739708"]},{"name":"Sam H. Noh","ids":["1719212"]}],"journalName":"","paperAbstract":"Recent interest in persistent memory (PM) has stirred development of index structures that are efficient in PM. Recent such developments have all focused on variations of the B-tree. In this paper, we show that the radix tree, which is another less popular indexing structure, can be more appropriate as an efficient PM indexing structure. This is because the radix tree structure is determined by the prefix of the inserted keys and also does not require tree rebalancing operations and node granularity updates. However, the radix tree as-is cannot be used in PM. As another contribution, we present three radix tree variants, namely, WORT (Write Optimal Radix Tree), WOART (Write Optimal Adaptive Radix Tree), and ART+CoW. Of these, the first two are optimal for PM in the sense that they only use one 8-byte failure-atomic write per update to guarantee the consistency of the structure and do not require any duplicate copies for logging or CoW. Extensive performance studies show that our proposed radix tree variants perform considerable better than recently proposed B-tree variants for PM such NVTree, wB+Tree, and FPTree for synthetic workloads as well as in implementations within Memcached.","inCitations":["e3a66a2e79c2a5f7a52bf4e8089bf522fdc04abe","ec62b73a97016f09d5b9859d31ed991ae84e55ad","cb2a018979184f87692d423322e367cc42a215d2","4994eb0dfa2d15d7b5013563d018e8c16b71b039","433143d5a065cbc4a127362aec99002a1421e322"],"pdfUrls":["https://www.usenix.org/system/files/conference/fast17/fast17-lee.pdf","http://www.usenix.org./system/files/conference/fast17/fast17-lee.pdf","https://www.usenix.org/conference/fast17/technical-sessions/presentation/lee-se-kwon","http://www.usenix.org./sites/default/files/conference/protected-files/fast17_slides_kwon_lee.pdf","https://www.usenix.org/sites/default/files/conference/protected-files/fast17_slides_kwon_lee.pdf"],"title":"WORT: Write Optimal Radix Tree for Persistent Memory Storage Systems","doi":"","sources":["DBLP"],"doiUrl":"","venue":"FAST"},
{"entities":["Chunk (information)","Cloud storage","Computation","Counterfeit consumer goods","Data deduplication","Delta encoding","Version control"],"journalVolume":"","journalPages":"73-88","pmid":"","year":2017,"outCitations":["1161b9270ada3686352100946f75a6d215dbd07c","61b8ade95787896bb16978586e14fdda63149006","67972276329a51525048b9cd10c4649e03efb9b5","029d525dd48347fa4b8a48dbf4b41b4b37199a6a","d12d1289d2384c2ce642f01855637b9f0519e189","459f06b4f27456617100ce4212af8b1dc589dfd2","0e578433d4e8bb2a571c87a2d22816074902f009","054658151ae048ac31140fc5bc32342a23c1e52b","39e3d058a5987cb643e000bce555676d71be1c80","5ae3566cd07a04e32f61ade2fe4dae98d766df8c","336c18c068d37533654d96236dc48758f55fc818","1a5464cda7cff01d5a0ebb49df17fa8c05882295","6f8745fe003a9fcb782ec98b5a1671781cbd5771","35339f6f2e99c04920f21883df1db8004436cdc7","7840d62754c327c362d7b141e199dbc0f42c999a","7aa0a3076808db60aab117179910213d932b2c0e","195500f47236d16b8797fa5e0b0ac90b0e5aedd2","044a9cb24e2863c6bcaaf39b7a210fbb11b381e9","7b90149891786d6c34665ec2130628b16384eca7","f19870a1b4847ca61beed722d557a50189479d27","148a4703eafdb2b708b144e8e49544d3476c4844","07d63fc651eda7771fd1962abbc5b5ad43a82d58","2fbbf89a921e4aa19ee3bfe73d0b34a6ad764656","8932c6d523f9f5aba76993c71c0089ba15155a7c","2f8da4fea7268c6f846af7453d763d2ec2da6111","182cb3740940f403ff6f311fa54c5c1c9d7edc3f","a22e9c937345e5d679773223a44e7b7ea30c20f1","114801eccb5eb0831fd1848f351a138253a42f15","6d5cfe7723c61149d9cf905fe173268075b8c976","49cd4b3efdc10090150523e14b36de7ec9ff8755"],"s2Url":"https://semanticscholar.org/paper/1b98c203e9bd26801267fced78ea33a1151fadbd","s2PdfUrl":"http://pdfs.semanticscholar.org/1b98/c203e9bd26801267fced78ea33a1151fadbd.pdf","id":"1b98c203e9bd26801267fced78ea33a1151fadbd","authors":[{"name":"Xianzheng Dou","ids":["2176759"]},{"name":"Peter M. Chen","ids":["37845066"]},{"name":"Jason Flinn","ids":["1693763"]}],"journalName":"","paperAbstract":"Cloud-based storage provides reliability and ease-ofmanagement. Unfortunately, it can also incur significant costs for both storing and communicating data, even after using techniques such as chunk-based deduplication and delta compression. The current trend of providing access to past versions of data exacerbates both costs. In this paper, we show that deterministic recomputation of data can substantially reduce the cost of cloud storage. Borrowing a well-known dualism from the faulttolerance community, we note that any data can be equivalently represented by a log of the nondeterministic inputs needed to produce that data. We design a file system, called Knockoff, that selectively substitutes nondeterministic inputs for file data to reduce communication and storage costs. Knockoff compresses both data and computation logs: it uses chunk-based deduplication for file data and delta compression for logs of nondeterminism. In two studies, Knockoff reduces the average cost of sending files to the cloud without versioning by 21% and 24%; the relative benefit increases as versions are retained more frequently.","inCitations":[],"pdfUrls":["https://www.usenix.org/conference/fast17/technical-sessions/presentation/dou","http://www.usenix.org./sites/default/files/conference/protected-files/fast17_slides_dou.pdf","http://web.eecs.umich.edu/~jflinn/group/papers/login17.pdf","http://www.usenix.org./system/files/conference/fast17/fast17-dou.pdf","https://web.eecs.umich.edu/~pmchen/papers/dou17.slides.pdf","https://www.usenix.org/system/files/conference/fast17/fast17-dou.pdf","https://www.usenix.org/sites/default/files/conference/protected-files/fast17_slides_dou.pdf","http://web.eecs.umich.edu/~jflinn/group/papers/fast17.pdf"],"title":"Knockoff: Cheap Versions in the Cloud","doi":"","sources":["DBLP"],"doiUrl":"","venue":"FAST"},
{"entities":["Cloud storage","Clustered file system","Distributed computing","Fault tolerance","Next-generation network","Redundancy (engineering)","Software bug","System Fault Tolerance","Unavailability"],"journalVolume":"","journalPages":"149-166","pmid":"","year":2017,"outCitations":["16f3275f76adb337de8b77f899f83fc1085d8f0c","33438b1148a84d6e5bf2cad70bf7754d546ca5d7","3533159037bc2c11bde6b314e040ee113ae52bdd","3492873a8bc6d1d501dcac97e891c43dfecc29c0","51dae50f2cf78af88e8d064671b91e6e059ca7a6","58f692e9b03cb973355aab46bb6f867239aeb513","8a7536f311d22bd588c5bc2306d54d13effaee82","088e3e939ad234b6fdd0e321290fb26937dc2553","229acac1bd70c57e6a17f2c24f153c06d54de252","59250c7388caba98bd4adc2f1969fbec5500ed6a","32d23ce43877aa8cd385a8e01f366329dd015a5c","e4d41e048722ece6c3efed7afc372319f6eed66d","02fdca5fdba792e4f2c70b8b637abe4824343800","407a55ea947f5f430e8def26c5f4183db0f53c3a","108c840d5d1847948a2de0250490a327ae069ee6","18fe996c6f43a8f301cd842507045b679ba3506a","254171a0d89e34c73e800d6ec120842d2058b075","bed6d0e530f20332c284a463c754ce1d304aca38","086432132c177cc5e6d50a39a92cad540a162b40","2f97a44c85b299485dacb1e6ad3ac6f4e1ba42ab","67d07ba0550bbccba4ef34b409c9263b902de21c","155ca30ef360d66af571eee47c7f60f300e154db","4d3c779b5a224133bd5c69e05103fedbd904590a","2a46b97a285acd23a5c1acdfb466c30b224169fc","84fe12df86bfd0cc99dada158c94b4b71a433a52","09d1a6f5a50a8c3e066fb05a8833bc00663ada0e","0b4ada5b8bdabf4ee378b0992b2d3b70de0c07f7","3e1dfc784252219f573e9540685ea2bc1666a850","0ea1fee8651da646c5f2bdbbe83a58d05b6c5505","42512431ca7fffdbc80eb7280d093efcead3d48d","05dd6cb44124b8a210ac391f15ec25e68918ef22","40a00e89195903fbaffb364fe410a215faf6715c","c1a2e9ae8f1de1a2d7057fb7bf26a5b0567c67de","10e0397e08f37ee5d3ba1f9e24ac9eb313c784f4","11b8ef5da9c8df214859bb41b60001a0abd2b5b2","ab112b6ed1393736b7074032fdb1d81fcce4c955","6a1df9dae902f3d377f9c85ba9732b8d2270bf2b","8c52daf711d10f54b14b8993bd4e2585a7d28ceb","1761d2c31b685b2f099d18ec3457da8fb38241bf","3d1353c3ad9f641ac81edc70ae6f3a3198102b74","a9d0bea76adbcaf114f267e4c72237c020a1f47a","4108e4635351d6f2d0916ee19d0a0ef878649c3c","2be26e8aa238ac37a80e08303f128d8014bb9f3b","2a0d9931c1e794ffe96c4b6c37507e96a3c4cafc","01e9ddf1062f9a7d7847bb9bcd2371ce6e0d3e29","f4f37512b9786dbd9980d343e47fdc41032d0ca6","1551df94e19afc9513ed81f54ec907e3f9da0278","05dc4814248843389e8d2557e2d1f0c45d494e10"],"s2Url":"https://semanticscholar.org/paper/4bbd9d77460a14a628119d05332360c5d78df8d3","s2PdfUrl":"http://pdfs.semanticscholar.org/4bbd/9d77460a14a628119d05332360c5d78df8d3.pdf","id":"4bbd9d77460a14a628119d05332360c5d78df8d3","authors":[{"name":"Aishwarya Ganesan","ids":["38962174"]},{"name":"Ramnatthan Alagappan","ids":["31817919"]},{"name":"Andrea C. Arpaci-Dusseau","ids":["1743175"]},{"name":"Remzi H. Arpaci-Dusseau","ids":["1703415"]}],"journalName":"","paperAbstract":"We analyze how modern distributed storage systems behave in the presence of file-system faults such as data corruption and read and write errors. We characterize eight popular distributed storage systems and uncover numerous bugs related to file-system fault tolerance. We find that modern distributed systems do not consistently use redundancy to recover from file-system faults: a single file-system fault can cause catastrophic outcomes such as data loss, corruption, and unavailability. Our results have implications for the design of next generation fault-tolerant distributed and cloud storage systems.","inCitations":["b2cdaead2767d790accf30ba4c7eb7e99804dc4d","67ffec9c10d9594eb9af4afe25b1f0b0bce5f85d","c426f6394eefb7e2e1b02928f6fa16afde125aa2","87ca0d1d3f9fde3c93e4e7102fab0a133da31857","e97372229adcf4c015fcf43b3dcf3b51ddc48f2e","347e1352fb903b40dce606a1e581e9d601bc289c","5802c2ecb6e2449d9d6ddb3cac902f7cb10eaa10"],"pdfUrls":["https://www.usenix.org/conference/fast17/technical-sessions/presentation/ganesan","https://www.usenix.org/system/files/conference/fast17/fast17-ganesan.pdf","http://www.usenix.org./system/files/conference/fast17/fast17-ganesan.pdf","http://research.cs.wisc.edu/adsl/Publications/fast17-ganesan.pdf"],"title":"Redundancy Does Not Imply Fault Tolerance: Analysis of Distributed Storage Reactions to Single Errors and Corruptions","doi":"","sources":["DBLP"],"doiUrl":"","venue":"FAST"},
{"entities":["Benchmark (computing)","Best, worst and average case","Correctness (computer science)","Crash (computing)","Eventual consistency","Linux","Linux","Scheduling (computing)"],"journalVolume":"13","journalPages":"19:1-19:29","pmid":"","year":2017,"outCitations":["885c666fbcfd1a10c613496d7a041d01b99c7a39","6e90c995cc9caa0f7d9d68d536f5e16e9bcbbcd6","05a1357946de5eca42a477b7b268db4944219a2e","aed6e488244198d8bd9b882c8a53fff619666e7e","13c27125584651329f66461981cbb20fa63e9023","8c0573ba5f6aeb5a6391132ef26d613c045e6e1c","39e3d058a5987cb643e000bce555676d71be1c80","120c8504b4290920309165d48bb032f2c724a161","243c522b56809292f1f50117a9915053d32bf4fb","08e7d789b23d616c4c04432cf14b1836a73bbb6f","0420266f84cc95d6b7a8100e601f67d1118d4965","1cbaf27b55717e503284cfe339438c98da3a9867","10f1faeec4ee2158b8535b249a20de5419998153","3bbcce40cc2b9c848cb98e7ea8cd03a483aaca6c","23ee1c97c4a1229618bf6a614b02f33dc678fe6b","833da56175762daf644fe42b230917367264208c","68a9005a5ec10daece36ca5ecb9cad7be44770b1","2be26e8aa238ac37a80e08303f128d8014bb9f3b","2b1f67102166434c404e5f0bcd6e3da1c6837363","14cb2d4f902544862076519d9e424d071612a15e","bee0a31573c37a5808a0af25d39de98e06c385d8","265d18ced11e2e64d98afa97b0e86965e68101f7","045a975c1753724b3a0780673ee92b37b9827be6","acca916dcf29e548a8f3bd53b05acd18380b0f03","47b78e7eb12859a141aed6a28a4e301eb0352629","075f51b0aeaac7ebed18d5fbf67e64a14c8943f1","3492873a8bc6d1d501dcac97e891c43dfecc29c0","7e4ecfc13aba74db770378e640d5fbcce7fd3d2e","36f49b05d764bf5c10428b082c2d96c13c4203b9","00918f711d847f9934b606b9a1d6622ca24fc3ec","128c3e04314e6fca8deed005d74a3d1ba36ad293","6c2bc356d3abc932d2a15068728261bef5aae69d","34ef9c71821bd3ed7fa52c9178e1ee272fedb803","088e3e939ad234b6fdd0e321290fb26937dc2553","199ac28b6bc68bf05c77645ffae7640df114bca5","765e6f4feeb1f7d59d2b3c011e2e38814a958afa","036b85d48048b47180058034bde97ae633ba8c28","09c0d62190aedb53e820695ccbe98d90f877cc46","274e495824827f5a9dc1ba3ab62620445e6b3d4b","071686697917fd56ae8ace0c4d6bfcf3bef5700a","29357ed9c2b0b6e76dda247bbe90aa1dd39089aa","26f820aa9e782f5d6ba8bcb272a31c32094dfd59","25a83ec7cc04a5bf22061b78164c9d09a4de21a5"],"s2Url":"https://semanticscholar.org/paper/00caa4dea9216bec01b465f8a69d0e1becc07b7a","s2PdfUrl":"","id":"00caa4dea9216bec01b465f8a69d0e1becc07b7a","authors":[{"name":"Thanumalayan Sankaranarayana Pillai","ids":["2598683"]},{"name":"Ramnatthan Alagappan","ids":["31817919"]},{"name":"Lanyue Lu","ids":["2170646"]},{"name":"Vijay Chidambaram","ids":["2002462"]},{"name":"Andrea C. Arpaci-Dusseau","ids":["1743175"]},{"name":"Remzi H. Arpaci-Dusseau","ids":["1703415"]}],"journalName":"TOS","paperAbstract":"Recent research has shown that applications often incorrectly implement crash consistency. We present the Crash-Consistent File System (ccfs), a file system that improves the correctness of application-level crash consistency protocols while maintaining high performance. A key idea in ccfs is the abstraction of a <i>stream</i>. Within a stream, updates are committed in program order, improving correctness; across streams, there are no ordering restrictions, enabling scheduling flexibility and high performance. We empirically demonstrate that applications running atop ccfs achieve high levels of crash consistency. Further, we show that ccfs performance under standard file-system benchmarks is excellent, in the worst case on par with the highest performing modes of Linux ext4, and in some cases notably better. Overall, we demonstrate that both application correctness and high performance can be realized in a modern file system.","inCitations":["0f4386d4a521e36cb15252b4e908a948a65252ef","a377d5f506a411c5d95361188c0b7f500fc2ca09","47f645013589f0c3babc505ee846711605f46226","4e731dfc4eee0006865d131b384f46b29965f42e","e97372229adcf4c015fcf43b3dcf3b51ddc48f2e","ade874e837a2a6b9ce67fad0c5dce6f4e3c68d11","ad42b4773cd461ba58bda07e1b7b0ff24c4ddba4","8d555af4ad0bcb45ac5ce62374fbd23ea429121f","41da20c0fb04dd4769f3772e392362acd893af57","347e1352fb903b40dce606a1e581e9d601bc289c"],"pdfUrls":["http://research.cs.wisc.edu/wind/Publications/ccfs-tos17.pdf","http://research.cs.wisc.edu/adsl/Publications/fast17-pillai.pdf","https://www.snia.org/sites/default/files/SDC/2017/presentations/etc/Pillai_Thanu_Application_Crash_Consistency_and_Performance_with_CCFS.pdf","http://www.cs.utexas.edu/~vijay/papers/ccfs-fast17-slides.pdf","http://research.cs.wisc.edu/adsl/Publications/fast17-thanu-slides.pdf","http://www.usenix.org./system/files/conference/fast17/fast17_pillai.pdf","http://www.usenix.org./sites/default/files/conference/protected-files/fast17_slides_pillai.pdf","http://www.cs.utexas.edu/~vijay/papers/fast17-c2fs.pdf","http://doi.acm.org/10.1145/3119897","https://www.usenix.org/conference/fast17/technical-sessions/presentation/pillai","https://www.usenix.org/sites/default/files/conference/protected-files/fast17_slides_pillai.pdf","https://www.usenix.org/system/files/conference/fast17/fast17_pillai.pdf","https://www.usenix.org/conference/atc17/technical-sessions/presentation/pillai"],"title":"Application Crash Consistency and Performance with CCFS","doi":"10.1145/3119897","sources":["DBLP"],"doiUrl":"https://doi.org/10.1145/3119897","venue":"FAST"},
{"entities":["Amortized analysis","Application programming interface","File system API","HTTP/2","High- and low-level","POSIX","Remote procedure call","Server (computing)","Unix","User space","Virtual private network"],"journalVolume":"","journalPages":"301-314","pmid":"","year":2017,"outCitations":["0e956dc3288b6ff2345238448236045d3033c09f","44028c00bf3872ae06aa46f569c3b9dceebdd909","3c693d3a4a40e6e98c0f839205c4c308d4326a8c","f93b1ccf2a97edd055afa4e6a3e32770cbc002de","5c6d7d5b165c1037db4434577db1f5155ef8f00e","03eb427813552b2165e5250105e55dbfb7ef151e","c14abc3126b27e15ca7c50f4b827cec912d00449","97158a13a871720757114a8dcb8d8f4e104d8693","5691b8cc2f9b76c5371125137b9410727d393fe3","45114ae3c9263c2ad9e042d66b224e5de3e649d4","b46cb54a87a448212af37f2594a512fec39a059e","af3735131085f326148fa8885ec41bd637d130a2","04b7525f514eff641e8e0e734ab96675c82c6a06","1e9f092e114393ba786cb2002b6f1b0dabe875fe","6d38e49cf1f121712f19805ec779905bc9507e58","29d45feaa50b0304ab52bd5c6d0381c21c2b42bc","0641c61c2709ea41536cf78bcc6316fb4951b5ab","44f474a25ee7d1fccfb97bd4e64ffc7ae0df61e8","1ea75edc65bf349f9f057213b391ab8be4cc3ab6","84af1a7cf881369e759e53ab143fc01e66d3b8a8","6da927dcc24e0550cfba60a4338052370c7892fb","10fede77f843e9eb5ef1768a17543013616d9243","627c0d36688b2252ae3ca0b5f68ce97e341d338d","1ef607f45cb77db12e7a52e6de052a9a0ebb830b","38a48d914e1a47e617fd5031f5a18388a2ddf4ab"],"s2Url":"https://semanticscholar.org/paper/70745c6514748e8d3764d77abf410edd90a597ac","s2PdfUrl":"","id":"70745c6514748e8d3764d77abf410edd90a597ac","authors":[{"name":"Ming Chen","ids":["1711264"]},{"name":"Dean Hildebrand","ids":["32401480"]},{"name":"Henry Nelson","ids":["40033094"]},{"name":"Jasmit Saluja","ids":["4884504"]},{"name":"Ashok Sankar Harihara Subramony","ids":["9755399"]},{"name":"Erez Zadok","ids":["1708491"]}],"journalName":"","paperAbstract":"Modern systems use networks extensively, accessing both services and storage across local and remote networks. Latency is a key performance challenge, and packing multiple small operations into fewer large ones is an effective way to amortize that cost, especially after years of significant improvement in bandwidth but not latency. To this end, the NFSv4 protocol supports a <i>compounding</i> feature to combine multiple operations. Yet compounding has been underused since its conception because the synchronous POSIX file-system API issues only one (small) request at a time.\n We propose <i>vNFS</i>, an NFSv4.1-compliant client that exposes a vectorized high-level API and leverages NFS <i>compound procedures</i> to maximize performance. We designed and implemented vNFS as a user-space RPC library that supports an assortment of bulk operations on multiple files and directories. We found it easy to modify several UNIX utilities, an HTTP/2 server, and Filebench to use vNFS. We evaluated vNFS under a wide range of workloads and network latency conditions, showing that vNFS improves performance even for low-latency networks. On high-latency networks, vNFS can improve performance by as much as two orders of magnitude.","inCitations":["247289ef2f00ba86c8e3a28cc692197d5741ae6e","c7e2c4bea500ea7926a50973d861f01bb8e5e364","09d446b184787213e45035c7e7657056373fb1b1"],"pdfUrls":["https://www.usenix.org/sites/default/files/conference/protected-files/fast17_slides_ming_chen.pdf","https://www.usenix.org/conference/fast17/technical-sessions/presentation/chen","http://www.usenix.org./sites/default/files/conference/protected-files/fast17_slides_ming_chen.pdf","https://www.usenix.org/system/files/conference/fast17/fast17-chen.pdf","http://www.usenix.org./system/files/conference/fast17/fast17-chen.pdf","http://doi.acm.org/10.1145/3116213","http://www.fsl.cs.stonybrook.edu/docs/nfs4perf/vnfs-fast17.pdf","http://www.fsl.cs.sunysb.edu/docs/nfs4perf/vnfs-fast17.pdf"],"title":"vNFS: Maximizing NFS Performance with Compounds and Vectorized I/O","doi":"10.1145/3116213","sources":["DBLP"],"doiUrl":"https://doi.org/10.1145/3116213","venue":"FAST"},
{"entities":["Adobe Flash","Allocate-on-flush","Expect","Flash memory","Garbage collection (computer science)","Instability","Long tail","Random-access memory","Service control point","Solid-state drive","USB flash drive","Whole Earth 'Lectronic Link"],"journalVolume":"","journalPages":"15-28","pmid":"","year":2017,"outCitations":["09bcd050bb006639ae8bcacb3af149f0b6d964f3","0831a5baf38c9b3d43c755319a602b15fc01c52d","13b925352e4ee3066a6d38ef9f16efdfa967cabb","057d21830cde5b3be2fdb3a74ee69a3c7e9109f8","10e0397e08f37ee5d3ba1f9e24ac9eb313c784f4","4ba4613eab33cddc53bec9e14e50d03fa66270ca","7a6987f6b0b47d8c6a39cccebb2d3c9566e45054","3cf9039fa2fc01f711870e33d868669caf5c4df4","3e8e43f61b3af63c6a8bb981b5d085c8afb1b9e2","a6069e65c318f07d2b35934b0d4109148f190342","070c3a8c3ce10277424f23c01a54b377478ee59c","bbb2c69a8018ac50d97a912282b1ec4ff8302ca7","61977858b3eea4f5a6d81393301e7298ade7a2d8","424a0f460b4f261b386787bdec37a2b01347a930","5271d6693ba950c389921ccc21110664f25a83db","f11d2748e1e26f3b01b54db85ddcc287b678cb04","1820a34042d6371a9e20484b0c63b698eb522a6c","1425d2c0d221762fe1f7d9be0c86e5b92adf3b44","2e46f9074bd81ea4ec29ecec7e0231c16fb2e8db","19b90ae79266b89ecc44113409b424044ec0300f","09d1a6f5a50a8c3e066fb05a8833bc00663ada0e","086820e40dc8046c30a8751394df167bec047fe1","3f9d4a16ec5d08c0309df743e73745f876b9abfa","e03a77db7bdbef30192b6846cfa09d57e135e8c7","f4f37512b9786dbd9980d343e47fdc41032d0ca6","f9fa36f07645df8765faeca8f8a95f1856bb5bb0","a04678ad8398a2579c249fff4b59bfbcfdd7e25b","303f71ad0e145415aba9efe9ba96a1f734c63391","0da73832dee2c9b3d4c0d039d8e714e6ff098e40","1ecbb1f2080029357bba55e3747bfcaac82aee51","26a88fcaf621270af5f5786fdb2df376a2bc00aa","1f0c405f9fa2cc9de23a45710fa85b9e7330a958"],"s2Url":"https://semanticscholar.org/paper/3de30c8dafc720bf066e5e3a005d16212dd31149","s2PdfUrl":"","id":"3de30c8dafc720bf066e5e3a005d16212dd31149","authors":[{"name":"Shiqin Yan","ids":["9765570"]},{"name":"Huaicheng Li","ids":["9751178"]},{"name":"Mingzhe Hao","ids":["2725752"]},{"name":"Michael Hao Tong","ids":["32249376"]},{"name":"Swaminathan Sundararaman","ids":["2752943"]},{"name":"Andrew A. Chien","ids":["1695232"]},{"name":"Haryadi S. Gunawi","ids":["1738725"]}],"journalName":"","paperAbstract":"Flash storage has become the mainstream destination for storage users. However, SSDs do not always deliver the performance that users expect. The core culprit of flash performance instability is the well-known garbage collection (GC) process, which causes long delays as the SSD cannot serve (blocks) incoming I/Os, which then induces the long tail latency problem. We present <scp>tt</scp>F<scp>lash</scp> as a solution to this problem. <scp>tt</scp>F<scp>lash</scp> is a &#x0201C;tiny-tail&#x0201D; flash drive (SSD) that eliminates GC-induced tail latencies by circumventing GC-blocked I/Os with four novel strategies: plane-blocking GC, rotating GC, GC-tolerant read, and GC-tolerant flush. These four strategies leverage the timely combination of modern SSD internal technologies such as powerful controllers, parity-based redundancies, and capacitor-backed RAM. Our strategies are dependent on the use of intra-plane copyback operations. Through an extensive evaluation, we show that <scp>tt</scp>F<scp>lash</scp> comes significantly close to a &#x0201C;no-GC&#x0201D; scenario. Specifically, between the 99 and 99.99th percentiles, <scp>tt</scp>F<scp>lash</scp> is only 1.0 to 2.6&#215; slower than the no-GC case, while a base approach suffers from 5&#x02013;138&#215; GC-induced slowdowns.","inCitations":["3a426d5e6835af117465e2bbef965cc19f7a6e8e","4d1a62de587f05084e85a4168f960af1e48b9697","ec3924af8c1cb428b4f1309b9a9ca3c86abd6631","8bbba8c51e79b4ec86d95141a24b6c9a3c6eac6b","1d08d231ec66645ec56d2210c1a7c6b44c6ff041","40f196e21a289394c4354961116587b8accba45e","1858ed4ca900d9afd06d0b8a8430d0dda8f957bc","8f849c0051edc612327e1121ccfa70a4ec0bacea","262c16d1bdd8d0ccef77bd66648144d584a24477","6450300d1d15ce03ddca2339184fc6b964189498","347e1352fb903b40dce606a1e581e9d601bc289c"],"pdfUrls":["https://www.usenix.org/conference/fast17/technical-sessions/presentation/yan","http://www.usenix.org./system/files/conference/fast17/fast17-yan.pdf","https://www.usenix.org/system/files/conference/fast17/fast17-yan.pdf","http://doi.acm.org/10.1145/3121133"],"title":"Tiny-Tail Flash: Near-Perfect Elimination of Garbage Collection Tail Latencies in NAND SSDs","doi":"10.1145/3121133","sources":["DBLP"],"doiUrl":"https://doi.org/10.1145/3121133","venue":"FAST"},
{"entities":["Benchmark (computing)","Disk storage","File server","Linux","Linux","Magnetic storage","Plug compatible","Server (computing)","Shingled magnetic recording","Throughput"],"journalVolume":"","journalPages":"105-120","pmid":"","year":2017,"outCitations":["4468cbc8a9ad13ebeaa210424e842f158415ab07","11fe43dfcf43802595c2076c7641aff6f025e1ec","b30cdebe589719c9780dbb7034fee48c109a1716","27c1898a013f0df5b5d1fb8a7edde5ce435c9d46","b84f8f1e8da2494d1bd5f3a065228d097d3cfe1f","2da760f90c3d2bf6598becdde9063093f488548c","b81b73725e1e037ffa1a935e54f67ea5e7703b86","1b0eace707f6b86e94793d1a7c83b7d065e604fa","093d1d23500d65e99c7d0cd5569ce0f0d4c37076","8dee6c0a8438a995b1d2452b84c7544be5f00578","9d5e9f98f85629d9dae20d181ff2c9fcdcdb5520","088e3e939ad234b6fdd0e321290fb26937dc2553","ba356329a7c6672eca15815ed622dac2c71b4513","5c06564087db9e53a72ef1eb5865696b0dddd8ca","081fa9cfe750d58db2cd1f5e53e728c5de636910","0ad098eca1a5cc262da3d8f99229efbaee88b02e","7a08e5fda1d9d1e0a8cc9bc4e62dfb74471fc32c","0420266f84cc95d6b7a8100e601f67d1118d4965","2018f3fc13cd38122abdf37bf939b5011cd2e3c9","aeef17ecae6f2469f68f9f3b709f640949a1f438","499b458ddd5d10dc4be158fd89aeed0e31b6b5cf","158ebe313a72857c5534a313f3ec0e413593b732","f1780c4ebcba175f8985058dcad9fe407f526587","bc0c53752004832ff9e6e0f56539fb63ae1df154","0bf50c9aff7d5182504dd18b7cc0f6041b5e520b","6d7569ecf4455d3f736dffdc3770213b59a07b44","2167c708155dac4bb63d29a4bcc960dd320d8e2a","12a0046a1197ae63c3d616c74e367dc583cef196","b8ddec47f9fab1eddb5c9cacf703781dd5337b87","4acdb61098053f38d5500a9ef974d24828696b9d","501f491dd60ea26bcb8152bfd3f9ac2456e69da8","006cb2c8713bff9e97a8c68c65e66b98379731f7"],"s2Url":"https://semanticscholar.org/paper/75d60809b9ac769a4a7e2a9907b3bc028ac58935","s2PdfUrl":"http://pdfs.semanticscholar.org/c0d0/bcbd802e8957b9220a92e78478f4f8cc670f.pdf","id":"75d60809b9ac769a4a7e2a9907b3bc028ac58935","authors":[{"name":"Abutalib Aghayev","ids":["40040203"]},{"name":"Theodore Y. Ts'o","ids":["2395925"]},{"name":"Garth A. Gibson","ids":["30367882"]},{"name":"Peter Desnoyers","ids":["2083064"]}],"journalName":"","paperAbstract":"Drive-Managed SMR (Shingled Magnetic Recording) disks offer a plug-compatible higher-capacity replacement for conventional disks. For non-sequential workloads, these disks show bimodal behavior: After a short period of high throughput they enter a continuous period of low throughput. We introduce ext4-lazy1, a small change to the Linux ext4 file system that significantly improves the throughput in both modes. We present benchmarks on four different drive-managed SMR disks from two vendors, showing that ext4-lazy achieves 1.7-5.4× improvement over ext4 on a metadata-light file server benchmark. On metadata-heavy benchmarks it achieves 2-13× improvement over ext4 on drive-managed SMR disks as well as on conventional disks.","inCitations":["ec3924af8c1cb428b4f1309b9a9ca3c86abd6631","2fe51b5c34484b5fb8f0ec54483750ffc842fd4a","c2b8a1485ad43085b80ccf8d29f029edcba08529","40f196e21a289394c4354961116587b8accba45e","0ae03e097cd936f564a60017b864beeb12635b09","a377d5f506a411c5d95361188c0b7f500fc2ca09","18e93539fe6163a0b56f3427fc562733f89449a6","537d37be13687758d01e35fc6a62be118ec48ea1","8e4cdaa006bce928ed7a6d37b9bfbfdffe2a6367"],"pdfUrls":["http://www.pdl.cmu.edu/PDL-FTP/Storage/ext4-lazy.pdf","https://www.usenix.org/sites/default/files/conference/protected-files/fast17_slides_aghayev_0.pdf","http://www.usenix.org./sites/default/files/conference/protected-files/fast17_slides_aghayev_0.pdf","https://www.usenix.org/conference/fast17/technical-sessions/presentation/aghayev","https://www.usenix.org/system/files/conference/fast17/fast17-aghayev.pdf","http://www.usenix.org./system/files/conference/fast17/fast17-aghayev.pdf"],"title":"Evolving Ext4 for Shingled Disks","doi":"","sources":["DBLP"],"doiUrl":"","venue":"FAST"},
{"entities":["Die (integrated circuit)","Experiment","Flash memory","Multitenancy","Parallel computing","Solid-state drive"],"journalVolume":"","journalPages":"375-390","pmid":"","year":2017,"outCitations":["27cb0c2229299a82cf767d19dcc68aa1e5f0f233","84127db83f5ce3a3f92f2f114a10a65a4a342b06","40f04909aaa24b09569863aa71e76fe3d284cdb0","26a88fcaf621270af5f5786fdb2df376a2bc00aa","088e3e939ad234b6fdd0e321290fb26937dc2553","01438abf044c42f90de0591e08fe33461908c6cd","438c51040ee6ccf9198e52d105c47e75d615b29c","3c0bc4e9d30719269b0048d4f36752ab964145dd","13d6c568c770ff5a070072e720fb34b0037cdab8","0e5c646909bb762da0cd325e084655c12445578f","33438b1148a84d6e5bf2cad70bf7754d546ca5d7","dc7f57cf92f8aa87c33853a724a3fa19c7ba12ce","490d862480cf30949dce90e832aa292c498ac768","4ba4613eab33cddc53bec9e14e50d03fa66270ca","81b761ea5c679b452f4a78fa176b8e2d608e77ac","72722e7602138e3896e5576d3f3ef730e7b7c4b4","2e46f9074bd81ea4ec29ecec7e0231c16fb2e8db","7f713eeef50a87ec595c64832fdaf25ffa38b5bd","b45e1f16cf2b6f735013e9f279e45bf8b7a8d5db","03543f75c4fe0c49f81af789a1c7293ff0e4e107","5909192b374eac0cda4df7c986ebc997cdcd6002","726099036bb32c3fbaf1650d5900eeaa2ecc8fd9","2269889c9085ff518ee9e7f5b2f92e4599dd3ff2","38a9120f780602521af9744e31d80ef5cd9593a7","1820a34042d6371a9e20484b0c63b698eb522a6c","3cf9039fa2fc01f711870e33d868669caf5c4df4","389b618c42c0d5d32a569b9cbaa02a7ff77c6be6","65a2cb8a02795015b398856327bdccc36214cdc6","6d44790b6d952eff28f302998e8121f90786e3ff","131e1e1d163a0f49881d7b5ac092892093391015","13b925352e4ee3066a6d38ef9f16efdfa967cabb","3e8e43f61b3af63c6a8bb981b5d085c8afb1b9e2","61977858b3eea4f5a6d81393301e7298ade7a2d8","19ffc4f5129ed9d39f498f4eb901024c514263c7","2451dc6bb08d2668f4a876ce94d0c15227ccab7a","9aa0d7253574e50fe3a190ccd924433f048997dd","5c06564087db9e53a72ef1eb5865696b0dddd8ca","8969f883979ac45fe24cecde39c15ddc4bd756d3","151fe4cd7d0c788b3e362636d5c31a4c13f90a9a","26e72340c47b7348e1b1de285f89dd96cc925b27","8c9a91b774fcc126db7ce7c67bd97d1d16143932","d67adb456a315aee244babf4f20e318cc14d13f3","7fb6b53bdc81f06fd34d5d9c2dd00f6e38cfd98b","4251f331db37a1c2c16c2e0c4daa729074c99110","0da73832dee2c9b3d4c0d039d8e714e6ff098e40","dbcdb4c402756b2b5ac910b9eb17ddb412290d16","061944ca83bb46fac511394dca642f7af2d2858a","d137b83c3e43d4953cc389cb0a50619cc7be5319","05961fc1d02ca30653dd0b4c906113db796df941"],"s2Url":"https://semanticscholar.org/paper/05a1bad1ef2341339e18d636d78594226d4ee8e6","s2PdfUrl":"http://pdfs.semanticscholar.org/4a85/7d84a1f410b5264683a3d2d1d959d2085e44.pdf","id":"05a1bad1ef2341339e18d636d78594226d4ee8e6","authors":[{"name":"Jian Huang","ids":["36937479"]},{"name":"Anirudh Badam","ids":["1783539"]},{"name":"Laura Caulfield","ids":["9725581"]},{"name":"Suman Nath","ids":["39496676"]},{"name":"Sudipta Sengupta","ids":["1690586"]},{"name":"Bikash Sharma","ids":["39807362"]},{"name":"Moinuddin K. Qureshi","ids":["1740036"]}],"journalName":"","paperAbstract":"A longstanding goal of SSD virtualization has been to provide performance isolation between multiple tenants sharing the device. Virtualizing SSDs, however, has traditionally been a challenge because of the fundamental tussle between resource isolation and the lifetime of the device – existing SSDs aim to uniformly age all the regions of flash and this hurts isolation. We propose utilizing flash parallelism to improve isolation between virtual SSDs by running them on dedicated channels and dies. Furthermore, we offer a complete solution by also managing the wear. We propose allowing the wear of different channels and dies to diverge at fine time granularities in favor of isolation and adjusting that imbalance at a coarse time granularity in a principled manner. Our experiments show that the new SSD wears uniformly while the 99th percentile latencies of storage operations in a variety of multi-tenant settings are reduced by up to 3.1x compared to software isolated virtual SSDs.","inCitations":["2971948a9229ed61604778b76e03d5a31328a7cb","65c43d1b70985054907e08fddb4a9907244b0801","40dc09f5fbd3776c3f34adedc7a4718307ace0d6","226ca798b529c13605a2aa7fe75d58f4188f850a","40f196e21a289394c4354961116587b8accba45e","0da73832dee2c9b3d4c0d039d8e714e6ff098e40","55318fe320d8217fdc0e1359f04ac79844222c8e","1d08d231ec66645ec56d2210c1a7c6b44c6ff041"],"pdfUrls":["http://www.cc.gatech.edu/grads/j/jhuang95/papers/flashblox-fast17.pdf","http://www.usenix.org./system/files/conference/fast17/fast17_huang.pdf","https://www.usenix.org/sites/default/files/conference/protected-files/fast17_slides_huang.pdf","http://www.cc.gatech.edu/~jhuang95/papers/flashblox-fast17.pdf","https://www.usenix.org/conference/fast17/technical-sessions/presentation/huang","http://www.usenix.org./sites/default/files/conference/protected-files/fast17_slides_huang.pdf","http://www.cc.gatech.edu/grads/j/jhuang95/papers/fast17_slides_huang.pdf","https://www.usenix.org/system/files/conference/fast17/fast17_huang.pdf"],"title":"FlashBlox: Achieving Both Performance Isolation and Uniform Lifetime for Virtualized SSDs","doi":"","sources":["DBLP"],"doiUrl":"","venue":"FAST"},
{"entities":["Action Replay","Heuristic","Operating system","Performance Evaluation","Replay value","Scalability"],"journalVolume":"","journalPages":"315-328","pmid":"","year":2017,"outCitations":["5ecd441bf4a54c7500cd6fc185d8dd09638e12cb","11e44206984ce4186fd4b6181a5d902056e50e64","02acd9c204a2f21087b1ea268aca3a90eb885c74","972f0a8fcebe4fe91f6dce31577a45f72654b494","a054aa76bdea18dbd20525ccb876de029dc2cc8a"],"s2Url":"https://semanticscholar.org/paper/b80efd014171185080e84b72cbdc038066c396c9","s2PdfUrl":"http://pdfs.semanticscholar.org/eb7c/5e816eb53a4dd310c50039773d9950de4792.pdf","id":"b80efd014171185080e84b72cbdc038066c396c9","authors":[{"name":"Alireza Haghdoost","ids":["3240730"]},{"name":"Weiping He","ids":["23473919"]},{"name":"Jerry Fredin","ids":["9765678"]},{"name":"David Hung-Chang Du","ids":["1717128"]}],"journalName":"","paperAbstract":"We introduce a replay tool that can be used to replay captured I/O workloads for performance evaluation of highperformance storage systems. We study several sources in the stock operating system that introduce the uncertainty of replaying a workload. Based on the remedies of these findings, we design and develop a new replay tool called hfplayer that can more accurately replay intensive block I/O workloads in a similar unscaled environment. However, to replay a given workload trace in a scaled environment, the dependency between I/O requests becomes crucial. Therefore, we propose a heuristic way of speculating I/O dependencies in a block I/O trace. Using the generated dependency graph, hfplayer is capable of replaying the I/O workload in a scaled environment. We evaluate hfplayer with a wide range of workloads using several accuracy metrics and find that it produces better accuracy when compared with two exiting available replay tools.","inCitations":["8bb173e48e7d3a6a083eef6b9aa3bc7f715ea550","7e126097bd676e496e7e4675e9d46fcef9220253"],"pdfUrls":["https://www.usenix.org/sites/default/files/conference/protected-files/fast17_slides_haghdoost_.pdf","http://www.usenix.org./sites/default/files/conference/protected-files/fast17_slides_haghdoost_.pdf","https://www.usenix.org/conference/fast17/technical-sessions/presentation/haghdoost","https://www.usenix.org/system/files/conference/fast17/fast17-haghdoost.pdf","http://www.usenix.org./system/files/conference/fast17/fast17-haghdoost.pdf"],"title":"On the Accuracy and Scalability of Intensive I/O Workload Replay","doi":"","sources":["DBLP"],"doiUrl":"","venue":"FAST"},
{"entities":["Control plane","Forwarding plane","High- and low-level","Multitenancy","Pervasive informatics","Requirement","Scalability","Software-defined storage","Swift (programming language)"],"journalVolume":"","journalPages":"243-256","pmid":"","year":2017,"outCitations":["138a9c2a9579435cd8cb0f24e7ec135821074557","044604ae6a0ba104f6ad5bc18a4f1dede23c17fe","003b9858f46501081d8609ba9ad12a5b34deffb3","27277971cb33674044917aacb4cc448b7084095f","4288771fb413ed92b006fd46ddbee56132e0a21b","be4c6170ee4fd72ff5c8fc92e3d6ba5cba774cf6","030c5d1b06de23942fbfc2f5eb3572f8dad24a90","2dcb7a97c2fe1184ed4ec2b83e984214a908d0a2","7b13f7706ab5341fa87128edb9860464675347ba","4b8fbe5e18af87ce47b728bf7b4e644c9de0c95e","1ea81e7477051ba7769dc50a97f3b2b01d5ee9da","111e2d5634cb30d5d841cdb22563f9b371fb5f54","65a2cb8a02795015b398856327bdccc36214cdc6","8a33c47c2a3f0e46dbb30f5203b6a1c6d8fefd8f","294b42e48050655469f4579daea4cae9bcffd861","5053d80a916aa6be5d1f2253a5f420954da7a3e4","18e35895d1f38608f61cbd4a9ecee05a28c1cd0b","62c8ebe08eff7fe23d949940b7802f6aa0eb81b5","75e74a0f013e9028c69df3addc0d161ef35d0c51","31ee28ad7207eb9e3f558488786a888a42bbb907","dbcdb4c402756b2b5ac910b9eb17ddb412290d16","c4415396bd0f182a01ac6f9cf9e14894e51d08e4","28eb88b180674f43381ede3e9573689496cfd321","941ee828449a815e3bee12a967691d18ebfc0780","1d2871c56d07a35e6709d535fbbb2df6b434962a","92e536c1789bf301f456b01590006c9a3eff6cd8","0b2c84be9e9f97f2464ad9d09be5f4c37edda47e","a3de178c43b990b5755be4d640a7525f97ce2f33","11ceeea43c970abede5aa95b4bfce621138a0bed","807df0de011be333fc1dd06ac58c426e8b3437ef","18a5db040efe208ec2728a91096d3cf3640282a1","396514fb219879a4a18762cddfae2a6a607f439f"],"s2Url":"https://semanticscholar.org/paper/1ddd410257b00370c1fe58377f02f608ff16c3bf","s2PdfUrl":"http://pdfs.semanticscholar.org/1ddd/410257b00370c1fe58377f02f608ff16c3bf.pdf","id":"1ddd410257b00370c1fe58377f02f608ff16c3bf","authors":[{"name":"Raúl Gracia Tinedo","ids":["2715350"]},{"name":"Josep Sampé","ids":["39637073"]},{"name":"Edgar Zamora-Gómez","ids":["2013067"]},{"name":"Marc Sánchez Artigas","ids":["1749419"]},{"name":"Pedro García López","ids":["1695568"]},{"name":"Yosef Moatti","ids":["2888002"]},{"name":"Eran Rom","ids":["3318516"]}],"journalName":"","paperAbstract":"Object stores are becoming pervasive due to their scalability and simplicity. Their broad adoption, however, contrasts with their rigidity for handling heterogeneous workloads and applications with evolving requirements, which prevents the adaptation of the system to such varied needs. In this work, we present Crystal, the first Software-Defined Storage (SDS) architecture whose core objective is to efficiently support multi-tenancy in object stores. Crystal adds a filtering abstraction at the data plane and exposes it to the control plane to enable high-level policies at the tenant, container and object granularities. Crystal translates these policies into a set of distributed controllers that can orchestrate filters at the data plane based on real-time workload information. We demonstrate Crystal through two use cases on top of OpenStack Swift: One that proves its storage automation capabilities, and another that differentiates IO bandwidth in a multi-tenant scenario. We show that Crystal is an extensible platform to deploy new SDS services for object stores with small overhead.","inCitations":["86991eb6eed3e12f5b3985340416302a2208bceb","214f4f4f555b608e59314168b08ed9daa4087200","0b53cea748bdb5a404ed6999c23eb097622f0f08"],"pdfUrls":["https://www.usenix.org/system/files/conference/fast17/fast17-gracia-tinedo.pdf","https://www.usenix.org/sites/default/files/conference/protected-files/fast17_slides_tinedo.pdf","https://www.usenix.org/conference/fast17/technical-sessions/presentation/gracia-tinedo","http://www.usenix.org./system/files/conference/fast17/fast17-gracia-tinedo.pdf","http://www.usenix.org./sites/default/files/conference/protected-files/fast17_slides_tinedo.pdf"],"title":"Crystal: Software-Defined Storage for Multi-Tenant Object Stores","doi":"","sources":["DBLP"],"doiUrl":"","venue":"FAST"},
{"entities":["Application checkpointing","Data-intensive computing","Database","Holism","Linux","Linux","MongoDB","PostgreSQL","Priority inversion","Redis","Throughput"],"journalVolume":"","journalPages":"345-358","pmid":"","year":2017,"outCitations":["012ab4527d6aee2387c243d304c624f3b9cf03f3","1d99b7749a9311d2db24a3d84728e444eff23e4b","6e90c995cc9caa0f7d9d68d536f5e16e9bcbbcd6","a30de973f68640b5032d07e2ee3ee80f03d292c5","9c046601e01d693c1d36a074c00d226c563c76f2","65fa329956f69119c4da6afa2ee2ed634ba9e464","01c933428cc95b901cc19da06d7ac5dbcb31e4f6","0340b5830450f7e94023af098d4e9af37a33fdcd"],"s2Url":"https://semanticscholar.org/paper/74d23cb8751120849bc908477b28c886c6a76252","s2PdfUrl":"http://pdfs.semanticscholar.org/9629/7c9a920b1c6cbd9712d00368a947b5062a50.pdf","id":"74d23cb8751120849bc908477b28c886c6a76252","authors":[{"name":"Sangwook Kim","ids":["1756753"]},{"name":"Hwanju Kim","ids":["2968918"]},{"name":"Joonwon Lee","ids":["6064655"]},{"name":"Jinkyu Jeong","ids":["1782453"]}],"journalName":"","paperAbstract":"In data-intensive applications, such as databases and keyvalue stores, reducing the request handling latency is important for providing better data services. In such applications, I/O-intensive background tasks, such as checkpointing, are the major culprit in worsening the latency due to the contention in shared I/O stack and storage. To minimize the contention, properly prioritizing I/Os is crucial but the effectiveness of existing approaches is limited for two reasons. First, statically deciding the priority of an I/O is insufficient since high-priority tasks can wait for low-priority I/Os due to I/O priority inversion. Second, multiple independent layers in modern storage stacks are not holistically considered by existing approacheswhich thereby fail to effectively prioritize I/Os throughout the I/O path. In this paper, we propose a request-centric I/O prioritization that dynamically detects and prioritizes I/Os delaying request handling at all layers in the I/O path. The proposed scheme is implemented on Linux and is evaluated with three applications, PostgreSQL, MongoDB, and Redis. The evaluation results show that our scheme achieves up to 53% better request throughput and 42× better 99 percentile request latency (84 ms vs. 3581 ms), compared to the default configuration in Linux.","inCitations":["a94bee6b9f3c9dc19465ac4c6c503c0c17ce846b","3c320a4d53946087ba6f29f109c17bdf270efff9","d1ab2cd6a008fa9fde2311f26dec32b9cfbf0aaf"],"pdfUrls":["http://www.usenix.org./sites/default/files/conference/protected-files/fast17_slides_kim_0.pdf","https://www.usenix.org/conference/fast17/technical-sessions/presentation/kim-sangwook","http://www.usenix.org./system/files/conference/fast17/fast17-kim-sangwook.pdf","https://www.usenix.org/sites/default/files/conference/protected-files/fast17_slides_kim_0.pdf","https://www.usenix.org/system/files/conference/fast17/fast17-kim-sangwook.pdf"],"title":"Enlightening the I/O Path: A Holistic Approach for Application Performance","doi":"","sources":["DBLP"],"doiUrl":"","venue":"FAST"},
{"entities":["Computer data storage","Constraint satisfaction","Control plane","Data center","Scalability"],"journalVolume":"","journalPages":"213-228","pmid":"","year":2017,"outCitations":["a205ec6ef5dba0ab862cb4d127737104aae5a476","67210b23a16f222f83d680a98ead15a3d953a434","45ffb71b454dcbd6a77537d81926eb3eca1c49dc","4af63ed343df388b6353b6fc77c7137d27822bf4","23a9e1f8cefc76b71f0cf5e1ccf5a6485c19cadf","282c6a3b573051e3e799d73cfc623ccbd68bcd6a","3358850706a8ad2eb8489bb7790e8bbd3a5b6dba","2607daeaf7bc30c7ba532471c628e989797d0384","5f3f9223c5c9f896be099bc177929febad508407","82c0d142c975e1cf224af674129d69647ef4d892","1c5de2067774468c623438106d4697b80ac043b8","7447b123eaef1d84a0f94b485f039534ad98015c","3d019723a6f8678b6adc901e8eae2076263d9089","1ccf3f5ad209fd736847fb2aa3252920d59efd88","514a5c15e8cf3f681febecad954a4508d9189c99","1f1269db397595f5b5a08eb2e65022e9a8759648","2f6af58c7905fb8367652fe62fbb1f6ec7e28be0","8b1d8d46836a6d5eb4355315b64d85c128cbff27","638c917d981915bc7a00bb0941cdd38111df51de","3ede477c16e5df0964eac62b3d33311514a66410","4452a2f4f4fd1df19777f4b0ff482403f7b5091e","7a2274412948765bf872b765dafd8139e51000ff","1430c094d6fb90d87f38c36a92cdbcf66b87f60a","41f07dfa0045c89aac19f97d7a471bdd514b8998","3fc93257ac94aa8d6505c19077058e68622345b6","70f3161f62a4a43580eb47d2157e98e880738594","66e7577cd919979c642319632f9435217b016d87","4b1ed250f7c53808d1910b6f040b47e178eb2460","0a4110fda21f0de29824ead1df591d2c5e1da8d0","120f401bf43b93d03a37c8c1de332e8b1664c5d9","130d640b53a1d6700b67a4ea4256071ae18e0ee8","6bd528b41b3f56a5ec85d2118bc5c4a00ad9944e","3bcc8272a88796ae5d86a3dded89f66034c1cef7","58429cae7ff648359ab83566d17ab3d7dd6ae3fd","0368d2445d3ee4205ee73da933cb8b810a89091c","497d61a0b8a4b03c57fe1c1ba45118fdfd3dace4","2da760f90c3d2bf6598becdde9063093f488548c","0da64b4241476515abcd6995baf2252c313c18ee","6a1df9dae902f3d377f9c85ba9732b8d2270bf2b","01e9ddf1062f9a7d7847bb9bcd2371ce6e0d3e29","828a6aa6984e292305aad964b9d167f0dd7dc513","2a2a03c864e53862a84c4555f218c2c92e039a5b","87b94c2f86b9e8838bf15276fcfe9be0fd293588","3c5cc0f17dc2f956dbc278f24433f57affe49dce","478f51822252e4221c920bbf9d30a0b0491045ec","0a587144ad09fb0a784515c89ddfb6b90e8c057e","0b9e161974df2e5563090d2c5b623de57f2c744a","1dc5c9675b1f9662deac7a9d5f4b38cd13f76dba"],"s2Url":"https://semanticscholar.org/paper/1bc029b715f29e95063f27dc36396093394a1e19","s2PdfUrl":"http://pdfs.semanticscholar.org/1bc0/29b715f29e95063f27dc36396093394a1e19.pdf","id":"1bc029b715f29e95063f27dc36396093394a1e19","authors":[{"name":"Jake Wires","ids":["3048886"]},{"name":"Andrew Warfield","ids":["1709411"]}],"journalName":"","paperAbstract":"This paper describes Mirador, a dynamic placement service implemented as part of an enterprise scale-out storage product. Mirador is able to encode multidimensional placement goals relating to the performance, failure response, and workload adaptation of the storage system. Using approaches from dynamic constraint satisfaction, Mirador migrates both data and client network connections in order to continuously adapt and improve the configuration of the storage system.","inCitations":["226ca798b529c13605a2aa7fe75d58f4188f850a"],"pdfUrls":["http://www.usenix.org./system/files/conference/fast17/fast17-wires.pdf","https://www.usenix.org/sites/default/files/conference/protected-files/fast17_slides_wires.pdf","https://www.usenix.org/system/files/conference/fast17/fast17-wires.pdf","http://www.usenix.org./sites/default/files/conference/protected-files/fast17_slides_wires.pdf","https://www.usenix.org/conference/fast17/technical-sessions/presentation/wires"],"title":"Mirador: An Active Control Plane for Datacenter Storage","doi":"","sources":["DBLP"],"doiUrl":"","venue":"FAST"},
{"entities":["Checksum","Copy-on-write","Data integrity","Digest access authentication","In-memory database","Production system (computer science)","Software bug"],"journalVolume":"","journalPages":"197-212","pmid":"","year":2017,"outCitations":["39f2a24878abb8699da6c5ab3b436ffef0c4fe6c","20a108587321823ca9cdd93ac84fc316a0400630","09c0d62190aedb53e820695ccbe98d90f877cc46","2335e1c1fcc1d430ed3049557a974a9cc9842e2c","4644983f2bc04241b6d5a43127e7d1609bfb24fb","3533159037bc2c11bde6b314e040ee113ae52bdd","d0b6d2075a653d60452b6df0fced4ee0ae093dd2","2c84daae142c5b0f4ca6a6772ca7e8cac7d7afca","7062268b78dff4a8819fe3f1e89c6b5344f715a5","6afa24070025c624e94b41e05168fd4223807cab","c9ef82a4ad0b1b33296cea86fb2ec7558cf798fb","33cb4013c7cc36a173e7fb4e541133056e8e43cf","ca16c4ad100e54d2458bf22f8a22c8c6f15e8d20","37ed4f9684e774157f38655768b996b6b875e80a","8a7536f311d22bd588c5bc2306d54d13effaee82","088e3e939ad234b6fdd0e321290fb26937dc2553","4cb7f6fd48468da2f985a44f021fa5b49eb7a6ce","517c5cd1dbafb3cfa0eea4fc78d0b5cd085209b2","5c5dd0ae1d8035eadbf2fd411663dd062a922941","8d8f082bf15960191b74377f5dbb14a4a9bfb62d","21318da2ea08c1f7b8c77701f67483882950df96","108c840d5d1847948a2de0250490a327ae069ee6","44d886f89cdbd4fdf5dd25d83b2d37deb7541bf7","26f820aa9e782f5d6ba8bcb272a31c32094dfd59","3abf71e837cb7b1e9fe7e54192d986142d87b1a2","d9706ba11e5af14d92a1f673f412f0765c082df9","11b8ef5da9c8df214859bb41b60001a0abd2b5b2","32d23ce43877aa8cd385a8e01f366329dd015a5c","04aae75ab8a040225024b6a96ab7cbb28ef74d0a","0ba03292a9b7cf0a0a5f2f76da3ae1309929d062","0ba16e5cd9c81282386362c8db0adcd4a203741f","ae705ea9428baedc1a2de4539a75f6aed444c096","318c86751f018b5d7415dafc58e20c0ce06c68b6","0e55379d27454c5d9d72e4ba4b3752007b9f886f","c769840e996e09f910b9c3ef64eb2686a89c40d3","3deb7adb74003800813b9e96ba799a1d86e6a47a","0f6a32792d0882db35fe9391445d4322232b619e","0f7d62354586d074fe6120700b4fc7597d877b57","4108e4635351d6f2d0916ee19d0a0ef878649c3c"],"s2Url":"https://semanticscholar.org/paper/556f01b6764f866d7bd4a2d955115ca72bd3413f","s2PdfUrl":"http://pdfs.semanticscholar.org/d7df/16b3ee5d58e856982607e38de47d1462b544.pdf","id":"556f01b6764f866d7bd4a2d955115ca72bd3413f","authors":[{"name":"Harendra Kumar","ids":["3182868"]},{"name":"Yuvraj Patel","ids":["40604671"]},{"name":"Ram Kesavan","ids":["2042885"]},{"name":"Sumith Makam","ids":["35153920"]}],"journalName":"","paperAbstract":"We introduce a low-cost incremental checksum technique that protects metadata blocks against in-memory scribbles, and a lightweight digest-based transaction auditing mechanism that enforces file system consistency invariants. Compared with previous work, our techniques reduce performance overhead by an order of magnitude. They also help distinguish scribbles from logic bugs. We also present a mechanism to pinpoint the cause of scribbles on production systems. Our techniques have been productized in the NetApp® WAFL® (Write Anywhere File Layout) file system with negligible performance overhead, greatly reducing corruption-related incidents over the past five years, based on millions of runtime hours.","inCitations":["8ee82c0bd80e86c55b56414a602d53164d4fb5c0","0f4386d4a521e36cb15252b4e908a948a65252ef","20a108587321823ca9cdd93ac84fc316a0400630","ad897b9261a39cdae6e8b0fdcd755e6001e004bc","ad42b4773cd461ba58bda07e1b7b0ff24c4ddba4","67ffec9c10d9594eb9af4afe25b1f0b0bce5f85d"],"pdfUrls":["https://www.usenix.org/sites/default/files/conference/protected-files/fast17_slides_patel.pdf","https://www.usenix.org/conference/fast17/technical-sessions/presentation/kumar","https://www.usenix.org/system/files/conference/fast17/fast17-kumar.pdf","http://www.usenix.org./sites/default/files/conference/protected-files/fast17_slides_patel.pdf","http://www.usenix.org./system/files/conference/fast17/fast17-kumar.pdf"],"title":"High Performance Metadata Integrity Protection in the WAFL Copy-on-Write File System","doi":"","sources":["DBLP"],"doiUrl":"","venue":"FAST"},
{"entities":["Big data","Code word","Data center","Erasure code","Experiment","Hard disk drive","Magnetic storage","Moore's law","Non-RAID drive architectures","Reed–Solomon error correction","Retry"],"journalVolume":"","journalPages":"135-148","pmid":"","year":2017,"outCitations":["bcdd54c3faa4d0fdebebba7b0815341ed45abd90","d2ab8a8fc0f4f06c0b89ef1d05314fb882ded44c","0e1a80517cb5ddba06111ec20a0da937bf105e5a","af0a16a96fae5407d7e83ebf33d96b1523c828a5","3f4297d2e5ae196f30f9a18dcd8400dfca5d2e47","a4c529cfc2cd2ff5f3f1f018ea82fb2e22630695","3b547d706d33c110f96bf1c0e805ab8cc82afdbf","0d77bb6ef2bb6d165f58bf0251bf3d7cf29f1491","6a180e2e4a171c39577afe90b9629bae3b9294ed","841489d2b678ca60b552b0610d06eaf43f54ac15","2976de31d1b21978e3ba8b723250553fe5d1d0c5","133eacaf0ad25b8364cb4510007d9363298e8adf","173f31b150fb7e000a4f0e9d5a880e54d94b4b21","114308433282441cbe153e39ff10ce4cf5fbc0ef","0b5c8ed0dac9bf073568870c86adfd5d0775c0bf","8a68400ed270a83c5239f0e90319b69c604354ab","09bed5a75cbdba4b930cdca6bd2499d61121e030","0831a5baf38c9b3d43c755319a602b15fc01c52d","42512431ca7fffdbc80eb7280d093efcead3d48d","c2f4ccc7feb6bd3928d14f2352d156d391eb0111","1f15211337ecfb76b9bcba5f3ab844351d0b063e","58b628792d3eb22a034a871ed3cf373afe591928","75032b6df2f3988d2cb6988c73cbc01c7b9e80cb","3b50c635cd65e286ac1e32c0a26842d7d84d20b6","4b14acc92dee6c04165cfa8a13a56dbd379bc2ec","01b1c99cb2e179a26d7237b835b085945ef28a00"],"s2Url":"https://semanticscholar.org/paper/3b07df59e873be87142a49c229456569ece15475","s2PdfUrl":"http://pdfs.semanticscholar.org/3b07/df59e873be87142a49c229456569ece15475.pdf","id":"3b07df59e873be87142a49c229456569ece15475","authors":[{"name":"Yin Li","ids":["1738814"]},{"name":"Hao Wang","ids":["39049654"]},{"name":"Xuebin Zhang","ids":["3133825"]},{"name":"Ning Zheng","ids":["2407199"]},{"name":"Shafa Dahandeh","ids":["1891136"]},{"name":"Tong Zhang","ids":["32180339"]}],"journalName":"","paperAbstract":"This paper presents a simple yet effective design solution to facilitate technology scaling for hard disk drives (HDDs) being deployed in data centers. Emerging magnetic recording technologies improve storage areal density mainly through reducing the track pitch, which however makes HDDs subject to higher read retry rates. More frequent HDD read retries could cause intolerable tail latency for large-scale systems such as data centers. To reduce the occurrence of costly read retry, one intuitive solution is to apply erasure coding locally on each HDD or JBOD (just a bunch of disks). To be practically viable, local erasure coding must have very low coding redundancy, which demands very long codeword length (e.g., one codeword spans hundreds of 4kB sectors) and hence large file size. This makes local erasure coding mainly suitable for data center applications. This paper contends that local erasure coding should be implemented transparently within filesystems, and accordingly presents a basic design framework and elaborates on important design issues. Meanwhile, this paper derives the mathematical formulations for estimating its effect on reducing HDD read tail latency. Using Reed-Solomon (RS) based erasure codes as test vehicles, we carried out detailed analysis and experiments to evaluate its implementation feasibility and effectiveness. We integrated the developed design solution into ext4 to further demonstrate its feasibility and quantitatively measure its impact on average speed performance of various big data benchmarks.","inCitations":["ec3924af8c1cb428b4f1309b9a9ca3c86abd6631"],"pdfUrls":["https://www.usenix.org/system/files/conference/fast17/fast17-li.pdf","http://www.usenix.org./system/files/conference/fast17/fast17-li.pdf","https://www.usenix.org/sites/default/files/conference/protected-files/fast17_slides_li.pdf","https://www.usenix.org/conference/fast17/technical-sessions/presentation/li","http://www.usenix.org./sites/default/files/conference/protected-files/fast17_slides_li.pdf"],"title":"Facilitating Magnetic Recording Technology Scaling for Data Center Hard Disk Drives through Filesystem-Level Transparent Local Erasure Coding","doi":"","sources":["DBLP"],"doiUrl":"","venue":"FAST"},
{"entities":["Anomaly detection","Chunking (computing)","Data model","Data retrieval","Database","Distributed computing","Generic data model","Instability","Memory footprint","Software system","Storage efficiency","Time series"],"journalVolume":"","journalPages":"229-242","pmid":"","year":2017,"outCitations":["0479b7e8c433e3f18a2b6c5dedd328f0229c1566","5f3f9223c5c9f896be099bc177929febad508407","5b042a76c6e61d411f68b8193ec67ad8dd1abc5e","a11b243c571ade72c1be5bbb4105b00388174bd6","6632e05bf8efe9498f622c7af82b4ac0ac1db23d","6d57e29ddb68c91256e59d82e8afe321152aa357","31b963f48ba38f1f9c5cc240f43331b07229861e","05a20cde15e172fc82f32774dd0cf4fe5827cad2","47fcd425e6e2a2c8ca059acf5c151a9da115c14c","8e0ff4b8bbeac8f301e00494a39bd1b4a199fba1","acbf00fc73320c3e4054556013e7a3fcae7f0675","03b84b789cb342587db621c7e88eeb005cc21578","1ab98540293251d02cf2b1db202d3ad9e4304a78","72e5e8d22b6e278673f0d9c912c06f666ac01d28","8fe636695d09b7c05369da9cb338c99006213c61","c66689fafa0ce5d6d85ac8b361068de31c623516","689c64a76a8bfe1cdf7cb8df31f523980555ef82","94ff90079b121da750b9257423fc9b4b6fc6ebbd","18869d8964793da4837b5b38d4aec5854d37f08c","2ef606258486d6c32fd0b9ca54244273c21331b9"],"s2Url":"https://semanticscholar.org/paper/4f53b1fbf0b21c75c11dc77c98a2ec08815227a0","s2PdfUrl":"http://pdfs.semanticscholar.org/4f53/b1fbf0b21c75c11dc77c98a2ec08815227a0.pdf","id":"4f53b1fbf0b21c75c11dc77c98a2ec08815227a0","authors":[{"name":"Florian Lautenschlager","ids":["2784868"]},{"name":"Michael Philippsen","ids":["1703535"]},{"name":"Andreas Kumlehn","ids":["2490293"]},{"name":"Josef Adersberger","ids":["1771962"]}],"journalName":"","paperAbstract":"Anomalies in the runtime behavior of software systems, especially in distributed systems, are inevitable, expensive, and hard to locate. To detect and correct such anomalies (like instability due to a growing memory consumption, failure due to load spikes, etc.) one has to automatically collect, store, and analyze the operational data of the runtime behavior, often represented as time series. There are efficient means both to collect and analyze the runtime behavior. But traditional time series databases do not yet focus on the specific needs of anomaly detection (generic data model, specific built-in functions, storage efficiency, and fast query execution). The paper presents Chronix, a domain specific time series database targeted at anomaly detection in operational data. Chronix uses an ideal compression and chunking of the time series data, a methodology for commissioning Chronix’ parameters to a sweet spot, a way of enhancing the data with attributes, an expandable set of analysis functions, and other techniques to achieve both faster query times and a significantly smaller memory footprint. On benchmarks Chronix saves 20%–68% of the space that other time series databases need to store the data and saves 80%–92% of the data retrieval time and 73%–97% of the runtime of analyzing functions.","inCitations":["27f503611a6020a2c6b196042ec63be0c79306ba"],"pdfUrls":["https://www.usenix.org/system/files/conference/fast17/fast17-lautenschlager.pdf","https://www.usenix.org/sites/default/files/conference/protected-files/fast17_slides_lautenschlager.pdf","http://www.usenix.org./sites/default/files/conference/protected-files/fast17_slides_lautenschlager.pdf","https://www.usenix.org/conference/fast17/technical-sessions/presentation/lautenschlager","http://www.usenix.org./system/files/conference/fast17/fast17-lautenschlager.pdf"],"title":"Chronix: Long Term Storage and Retrieval Technology for Anomaly Detection in Operational Data","doi":"","sources":["DBLP"],"doiUrl":"","venue":"FAST"},
{"entities":["Cold boot attack","DriveSpace","Experiment","Garbage collection (computer science)","Hard disk drive","In-place algorithm","Magnetic storage","Magnetic tape data storage","Overhead (computing)","SMART","Shingled magnetic recording","Simulation","Space–time tradeoff"],"journalVolume":"","journalPages":"121-134","pmid":"","year":2017,"outCitations":["3de54cb50512fffe9ab48f69b92dbf6a43bd2d1b","501f491dd60ea26bcb8152bfd3f9ac2456e69da8","8c06fb59a79b3b47dff8588302d8e6514a7f7a4a","31ceeced5d23193c369b98170c45e66bae6ff77d","4956257ba37029ffdaadf3bdcca9b89bb5eea561","238c00c28e016b9341a4cd8c086aa76018db9133","2018f3fc13cd38122abdf37bf939b5011cd2e3c9","8dee6c0a8438a995b1d2452b84c7544be5f00578","0edf5e6b5caab5f62f8e71293c58b29e7b8bb6e1","9d5e9f98f85629d9dae20d181ff2c9fcdcdb5520","ba356329a7c6672eca15815ed622dac2c71b4513","0bf50c9aff7d5182504dd18b7cc0f6041b5e520b"],"s2Url":"https://semanticscholar.org/paper/3fc68bf55557ac4b377bd97bed8b28f3e201d775","s2PdfUrl":"http://pdfs.semanticscholar.org/3fc6/8bf55557ac4b377bd97bed8b28f3e201d775.pdf","id":"3fc68bf55557ac4b377bd97bed8b28f3e201d775","authors":[{"name":"Weiping He","ids":["23473919"]},{"name":"David Hung-Chang Du","ids":["1717128"]}],"journalName":"","paperAbstract":"Shingled Magnetic Recording (SMR) is a new technique for increasing areal data density in hard drives. Drivemanaged SMR (DM-SMR) drives employ a shingled translation layer to mask internal data management and support block interface to the host software. Two major challenges of designing an efficient shingled translation layer for DM-SMR drives are metadata overhead and garbage collection overhead. In this paper we introduce SMaRT, an approach to Shingled Magnetic Recording Translation which adapts its data management scheme as the drive utilization changes. SMaRT uses a hybrid update strategy which performs in-place update for the qualified tracks and outof-place updates for the unqualified tracks. Background Garbage Collection (GC) operations and on-demand GC operations are used when the free space becomes too fragmented. SMaRT also has a specially crafted space allocation and track migration scheme that supports automatic cold data progression to minimize GC overhead in the long term. We implement SMaRT and compare it with a regular Hard Disk Drive (HDD) and a simulated Seagate DM-SMR drive. The experiments with several block I/O traces demonstrate that SMaRT performs better than the Seagate drive and even provides comparable performance as regular HDDs when drive space usage is below a certain threshold.","inCitations":["ec3924af8c1cb428b4f1309b9a9ca3c86abd6631","537d37be13687758d01e35fc6a62be118ec48ea1","2fe51b5c34484b5fb8f0ec54483750ffc842fd4a","218ad6a5c79d2bc61c513e8b65b7504ad6a2187b","8e4cdaa006bce928ed7a6d37b9bfbfdffe2a6367","7626ba5ea8754f99699509784251e49d1e700d86"],"pdfUrls":["https://www.usenix.org/conference/fast17/technical-sessions/presentation/he","http://www.usenix.org./sites/default/files/conference/protected-files/fast17_slides_he.pdf","http://www.usenix.org./system/files/conference/fast17/fast17-he.pdf","https://www.usenix.org/system/files/conference/fast17/fast17-he.pdf","https://www.usenix.org/sites/default/files/conference/protected-files/fast17_slides_he.pdf","http://www-users.cselabs.umn.edu/classes/Spring-2017/csci5980/files/SMR/SMaRT.pdf"],"title":"SMaRT: An Approach to Shingled Magnetic Recording Translation","doi":"","sources":["DBLP"],"doiUrl":"","venue":"FAST"},
{"entities":["Dictionary","Experiment","Heuristic","Locality of reference","Synthetic data","ZFS"],"journalVolume":"","journalPages":"45-58","pmid":"","year":2017,"outCitations":["0af5cc5352264a890a22b6e910c183b64679dc0e","2538812c0f7922329f2c86c6c5b7190313976fb3","5bb770af1973f929e8622f17ddf378d439245144","1547503c340ee99713cad4516d059e01d349707e","3e8e43f61b3af63c6a8bb981b5d085c8afb1b9e2","12a0046a1197ae63c3d616c74e367dc583cef196","b8ddec47f9fab1eddb5c9cacf703781dd5337b87","1772fdf329f526d4f6c9e62b99bc65eac0ff31b5","de8f972df6b7bfd32692db268ec54bb031b1ef3c","3b2af12a43d06338dd62681328c75a1999fc87fd","14f03cc21d8eb6a4b6498b46e9780d60784356ee","012ab4527d6aee2387c243d304c624f3b9cf03f3","1d6ded112a3462c748d45f7ad11f594c79e6fc59","b1ec820da48f69a4652ddf08f00e2e991126cf4b","a04678ad8398a2579c249fff4b59bfbcfdd7e25b","3492873a8bc6d1d501dcac97e891c43dfecc29c0","d0b6d2075a653d60452b6df0fced4ee0ae093dd2","5ecd441bf4a54c7500cd6fc185d8dd09638e12cb"],"s2Url":"https://semanticscholar.org/paper/b7437111bf04a803878ebacbc275ba3715bccb18","s2PdfUrl":"http://pdfs.semanticscholar.org/d6f8/84639f7599d0e772a54da7cdf986a4302fa2.pdf","id":"b7437111bf04a803878ebacbc275ba3715bccb18","authors":[{"name":"Alexander Conway","ids":["38709123"]},{"name":"Ainesh Bakshi","ids":["35300553"]},{"name":"Yizheng Jiao","ids":["2889496"]},{"name":"William Jannen","ids":["3354088"]},{"name":"Yang Zhan","ids":["31800663"]},{"name":"Jun Yuan","ids":["1746449"]},{"name":"Michael A. Bender","ids":["33877556"]},{"name":"Rob Johnson","ids":["2387399"]},{"name":"Bradley C. Kuszmaul","ids":["1871661"]},{"name":"Donald E. Porter","ids":["1755646"]},{"name":"Martin Farach-Colton","ids":["1680147"]}],"journalName":"","paperAbstract":"File systems must allocate space for files without knowing what will be added or removed in the future. Over the life of a file system, this may cause suboptimal file placement decisions which eventually lead to slower performance, or aging. Traditional file systems employ heuristics, such as collocating related files and data blocks, to avoid aging, and many file system implementors treat aging as a solved problem. However, this paper describes realistic as well as synthetic workloads that can cause these heuristics to fail, inducing large performance declines due to aging. For example, on ext4 and ZFS, a few hundred git pull operations can reduce read performance by a factor of 2; performing a thousand pulls can reduce performance by up to a factor of 30. We further present microbenchmarks demonstrating that common placement strategies are extremely sensitive to file-creation order; varying the creation order of a few thousand small files in a real-world directory structure can slow down reads by 15− 175×, depending on the file system. We argue that these slowdowns are caused by poor layout. We demonstrate a correlation between read performance of a directory scan and the locality within a file system’s access patterns, using a dynamic layout score. In short, many file systems are exquisitely prone to read aging for a variety of write workloads. We show, however, that aging is not inevitable. BetrFS, a file system based on write-optimized dictionaries, exhibits almost no aging in our experiments. BetrFS typically outperforms the other file systems in our benchmarks; aged BetrFS even outperforms the unaged versions of these file systems, excepting Btrfs. We present a framework for understanding and predicting aging, and identify the key features of BetrFS that avoid aging.","inCitations":["27a36203f14d73b95dfffec857b4ff923d9ef430","8cfa25e85c2c6c9305f696819d764ed5490f3faf","1ee52dc91368b925a15bc2448d9e1ea4a1643dc4","3e8f2997d81682e3bfbfdc2ea35be9ce6838a057","75c3f38c4268097b45212b8c67b028f6bf4ecc2d","8ec17bf5cf4a4acd88455e05fc2f17bc5b29e78f"],"pdfUrls":["http://aineshbakshi.com/pubs/Filesystems.pdf","http://www.usenix.org./system/files/conference/fast17/fast17-conway.pdf","https://www.usenix.org/sites/default/files/conference/protected-files/fast17_slides_conway.pdf","http://www.usenix.org./sites/default/files/conference/protected-files/fast17_slides_conway.pdf","https://www.usenix.org/system/files/conference/fast17/fast17-conway.pdf","https://www.usenix.org/conference/fast17/technical-sessions/presentation/conway"],"title":"File Systems Fated for Senescence? Nonsense, Says Science!","doi":"","sources":["DBLP"],"doiUrl":"","venue":"FAST"},
{"entities":["Attribute–value pair","Cache (computing)","Experiment","Flash memory","Open-channel SSD","Solid-state drive","Systems design","Throughput"],"journalVolume":"","journalPages":"391-405","pmid":"","year":2017,"outCitations":["ff64eac2cf7d5e58c02fefa398ba1a1b9670f09f","02544c9b385813aade4512532cd357e294a74eb4","b0b2f180faa09e7bfcb6bb8e57288c3b61f11116","861ead96ed080f88df28473b16f1fcd98d735445","088e3e939ad234b6fdd0e321290fb26937dc2553","274e495824827f5a9dc1ba3ab62620445e6b3d4b","05961fc1d02ca30653dd0b4c906113db796df941","0bba65fd5ac1db9a3293e9ebcfba092cf4ae58ee","c1c4cbcf12c283d9b88bfa7de6ab5a1d02f3f7f6","199ac28b6bc68bf05c77645ffae7640df114bca5","8cded4cc565f8b7c41b40de6fe8d20231a7e8652","7019d566d10fcdb836aa338c344de4f0ed2131b6","3b2af12a43d06338dd62681328c75a1999fc87fd","1693e83e47a99667f4bd6ad6e24d8b62a1ba22c8","05948e66aeefea1c969fdce16edb94ae94fb651e","28f13ebe8e17fdb4c2500c515759a3ee0c2783ce","048a09d7c8713dc2533c1e31ac3f224868293461","d67adb456a315aee244babf4f20e318cc14d13f3","0e216e95f17f64ff18cd50463dd8ec023aa08248","0720cfa5330462593b20ea0bbb7d8b5862a6b730","34bbe13996b8cd0ea21cb1fe125fe79979587049","70ce10f47aafa0994627a9575565b5c98af58d98","098d792d1783b5f6fc098203f71f21f5d053c653","13d6c568c770ff5a070072e720fb34b0037cdab8","20a44558eed182a971f7add68ecc5931fbca2a65","40f04909aaa24b09569863aa71e76fe3d284cdb0","9efc40b9a71a128c073fe09bb77e0e97f08514d8","b4087345c63a7b2412eeb31066b5e4bceadbbcb2","ec5bcf2186fa0b4b7399f7233f9ade80966e6fe5","0903d6b3b5a26fea2cb7b4956f66365d71c78549","0a5882fc7600383eb9d6cc119942f48a70f896ad","4cda001811dea15a35894cd1b657003bb7f3c6de","248a93ed3be23972343d18bc27cf4a2b43781972","627b93073977b7b7c5ae0cf610f41ee0ed27669c","726099036bb32c3fbaf1650d5900eeaa2ecc8fd9","1820a34042d6371a9e20484b0c63b698eb522a6c","3cb34f7a770836bcfeef28f844d670b8a014ffa8","389b618c42c0d5d32a569b9cbaa02a7ff77c6be6","73e85836599b5ab4f83afa2ae10fea99cb5d29d7","61977858b3eea4f5a6d81393301e7298ade7a2d8","68a047707ff765af006fcb481feb3d6eaa4625b1","0e5c646909bb762da0cd325e084655c12445578f","0b6adc0dbc55076dc9c9a8931f4a4df58fd291b6"],"s2Url":"https://semanticscholar.org/paper/35bb4201683cf3525bfab90c35ca1a6ab72f3e60","s2PdfUrl":"http://pdfs.semanticscholar.org/35bb/4201683cf3525bfab90c35ca1a6ab72f3e60.pdf","id":"35bb4201683cf3525bfab90c35ca1a6ab72f3e60","authors":[{"name":"Zhaoyan Shen","ids":["1851223"]},{"name":"Feng Chen","ids":["1692998"]},{"name":"Yichen Jia","ids":["2246860"]},{"name":"Zili Shao","ids":["1714148"]}],"journalName":"","paperAbstract":"In recent years, flash-based key-value cache systems have raised high interest in industry, such as Facebook’s McDipper and Twitter’s Fatcache. These cache systems typically use commercial SSDs to store and manage key-value cache data in flash. Such a practice, though simple, is inefficient due to the huge semantic gap between the key-value cache manager and the underlying flash devices. In this paper, we advocate to reconsider the cache system design and directly open device-level details of the underlying flash storage for key-value caching. This co-design approach bridges the semantic gap and well connects the two layers together, which allows us to leverage both the domain knowledge of key-value caches and the unique device properties. In this way, we can maximize the efficiency of key-value caching on flash devices while minimizing its weakness. We implemented a prototype, called DIDACache, based on the Open-Channel SSD platform. Our experiments on real hardware show that we can significantly increase the throughput by 35.5%, reduce the latency by 23.6%, and remove unnecessary erase operations by 28%.","inCitations":["c49feb5f91c8ba846eb2e90edf1b01c62a25c8d5","c9e997cccec19141972a64fafcbb55c1f007c370","226ca798b529c13605a2aa7fe75d58f4188f850a","1d08d231ec66645ec56d2210c1a7c6b44c6ff041"],"pdfUrls":["http://www.usenix.org./system/files/conference/fast17/fast17-shen.pdf","https://www.usenix.org/system/files/conference/fast17/fast17-shen.pdf","https://www.usenix.org/conference/fast17/technical-sessions/presentation/shen","http://www.csc.lsu.edu/~fchen/publications/papers/fast17-didacache.pdf"],"title":"DIDACache: A Deep Integration of Device and Application for Flash Based Key-Value Caching","doi":"","sources":["DBLP"],"doiUrl":"","venue":"FAST"},
{"entities":["Asynchronous I/O","Bitmap","Graph (abstract data type)","Graphene","In-memory database","In-memory processing","Programming model","Scalability"],"journalVolume":"","journalPages":"285-300","pmid":"","year":2017,"outCitations":["2209304ccc2b0501debd5e9a90ae739f9a30cdef","caacd536fa218ef5218021506ebc041e3f460064","2b9e6181502369199bd89691a27f89bdbaac36e4","0a791a760dd883342c8b8456a3e7cb75fb996ef4","0eff3eb68ae892012f0d478444f8bb6f50361be5","1156f60e40548096df49528b1342bb3e88b0f378","027485f716ca4f6d9ee2e189790d6560e37fcab2","2a30b4cb56853002133311372ce8313b14fba158","8e67d1085da29e5aa1e758751bfa5469ac07023e","1e27b9b447cebd5047050e39bb9246fa6364b760","0ea110472ee018a8034898588c9bdede1e0c8df8","6536b5743e53c00bb1600f954959ae00dc24da98","2b0cc03aa4625a09958c20dc721f4e0a52c13fd0","11b93c5f176431efa72208d8b4c4c88d46261695","6de3915df2b9927a78f213629f3bcb052ec21e8b","0a8e0a9f0ae9910d5ecf165071559a2c4191a098","4dc578364f357b993b5554b9181c90c84aa6b4d1","eb82d3035849cd23578096462ba419b53198a556","e1e066a860978918808db9ba2bc6a2dff63a1455","3486aeaf540c48952120fe853d672af984f40a6a","175d795f44037ef60dd9df341701cd5fdc449f1f","131ec93c0751b6cdeeff4a5d62a7e4810d06f0de","ef9d9821df55442f039b128bb5cef2b41ab2cadc","0f34ea8535dc5833a1a3692ffc7abc6740d2406a","98921bac0c208cd0d2fcd51101902f51ab88416f","0ee5abec0c7002c759d70e4d75921b65a6d8666a","de5bd35339e5692002a77145d8b861940429ad77","e2462bde978023a9069cc08326f626135a95cb89","41880f9408bf4d826e4a715ee783e2d9d8666c2f","31ffb232b5c1186bb90502254162ac3d99baf50b","48930aa2539b12d60352283dd4f91c845cf9b69c","080f44d89bf6f4404f476ffec8d2f8ad3f60e07d","0608d9937c074520cdc93cc444cc1c77039c5332","3baecc04e1341cbae7999e8f61a3946c76504828","5e49e7a0a6c6d46a368a4c036bc9e89a0ac4edd5","3d4145d8b555d27a78fdea734fe712121dc86526","3d985a05e4a49be71d497e7a2ff3fcbeb74c4bc8","8d1a4ae1d3a17edd5b16653f2f582f9952b71612","259e93de2f10d395a1bdfb2dc6da72b6a3998572","6f7cd29a3dfdcb2f6880a022e13054542020c5ce","225ca2f92481b253310686a7b6c40032bde507ea","0e3253e5ca318e70d4968a45f8d41f88dbffd9e3","0ad8e89091eed09217e66adc98136126addc2619","2ae3ac3f7463f838c38e6ca250ca294e813529f2","141004dee9e799b40bfaf50b4a72618613137250","6a888f3dd0a17b0241be61daa378ba6caffa6617","191fd33f17c2a79b3825d4cc2105c47a8f16ba44","7ebb9fad71ce8e08d5284b7644a5452cff6c75b3","24e8be45a2b2a30a01b7e9f1502e7bd6a7870e7a","282bc59faefb734137d2ea978cb1eb5699e67c7c"],"s2Url":"https://semanticscholar.org/paper/4983002656dc35c6a1d0ce39eb56c70d3e55b7fd","s2PdfUrl":"http://pdfs.semanticscholar.org/4983/002656dc35c6a1d0ce39eb56c70d3e55b7fd.pdf","id":"4983002656dc35c6a1d0ce39eb56c70d3e55b7fd","authors":[{"name":"Hang Liu","ids":["26729512"]},{"name":"H. Howie Huang","ids":["1744674"]}],"journalName":"","paperAbstract":"As graphs continue to grow, external memory graph processing systems serve as a promising alternative to inmemory solutions for low cost and high scalability. Unfortunately, not only does this approach require considerable efforts in programming and IO management, but its performance also lags behind, in some cases by an order of magnitude. In this work, we strive to achieve an ambitious goal of achieving ease of programming and high IO performance (as in-memory processing) while maintaining graph data on disks (as external memory processing). To this end, we have designed and developed Graphene that consists of four new techniques: an IO request centric programming model, bitmap based asynchronous IO, direct hugepage support, and data and workload balancing. The evaluation shows that Graphene can not only run several times faster than several external-memory processing systems, but also performs comparably with in-memory processing on large graphs.","inCitations":["c7e2c4bea500ea7926a50973d861f01bb8e5e364","a61e5aab233900f6febae67471e198eb3bb89e3a","a038202963e55feb5f7a41ed3ec6d7073beec6b9","356100b33d589bb48fa1a6518a85efb551a13d9b","5a4655bb21fed59e0a1eaa6eb7d31c00be1c3f84","ab77fa0fcdb5882b9cc992a4d870bc1ebf69cf5d","3b57c7bcece47f2a3198e6adec38f712f2914be5"],"pdfUrls":["http://www2.seas.gwu.edu/~howie/publications/Graphene_FAST17.pdf","https://www.usenix.org/conference/fast17/technical-sessions/presentation/liu","http://www.usenix.org./system/files/conference/fast17/fast17-liu.pdf","https://www.usenix.org/system/files/conference/fast17/fast17-liu.pdf"],"title":"Graphene: Fine-Grained IO Management for Graph Computing","doi":"","sources":["DBLP"],"doiUrl":"","venue":"FAST"},
{"entities":["Central processing unit","Experiment","Internet bottleneck","Operating system","Systems design","User interface","User space"],"journalVolume":"","journalPages":"59-72","pmid":"","year":2017,"outCitations":["12a0046a1197ae63c3d616c74e367dc583cef196","03e255b248ce618f8891484cb747b2ef4bb75448","6636baa2e6310cbc0da231c74e66e0f9f732e55c","0abaa5b259f47e141b8888db3a102048b8a37554","67ac8e37fd240844e0726a2d171e20042c7648f4","01e9ddf1062f9a7d7847bb9bcd2371ce6e0d3e29","20993e1f999c6b4138b4a7ae61dc6471095d69f9","5690155f0b0a16daf17a99da2b68e67495ec63cd","160f182bf5c01c9559473a583e8a730fc1de3c3e","0ce479229630e55e732597cf9b2aeb5018aae4c2","29d45feaa50b0304ab52bd5c6d0381c21c2b42bc","0d5c71866e3e118fcba455feab3d06f86858adee","b46cb54a87a448212af37f2594a512fec39a059e","022fc284362d04569a1561c3d04dfe0f377d6112","03695a9a42b0f64b8a13b4ddd3bfde076e9f604a","0f55217987ec25afa0f815e0aa3957e669b0280e","55becb668bc6cbf0c13b09caa92b849246c36882","2d60d3596490d9999d8433bf41405060779bc11d","35aecf2a6ad7f12ad06d9f9e6b7d4935fea840ac","012ab4527d6aee2387c243d304c624f3b9cf03f3","3c59bf68086f4cc1f216bd4cf461293877d9f46a","1e111e72efe7bbe2c8eeb792318eac40e8f56b80","28e0b55b96bcab20c0f914d4c2d023c361c1b3c7","7c5dc4de32a0d833eb87ae56fb24f9cb35f68fa9"],"s2Url":"https://semanticscholar.org/paper/257c1c169dd0ae98e273efd0d0948f2a028d4c3f","s2PdfUrl":"http://pdfs.semanticscholar.org/257c/1c169dd0ae98e273efd0d0948f2a028d4c3f.pdf","id":"257c1c169dd0ae98e273efd0d0948f2a028d4c3f","authors":[{"name":"Bharath Kumar Reddy Vangoor","ids":["3418470"]},{"name":"Vasily Tarasov","ids":["28670096"]},{"name":"Erez Zadok","ids":["1708491"]}],"journalName":"","paperAbstract":"Traditionally, file systems were implemented as part of OS kernels. However, as complexity of file systems grew, many new file systems began being developed in user space. Nowadays, user-space file systems are often used to prototype and evaluate new approaches to file system design. Low performance is considered the main disadvantage of user-space file systems but the extent of this problem has never been explored systematically. As a result, the topic of user-space file systems remains rather controversial: while some consider user-space file systems a toy not to be used in production, others develop full-fledged production file systems in user space. In this paper we analyze the design and implementation of the most widely known user-space file system framework—FUSE—and characterize its performance for a wide range of workloads. We instrumented FUSE to extract useful statistics and traces, which helped us analyze its performance bottlenecks and present our analysis results. Our experiments indicate that depending on the workload and hardware used, performance degradation caused by FUSE can be completely imperceptible or as high as –83% even when optimized; and relative CPU utilization can increase by 31%.","inCitations":["f912e1c1ded4faaa8576fc942a8931740d43664b","af6365cce49512c386f97976e7ef1dab10aa2dbf","18e93539fe6163a0b56f3427fc562733f89449a6","ad42b4773cd461ba58bda07e1b7b0ff24c4ddba4","9809bc2847bc9274564c6c3545561d920c5e44f3","7f640b84dedbb95ec84d86563b3304035c9fc980","9a397280f7e809008ebe027b0d53e0a8701933d3"],"pdfUrls":["http://www.fsl.cs.stonybrook.edu/docs/fuse/fuse-performance-fast17.pdf","https://www.usenix.org/system/files/conference/fast17/fast17-vangoor.pdf","https://www.usenix.org/conference/fast17/technical-sessions/presentation/vangoor","http://www.usenix.org./system/files/conference/fast17/fast17-vangoor.pdf","http://www.fsl.cs.sunysb.edu/docs/fuse/fuse-performance-fast17.pdf"],"title":"To FUSE or Not to FUSE: Performance of User-Space File Systems","doi":"","sources":["DBLP"],"doiUrl":"","venue":"FAST"},
