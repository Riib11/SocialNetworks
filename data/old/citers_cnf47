{"entities":["Benchmark (computing)","Computation","Cost efficiency","Data center","Experiment","Message Passing Interface","Model of computation","Multi-core processor","Run time (program lifecycle phase)","SimGrid","Simulation","Supercomputer"],"journalVolume":"","journalPages":"92-102","pmid":"","year":2017,"outCitations":["14bd3627a85b658ea1b8450039df7fe0fb57379e","75af374478be81cea1c0c15332c71f3379860691","381c7853690a0fee6f00d2608a7779737f1365f9","4535a96bbd868cbc578da3c512f0db2c3e4ccc2a","dc02287acce63d8b22fb7df8676b415bf0f430ca","f2c6a0039f99bf33fd8eccc1cc16a01c5fe0ffdd","59c6c3ce034e5dadbc378604c7294eccec4ec47f","995c6b5e9ee851f1b70ed85a00867eb79714c246","c39c26d510c1a965c5f132edc989a598ca92b700","705c20122d0f139e747c14a9879f9bb5ae65387a","0f0a5c1844ca682979c6d2d0e9374e331cb9fd99","ed476d78f163b569ed3f95fa7c9628cf95c28799","30a82a63a339c1e69aac36b23900544fe9ec97bb","b2b9114be56ff4b02877a98ac6383b174cb49143","74911278247239eef9e591575822ab2fbd78ad8c","4e23533b91d5ee16674ee632fcbfa5126c8ad125","6049062a3a73d22c914e7fa8951b3b0e5f09b309","306c1c1c05e9fb8db5ad4d0b4e715073f54de6fc","38a0bced15718230eeec1f5ffd29ada0f4f10a7a","42ba158ffa52f859e6849628c550fc65cd936cb1","c4cbcaecad03438bc0639cb382997857a98e8b3d","9bb6ee03d15def91dd6d99e6cf0dfbf503964a5a","3ed3f14d93eb31ff003e5e799e1be811793c8834","35a1ae598c53785ec3957e368040563ee366ecbe","1661baf451086d8a33cc11ae390fd1c5cdd8dc40","abff053bf48012569ef5b858fcf88ee49504dde6","010a2d16eef8be8773ee2a73600f685ec0b2e371","37525b2c3cc16a2fe166708a4f7081b949b1888e","50b8906280a9e8834e9d4a6df3f9d8d069c5e2ff"],"s2Url":"https://semanticscholar.org/paper/fd840a2cb6fb6918689c8374c7316dbb23847c89","s2PdfUrl":"","id":"fd840a2cb6fb6918689c8374c7316dbb23847c89","authors":[{"name":"Franz Christian Heinrich","ids":["26323673"]},{"name":"Tom Cornebize","ids":["1877260"]},{"name":"Augustin Degomme","ids":["2376907"]},{"name":"Arnaud Legrand","ids":["1998583"]},{"name":"Alexandra Carpen-Amarie","ids":["2573681"]},{"name":"Sascha Hunold","ids":["1719340"]},{"name":"Anne-Cécile Orgerie","ids":["3164170"]},{"name":"Martin Quinson","ids":["1756289"]}],"journalName":"2017 IEEE International Conference on Cluster Computing (CLUSTER)","paperAbstract":"Monitoring and assessing the energy efficiency of supercomputers and data centers is crucial in order to limit and reduce their energy consumption. Applications from the domain of High Performance Computing (HPC), such as MPI applications, account for a significant fraction of the overall energy consumed by HPC centers. Simulation is a popular approach for studying the behavior of these applications in a variety of scenarios, and it is therefore advantageous to be able to study their energy consumption in a cost-efficient, controllable, and also reproducible simulation environment. Alas, simulators supporting HPC applications commonly lack the capability of predicting the energy consumption, particularly when target platforms consist of multi-core nodes. In this work, we aim to accurately predict the energy consumption of MPI applications via simulation. Firstly, we introduce the models required for meaningful simulations: The computation model, the communication model, and the energy model of the target platform. Secondly, we demonstrate that by carefully calibrating these models on a single node, the predicted energy consumption of HPC applications at a larger scale is very close (within a few percents) to real experiments. We further show how to integrate such models into the SimGrid simulation toolkit. In order to obtain good execution time predictions on multi-core architectures, we also establish that it is vital to correctly account for memory effects in simulation. The proposed simulator is validated through an extensive set of experiments with wellknown HPC benchmarks. Lastly, we show the simulator can be used to study applications at scale, which allows researchers to save both time and resources compared to real experiments.","inCitations":[],"pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/CLUSTER.2017.66"],"title":"Predicting the Energy-Consumption of MPI Applications at Scale Using Only a Single Node","doi":"10.1109/CLUSTER.2017.66","sources":["DBLP"],"doiUrl":"https://doi.org/10.1109/CLUSTER.2017.66","venue":"2017 IEEE International Conference on Cluster Computing (CLUSTER)"},
{"entities":["Computer performance","Dynamic random-access memory","High Bandwidth Memory","Matrix multiplication","Memory hierarchy","Non-volatile memory","Supercomputer","Volatile memory"],"journalVolume":"","journalPages":"147-151","pmid":"","year":2017,"outCitations":["0c36ce0ea8ec9070edef08d833e2bd18ff919b20","0975baea2e5a34f75c06284ac355af7f2de2499b","c250168f0103da6ed94d20a9fb00e98cce05756d"],"s2Url":"https://semanticscholar.org/paper/cc31cd9d8371add2a99b5894ebd30f28f985c821","s2PdfUrl":"","id":"cc31cd9d8371add2a99b5894ebd30f28f985c821","authors":[{"name":"Xiang Ni","ids":["3209205"]},{"name":"Nikhil Jain","ids":["1812494"]},{"name":"Kavitha Chandrasekar","ids":["2441027"]},{"name":"Laxmikant V. Kalé","ids":["1731961"]}],"journalName":"2017 IEEE International Conference on Cluster Computing (CLUSTER)","paperAbstract":"The increase in memory capacity is substantially behind the increase in computing power in today's supercomputers. In order to alleviate the effect of this gap, diverse options such as NVM - non-volatile memory (less expensive but slow) and HBM - high bandwidth memory (fast but expensive) are being explored. In this paper, we present a common approach using parallel runtime techniques for utilizing NVM and HBM as extensions of the existing memory hierarchy. We evaluate our approach using matrix-matrix multiplication kernel implemented in CHARM++ and show that applications with memory requirement four times the HBM/DRAM capacity can be executed efficiently using significantly less total resources.","inCitations":[],"pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/CLUSTER.2017.94"],"title":"Runtime Techniques for Programming with Fast and Slow Memory","doi":"10.1109/CLUSTER.2017.94","sources":["DBLP"],"doiUrl":"https://doi.org/10.1109/CLUSTER.2017.94","venue":"2017 IEEE International Conference on Cluster Computing (CLUSTER)"},
{"entities":["Approximation error","Complex dynamics","Error detection and correction","Experiment","Instability","Numerical integration","Overhead (computing)","PETSc","Protection mechanism","Rejection sampling","Smart Data Compression","Supercomputer"],"journalVolume":"","journalPages":"592-602","pmid":"","year":2017,"outCitations":["7027eb880ee4d5a1f71bfc861bb36ae980f781fc","fdb03db0c062964fa9fe1beff69efe39a5e07e2d","2208278a005a638e598ddad9f5876073ced238a8","531969063ddef6632e7e5caab72420dc75809ff9","6c5494521e2e88306c8881098c35237f477d10e8","7edb887ed7f15203eccb614095af001ea74bcfb6","24760588276cca66402775b92604bcc061084491","05bdfe9795a9212641fb636425eaa55a4330113a","d672b95b7809a10de53ccd960e04ddc4463795a3","627a32d5f5c0f848e8d6b522fa101f82f856d7b8","dd286cdefbca8f6e435298f058ca413d131f53b0","85f9179c37cc0df6a08f66e6fbf6c5bd651fe618","5c0e8af36e20b8ea213561e8c3d706b4e2f2cc8d","18fe996c6f43a8f301cd842507045b679ba3506a","8d8ae22042e7b189a4c37e39e5ea4f74e6210860","95caf0ab8e5613d4db4cea36545a44a5a39815b6","32d23ce43877aa8cd385a8e01f366329dd015a5c","04d04a9a2942b8f13e405b9feafe854dca7952a7","31f4bdde3501a9d52499668bf67f548220afbb79","01d62cd850496455ce1616500f491690effa5c98","f73620d1a4b1b03244256a37b852a4fd0acab1e4","4b8eaa76c2d517eaf9df9823a7cb77c7ee4e8725","947c449755567fb4560e543e966788cebb7cc4cc","62b996c8b0845277f1b8a1459ecae454c054cd7c","983d5cf52ccc1cd4e8338c6b7c4ee24168a6e807","3eb59f28fbe7f2e3a61f54a5bf1a35cbe22bf1f8"],"s2Url":"https://semanticscholar.org/paper/1239eeb350a9d6c90006c8fc4f6e70adf0643ab2","s2PdfUrl":"","id":"1239eeb350a9d6c90006c8fc4f6e70adf0643ab2","authors":[{"name":"Pierre-Louis Guhur","ids":["3441800"]},{"name":"Emil M. Constantinescu","ids":["35084316"]},{"name":"Debojyoti Ghosh","ids":["1955022"]},{"name":"Tom Peterka","ids":["2284463"]},{"name":"Franck Cappello","ids":["1721552"]}],"journalName":"2017 IEEE International Conference on Cluster Computing (CLUSTER)","paperAbstract":"Scientific computing requires trust in results. In high-performance computing, trust is impeded by silent data corruption (SDC), in other words corruption that remains unnoticed. Numerical integration solvers are especially sensitive to SDCs because an SDC introduced in a certain step affects all the following steps. SDCs can even cause the solver to become unstable. Adaptive solvers can change the step size, by comparing an estimation of the approximation error with an user-defined tolerance. If the estimation exceeds the tolerance, the step is rejected and recomputed. Adaptive solvers have an inherent resilience, because some SDCs might have no consequences on the accuracy of the results, and some SDCs might push the approximation error beyond the tolerance. Our first contribution shows that the rejection mechanism is not reliable enough to reject all SDCs that affect the results' accuracy, because the estimation is also corrupted. We therefore provide another protection mechanism: at the end of each step, a second error estimation is employed to increase the redundancy. Because of the complex dynamics, the choice of the second estimate is difficult: two methods are explored. We evaluated them in HyPar and PETSc, on a cluster of 4,096 cores. We injected SDCs that are large enough to affect the trust or the convergence of the solvers. The new approach can detect 99% of the SDCs, reducing by more than 10 times the number of undetected SDCs. Compared with replication, a classic SDC detector, our protection mechanism reduces the memory overhead by more than 2 times and the computational overhead by more than 20 times in our experiments.","inCitations":[],"pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/CLUSTER.2017.13"],"title":"Detection of Silent Data Corruption in Adaptive Numerical Integration Solvers","doi":"10.1109/CLUSTER.2017.13","sources":["DBLP"],"doiUrl":"https://doi.org/10.1109/CLUSTER.2017.13","venue":"2017 IEEE International Conference on Cluster Computing (CLUSTER)"},
{"entities":["Algorithm","Best, worst and average case","Central processing unit","Deadlock","Docker","Graphics","Graphics processing unit","Middleware","Operating-system-level virtualization","Parallel computing","Scheduling (computing)","Supercomputer"],"journalVolume":"","journalPages":"301-309","pmid":"","year":2017,"outCitations":["04704080ae469d24797ee6369f2e2a72ffcca828","58c26f09f59a47b89a82c9e4f82708e3798e694d","ae69aa2a18530be63f789e6c4399dbdd6a3790f5","7f01ded4bc1d3e658e7969a4ba7d262a6f7d2ed9","6b2c12c91f904781019f187681833d35f5c06e57","0be302437cec82b9200d61d13d3125e62a8ef499","45472bef11491245ad51dde6963e3cc40c5f3b79","01f0204e33faa4f3524f8fe652c9ec42955891b9","7cfd14cec177b2ea04ff2855b94009cf739d42b0"],"s2Url":"https://semanticscholar.org/paper/b33b795533155637ceaa2c89da8bd20794a34d51","s2PdfUrl":"","id":"b33b795533155637ceaa2c89da8bd20794a34d51","authors":[{"name":"Daeyoun Kang","ids":["25106267"]},{"name":"Tae Joon Jun","ids":["8102722"]},{"name":"Dohyeun Kim","ids":["2525559"]},{"name":"Jaewook Kim","ids":["2155208"]},{"name":"Daeyoung Kim","ids":["38004716"]}],"journalName":"2017 IEEE International Conference on Cluster Computing (CLUSTER)","paperAbstract":"Nowadays, Graphics Processing Unit (GPU) is essential for general-purpose high-performance computing, because of its dominant performance in parallel computing compare to that of CPU. There have been many successful trials on the use of GPU in virtualized environment. Especially, NVIDIA Docker obtained a most practical way to bring GPU into the container-based virtualized environment. However, most of these trials did not consider sharing GPU among multiple containers. Without the above consideration, a system will experience a program failure or a deadlock situation in the worst case. In this paper, we propose ConVGPU, a solution to share the GPU in multiple containers. With ConVGPU, the system can guarantee the required GPU memory which the container needs to execute. To achieve it, we introduce four scheduling algorithms that manage the GPU memory to be taken by the containers. These algorithms can prevent the system from falling into deadlock situations between containers during execution.","inCitations":[],"pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/CLUSTER.2017.17"],"title":"ConVGPU: GPU Management Middleware in Container Based Virtualized Environment","doi":"10.1109/CLUSTER.2017.17","sources":["DBLP"],"doiUrl":"https://doi.org/10.1109/CLUSTER.2017.17","venue":"2017 IEEE International Conference on Cluster Computing (CLUSTER)"},
{"entities":["Algorithm","Anatomic Node","Central processing unit","Computation","Hybrid system","Image analysis","Image resolution","Image segmentation","Quantitation","Reuse (action)","Socket Device Component","Speedup","Value (ethics)","algorithm","cellular targeting"],"journalVolume":"","journalPages":"25-35","pmid":"29081725v1","year":2017,"outCitations":["8630c01e55b4025ad30f857c0218f392facb8f21","791c8b4e7538cfd542e20dd1a27d0a78b33bed6f","b9d5572f70e5b3c4287b17ba23c223e9515d3714","6b9f7f1e8a602ff83126d087c5a08aa9c8c12f16","3fddfe82fbd1866cccd9eb6f3577533521bfe0b0","b10145f7fc3d07e43607abc2a148e58d24ced543","b6fdeb0c962a7e0b24480b20a044cb925f15f077","0f5de500a6bfdd7c0c7ff40a9717af3a56fdefc2","09045f55ef18cfb6bb97934857bc5906f0f14c70","8fc52ce413863e5b9d78f884912858cd8a1f4ad9","2566acc500a8f013610d306bea7a8f548930dfed","03daf2d17337f000538d9d4727fa49d52bdb922c","7b0569980ca59e6b7d5c1f9dea97464640149b84","061e80ca3bc302b1f5031d0065e563423dafb12e","bf9cdf51852562e5f09a3ddbd6c93b12abbc152a","0a756312d6a6dfcf0a9e27f91affce6412833a9f","d119a886aa6a2062038567b6f840f843930e1f1f"],"s2Url":"https://semanticscholar.org/paper/0418905a962864523b9d0283e5b1dfa940038cfe","s2PdfUrl":"","id":"0418905a962864523b9d0283e5b1dfa940038cfe","authors":[{"name":"Willian Barreiros","ids":["26388047"]},{"name":"George Teodoro","ids":["2711977"]},{"name":"Tahsin M. Kurç","ids":["1753288"]},{"name":"Jun Kong","ids":["1711386"]},{"name":"Alba Cristina Magalhaes Alves de Melo","ids":["1771683"]},{"name":"Joel H. Saltz","ids":["1735710"]}],"journalName":"2017 IEEE International Conference on Cluster Computing (CLUSTER)","paperAbstract":"We investigate efficient sensitivity analysis (SA) of algorithms that segment and classify image features in a large dataset of high-resolution images. Algorithm SA is the process of evaluating variations of methods and parameter values to quantify differences in the output. A SA can be very compute demanding because it requires re-processing the input dataset several times with different parameters to assess variations in output. In this work, we introduce strategies to efficiently speed up SA via runtime optimizations targeting distributed hybrid systems and reuse of computations from runs with different parameters. We evaluate our approach using a cancer image analysis workflow on a hybrid cluster with 256 nodes, each with an Intel Phi and a dual socket CPU. The SA attained a parallel efficiency of over 90% on 256 nodes. The cooperative execution using the CPUs and the Phi available in each node with smart task assignment strategies resulted in an additional speedup of about 2&#xd7;. Finally, multi-level computation reuse lead to an additional speedup of up to 2.46&#xd7; on the parallel version. The level of performance attained with the proposed optimizations will allow the use of SA in large-scale studies.","inCitations":[],"pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/CLUSTER.2017.28"],"title":"Parallel and Efficient Sensitivity Analysis of Microscopy Image Segmentation Workflows in Hybrid Systems","doi":"10.1109/CLUSTER.2017.28","sources":["Medline","DBLP"],"doiUrl":"https://doi.org/10.1109/CLUSTER.2017.28","venue":"2017 IEEE International Conference on Cluster Computing (CLUSTER)"},
{"entities":["Algorithm","Computation","GeForce 700 series","Graphics processing unit","Manycore processor","Multi-core processor","Sparse matrix","Speedup","Titan"],"journalVolume":"","journalPages":"47-57","pmid":"","year":2017,"outCitations":["6206e90af42990d97e547c08a2fb75447155fd86","1d0f25989452abbbc8feaf00a034ff110fc4b350","008a6e4b2763736d2c6363ee6b546b09c0022e53","53132a1619b13215bcd791cd6b850ff154f4f837","231f97057e1efed073c20ccdf3aa3c5aaf063ffb","6074c1108997e0c1f97dc3c199323a162ffe978d","5c3785bc4dc07d7e77deef7e90973bdeeea760a5","374e96d4ce090569323cb63a6dd084f06cb833d5","a9653a27052d666b7ed47524871dc9c3a9b92cc4","aca75724674bd0f608086f90f3e229ae2c0f92a7","07ed71b436b9adf23f0f93c8e4533461b82e769a","03b6c2f7876cb9f4e8218c5f749b6959bbc3c983","669508257d4621864011252d0423047f98d9329c","02c1580617a9b1ccd809f06ae57773c00cf96647","000b693f3398a7fff3d393ae89f9ca18d8f10856","280bbaa66095fd6f89999003b802700935fdf77c","53a225f2843e8544ca9c615ecfcc5fad26083e49","514514e3f6150d1f36a7820fc5da5a17953d62f7","1322c225b4e05dc22bbff7c5b9f5464f3cb7754b","2e8ab628bc9f256c11c898aa44f049143c74d05d","16d946c1113fcebf79a2d3af2062be37a995d133","255aeb5c2a8eea15db08c617481ddbb35a41bfe4","1cd294f3bcd647c8a2b2bbce47e827a8ece8b973","048bfc88b9f54512304433bb2eeb68a3172159a8","62dd02837c65b9c90de8d80c493f23ce1116cb3d"],"s2Url":"https://semanticscholar.org/paper/6430973555601cfd810351a33f539bc2f5567f35","s2PdfUrl":"","id":"6430973555601cfd810351a33f539bc2f5567f35","authors":[{"name":"Bangtian Liu","ids":["2900532"]},{"name":"Chengyao Wen","ids":["12181108"]},{"name":"Anand D. Sarwate","ids":["9208982"]},{"name":"Maryam Mehri Dehnavi","ids":["2917750"]}],"journalName":"2017 IEEE International Conference on Cluster Computing (CLUSTER)","paperAbstract":"Sparse tensors appear in many large-scale applications with multidimensional and sparse data. While multidimensional sparse data often need to be processed on manycore processors, attempts to develop highly-optimized GPU-based implementations of sparse tensor operations are rare. The irregular computation patterns and sparsity structures as well as the large memory footprints of sparse tensor operations make such implementations challenging. We leverage the fact that sparse tensor operations share similar computation patterns to propose a unified tensor representation called F-COO. Combined with GPU-specific optimizations, F-COO provides highly-optimized implementations of sparse tensor computations on GPUs. The performance of the proposed unified approach is demonstrated for tensor-based kernels such as the Sparse Matricized Tensor-Times-Khatri-Rao Product (SpMTTKRP) and the Sparse Tensor-Times-Matrix Multiply (SpTTM) and is used in tensor decomposition algorithms. Compared to state-of-the-art work we improve the performance of SpTTM and SpMTTKRP up to 3.7 and 30.6 times respectively on NVIDIA Titan-X GPUs. We implement a CANDECOMP/PARAFAC (CP) decomposition and achieve up to 14.9 times speedup using the unified method over state-of-the-art libraries on NVIDIA Titan-X GPUs.","inCitations":["934e7c28243a136099a75c5c518bc8bb5a61ca49"],"pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/CLUSTER.2017.75","https://arxiv.org/pdf/1705.09905v1.pdf","http://arxiv.org/abs/1705.09905"],"title":"A Unified Optimization Approach for Sparse Tensor Operations on GPUs","doi":"10.1109/CLUSTER.2017.75","sources":["DBLP"],"doiUrl":"https://doi.org/10.1109/CLUSTER.2017.75","venue":"2017 IEEE International Conference on Cluster Computing (CLUSTER)"},
{"entities":["Booting","Cloud computing","Compiler","Docker","Graphical user interface","Interference (communication)","Operating system","Operating-system-level virtualization","Platform as a service","Scalability","Solution stack","Swarm","Usability"],"journalVolume":"","journalPages":"290-300","pmid":"","year":2017,"outCitations":["223dca51178da8c61e2428b8909443ab2945ab51","7a1eec7fc53fa2e4de69bab6aed1b68e949331f1","f7d59af41e7661b78fcba086019d3e12fbb283e9","1bdc4138a8a56a331c17141df594c73e109efcc7","dbe151dee791f2f092a75090a7d562236da47981","534774678395c584ec75dd4857a3a2001534d242","4355acea7b73a74a353d54156c2cdc889f0af319","1bdf058ab0788c2c6f538b78df5570acfe065647","82ec25dc56f95f8bc63cc4cb4d7f1b1f4f9719d8","8e13e107e040f44ca190b6a65bae8e14596ca4f5","2bbc14aa15c5cf0985fb28d9b15946bf4230d372","ab1a28bb87dc9271649f1676ac08d42fb7f0d506","2e7699f88c75d0b5b3fb2d4de2c9ba82c87292d5","f8ac1f09e3e9e25311324a2f12c91a44eb198009","53d8407a77b0a7bf76fc8d7e3b3fcff77e3f5e4d","9c96514250c4a35deba5ae3ffb93e9731fe23a79","88525e710cf0c1aef79ffad59906f43fffd8c757","a6b4dd1c9d8ea3d696f8009dcb25c30fe1eff625","26f3cbecbc636984d57e52191c1d87c9377aff6f"],"s2Url":"https://semanticscholar.org/paper/c8b75340394736a2658ce08b3af9cae8672c514b","s2PdfUrl":"","id":"c8b75340394736a2658ce08b3af9cae8672c514b","authors":[{"name":"Panagiotis Patros","ids":["3293985"]},{"name":"Dayal Dilli","ids":["37233910"]},{"name":"Kenneth B. Kent","ids":["1737718"]},{"name":"Michael Dawson","ids":["31500848"]}],"journalName":"2017 IEEE International Conference on Cluster Computing (CLUSTER)","paperAbstract":"Platform as a Service (PaaS) clouds provide part of the hardware/software stack and related services to tenant applications. Increased load is handled elastically by scaling, which either modifies the number of instances an application has available on the cloud or increases their available resources. However, because all these instances run inside isolated containers, experience gained by the first instance of an application cannot be easily shared with subsequent scaled instances. This results in both increased startup time and response timeout errors for the scaled instances as well as increased performance interference for any co-located applications; reacquiring this experience is a time-consuming and resource-intensive process. We propose a scalable and secure technique to share dynamically compiled artifacts produced by the first execution instance of an application and otherwise created for intra-OS sharing only with subsequent scaled or restarted instances as a solution to these problems. Our solution abides by the usual PaaS limitations and uses a distributed and containerized cloud service, which we experimentally show to be scalable on a Docker Swarm running on top of a 6-VM cluster; also, we discuss the results of a usability survey for the service's GUI conducted with expert subjects. The effectiveness of the DCAS technique was experimentally tested on an isolated installation of the PaaS software Cloudy Foundry; we measured significant reductions in both the startup time and response errors of scaled out instances as well as performance interference to co-located tenants during scaling.","inCitations":[],"pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/CLUSTER.2017.9"],"title":"Dynamically Compiled Artifact Sharing for Clouds","doi":"10.1109/CLUSTER.2017.9","sources":["DBLP"],"doiUrl":"https://doi.org/10.1109/CLUSTER.2017.9","venue":"2017 IEEE International Conference on Cluster Computing (CLUSTER)"},
{"entities":["Algorithm","Backup","Computational complexity theory","Correctness (computer science)","Experiment","Failure rate","Fault tolerance","Heuristic","Heuristic (computer science)","Real-time computing","Stream processing","Streaming media"],"journalVolume":"","journalPages":"379-383","pmid":"","year":2017,"outCitations":["9e074f3d1c0e6212282818c8fb98cc35fe03f4d0","63115442310908b876aa1e81d877813ebee8b247","962d0f79f2a3adabef266375685e551844156130","edd661ca12ef8ef988679f7b399f2f846ef01fbd","04afd5f18d3080c57d4b304dfbd1818da9a02e8e","5916526f6aed7da0ec6812729cf468cda5b9d49b","036e006a9f2049d15c1533ac254dcfce2483a1f6","e0b3d5095ca65792b0ae77417c66578c0253d1aa","fbe47f7d7e8df21cbe39c1f65d25165195ecba54","2495ac46de086e8b217e87400ab4b2e637d81dcf","9d46900406ba1bfee140ce048350504ffb1fe7e5","20eb6a33ebc85a551510447b73928148cec1dbeb","478fbef8568a021c3d91c13128efa19ad719dd88","0cded775165fd38f333e5b80ee233ea8d4405139","e4fd518cd67e03ef263eb0ad6876c3578cd5bbf8","bad84100cd1bffe83bd33212a79d5cbb7f4ffb12"],"s2Url":"https://semanticscholar.org/paper/21501c56fbd11f0a5e3347feabbbe217d03fcdfe","s2PdfUrl":"","id":"21501c56fbd11f0a5e3347feabbbe217d03fcdfe","authors":[{"name":"Hongliang Li","ids":["1741866"]},{"name":"Jie Wu","ids":["36416867"]},{"name":"Zhen Jiang","ids":["1764407"]},{"name":"Xiang Li","ids":["1737850"]},{"name":"Xiaohui Wei","ids":["2094025"]}],"journalName":"2017 IEEE International Conference on Cluster Computing (CLUSTER)","paperAbstract":"Stream processing applications continuously process large amounts of online streaming data in real-time or near real-time. They have strict latency constraints, but they are also vulnerable to failures. Failure recoveries may slow down the entire processing pipeline and break latency constraints. Upstream backup is one of the most widely applied fault-tolerant schemes for stream processing systems. It introduces complex backup dependencies to tasks, and increases the difficulty of controlling recovery latencies. Moreover, when dependent tasks are located on the same processor, they fail at the same time in processor-level failures, bringing extra recovery latencies that increase the impacts of failures. This paper presents a correlated failure effect model to describe the recovery latency of a stream topology in processor-level failures for an allocation plan. We introduce a Recovery-latency-aware Task Allocation Problem (RTAP) that seeks task allocation plans for stream topologies that will achieve guaranteed recovery latencies. We present a heuristic algorithm with a computational complexity of O(nlog^2n) to solve the problem. Extensive experiments were conducted to verify the correctness and effectiveness of our approach.","inCitations":["585aa0d32c0c5a510e04a33039b7af9a85dab93a"],"pdfUrls":["https://cis.temple.edu/~jiewu/research/publications/Publication_files/Li_IPCCC_2017.pdf","http://doi.ieeecomputersociety.org/10.1109/CLUSTER.2017.10"],"title":"Task Allocation for Stream Processing with Recovery Latency Guarantee","doi":"10.1109/CLUSTER.2017.10","sources":["DBLP"],"doiUrl":"https://doi.org/10.1109/CLUSTER.2017.10","venue":"2017 IEEE International Conference on Cluster Computing (CLUSTER)"},
{"entities":["Comet (programming)","FLOPS","InfiniBand","Message Passing Interface","Network switch","Non-blocking algorithm","Omni-Path","Pre-rendering","Profiling (computer programming)","San Diego Supercomputer Center","Scalability"],"journalVolume":"","journalPages":"354-358","pmid":"","year":2017,"outCitations":["30da25050155258022cee56f78c06a601356ed62","24851a5eb63727a13fbfd4f2769262352d89a5f6","373923156a288897fc1245ed1dc7e0b7d0ca3df9","0323b626078b11e63509339771c20a7e283a1d70","5fd84359b5b3580c9a54a0db5a4bc82c2458530c"],"s2Url":"https://semanticscholar.org/paper/cfd4f0f4a73a7f09b545dcb5f0e15b5e4f173f50","s2PdfUrl":"","id":"cfd4f0f4a73a7f09b545dcb5f0e15b5e4f173f50","authors":[{"name":"Hari Subramoni","ids":["1802958"]},{"name":"Xiaoyi Lu","ids":["1720335"]},{"name":"Dhabaleswar K. Panda","ids":["1731654"]}],"journalName":"2017 IEEE International Conference on Cluster Computing (CLUSTER)","paperAbstract":"Studying the interaction among applications, MPI runtimes, and the fabric they run on is critical to understanding application performance. There exists no high-performance and scalable tool that enables understanding this interplay on modern multi-petaflop systems. Designing such a tool is non-trivial and involves multiple components including 1) data profiling/collection from network/MPI library, 2) storing and, 3) rendering the data. Furthermore, achieving this with minimal overhead and scalability is a challenging task. We take up this challenge and propose a high-performance and scalable network-based performance analysis tool for MPI libraries operating on modern networks like InfiniBand and Omni-Path. Our designs facilitate caching and pre-rendering, allowing a cluster with 6,541 nodes, 764 switches and, 16,893 network links renders in just 30 seconds &#x2013; a 44X speed up over non-prerendered solutions. The proposed lock-free and optimized memory-backed storage design enables the tool to handle over a quarter million inserts into the database every 45 seconds (data from 27,504 switch ports and 104,656 MPI processes). The tool has been successfully deployed and validated on HPC systems at OSC and on Comet at SDSC.","inCitations":["7a7599b0f68e5997d857fca6b8da2c151f08fa62"],"pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/CLUSTER.2017.78"],"title":"A Scalable Network-Based Performance Analysis Tool for MPI on Large-Scale HPC Systems","doi":"10.1109/CLUSTER.2017.78","sources":["DBLP"],"doiUrl":"https://doi.org/10.1109/CLUSTER.2017.78","venue":"2017 IEEE International Conference on Cluster Computing (CLUSTER)"},
{"entities":["Apache Hadoop","Attribute–value pair","Cache (computing)","Consistent hashing","Distributed hash table","Distributed memory","Fair queuing","Hit (Internet)","In-memory database","Job scheduler","Key-value database","Load balancing (computing)","Locality of reference","Look and feel","MapReduce","Scheduling (computing)","USB flash drive"],"journalVolume":"","journalPages":"322-332","pmid":"","year":2017,"outCitations":["876186bfd05bdd01c1f2ea288c532b16b8a0694f","6a21158317711c9b349c68cd7e7b2a92efbbb074","b4d8da39f041d1f16fd106792df5d92e136af187","c131f2b65169e3162e2d6430019bad81c7919ed5","24281c886cd9339fe2fc5881faf5ed72b731a03e","2f47c7304aa5008911db59bef5c0fd3d3e212088","3af2153f2c6825fd3106aa9efd56db8eef311767","62a7c092e607640273f69cae1372d0677bad2615","0368d2445d3ee4205ee73da933cb8b810a89091c","1f50075cd3100832c9b82c4d78259d833a4f3288","04fe7d8276178be18afd6c17e399e8df4ab693c7","01e9ddf1062f9a7d7847bb9bcd2371ce6e0d3e29","3b75c563febb906bce3349c760def9337070946d","c737aa8b2c916fe1f13a6fd4e847fa45da1e5434","04a804e9720c67c16715ba96288821af92166a45","35e8655b2c8845d607fc14ca12a42311dc30c379","33316a454cf8eb7bf78e7d4fdc9525c945d9a118","09dbd5e0e3b4ed956a0dfcacb2a2d007fb8e3d17","4a0bb4eece00f3e9445d1a0d933422aa408ce8d1","389965aeae8a46b725267b3bf025440609f67012","661b19ff987b9ed9d9252324d4a72ab1fbd588ae","7ebb32dfffe74228b3877f3c825ca191eb5e7469","0558c94a094158ecd64f0d5014d3d9668054fb97","332f77fd05703c1607e3b57884ad31fb1fad0104","0a12a179bebdf4bb69d692a1127795b3f536270b","52bfb3aa30ec06784d839ab431287a657d0d7907","47947ed7d4c12855b1b5a4c4ec3123528761d64b","036d544defb7f8e6297bd4c57a3b430d04a269e8","0541d5338adc48276b3b8cd3a141d799e2d40150","277fdd6dbd792fd41e401b13e0fd897bfd911378","16bdb244d50b0892535c6c8be4c4ec7e25a43de6","5599ed5b57958b32889c9f4f6c9261941ce2e79f","8dd808bd68d1c46e3678dce30ecc4791d71f9ee1","37601bb6e655f2392ba1ca2086da0d1e03e19edc","f19870a1b4847ca61beed722d557a50189479d27"],"s2Url":"https://semanticscholar.org/paper/17d77d5e2db5b9aaf54b8240f829b1d4f077df29","s2PdfUrl":"","id":"17d77d5e2db5b9aaf54b8240f829b1d4f077df29","authors":[{"name":"Vicente A. B. Sanchez","ids":["38668659"]},{"name":"Wonbae Kim","ids":["17804514"]},{"name":"Youngmoon Eom","ids":["3289855"]},{"name":"Kibeom Jin","ids":["26393736"]},{"name":"Moohyeon Nam","ids":["3461436"]},{"name":"Deukyeon Hwang","ids":["2951933"]},{"name":"Jik-Soo Kim","ids":["1687878"]},{"name":"Beomseok Nam","ids":["1739708"]}],"journalName":"2017 IEEE International Conference on Cluster Computing (CLUSTER)","paperAbstract":"We present EclipseMR, a novel MapReduce framework prototype that efficiently utilizes a large distributed memory in cluster environments. EclipseMR consists of double-layered consistent hash rings - a decentralized DHT-based file system and an in-memory key-value store that employs consistent hashing. The in-memory key-value store in EclipseMR is designed not only to cache local data but also remote data as well so that globally popular data can be distributed across cluster serversand found by consistent hashing.In order to leverage large distributed memories and increase the cache hit ratio, we propose a locality-aware fair (LAF) job scheduler that works as the load balancer for the distributed in-memorycaches. Based on hash keys, the LAF job scheduler predicts which servers have reusable data, and assigns tasks to the servers so that they can be reused. The LAF job scheduler makes its best efforts to strike a balance between data locality and load balance, which often conflict with each other. We evaluate EclipseMR by quantifying the performance effect of each component using several representative MapReduce applications and show EclipseMR is faster than Hadoop andSpark by a large margin for various applications.","inCitations":[],"pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/CLUSTER.2017.12"],"title":"EclipseMR: Distributed and Parallel Task Processing with Consistent Hashing","doi":"10.1109/CLUSTER.2017.12","sources":["DBLP"],"doiUrl":"https://doi.org/10.1109/CLUSTER.2017.12","venue":"2017 IEEE International Conference on Cluster Computing (CLUSTER)"},
{"entities":["Binary file","Dynamic random-access memory","Multitier architecture","Volatility","Xeon Phi"],"journalVolume":"","journalPages":"126-136","pmid":"","year":2017,"outCitations":["f6d5ff67efd843caa15002b1a4a66cb704668044","48b22e82dba733f04a5af3408ecab872ba38f0c3","5d0fc2dcc3ce19998e4de6c4dd1702df0367c108","f4ed7fb35916bd0d36a53198384bb0ed2ff34c3f","d1a6a735e72ababac022d0d5a35fcb620742ca98","4511599bc8cf05af6355a36dad3e1a9b75bb2301","f3325ace129dec914966f9894d9f412e5e04bdc2","298c14f1afc65a9c58b8ae5abe16a27ea4f13a71","c9eac2b2f2224f8aa4deb1a08d8e1228cf3dbd62","8b8d9dbe3e755cbbab950b6133b1cc11d8e08943","398aaf00253e2c29e6238dd0499aa3a75c76914c","6d124439630bd2347ebe25b48879e01ee747f716","18f1e9248ad1dc3d249f911b3f609a087c3aca39","403cbd3649669b52f0fac0d23af13f8e292864d5","0653e2ed9f683868cb4539eb8718551242834f6b","912358039b7ecf026a04e7e34b2f36c19913b1ef","04ce0bd4df15e05d376cad98de8b9a83380341d3","178599e5e976e82528e71cb2e1b812d588fa0e44","7dc5dcf29c65c576b37ee6359f58df3ede32b90c","534c2f4f1165a6afcbab125254cf8fcdf4ca10d1","1c15910d27ee940f71bd1d9a5c25c0230e3025fb","24ff16b30689cb61df2ac391f5306584769ea7fb","65128e128751d8f27d0bf765db70e563755f027a"],"s2Url":"https://semanticscholar.org/paper/2fb9d624d8d5fac39e32e067c272e1260b8b9e84","s2PdfUrl":"","id":"2fb9d624d8d5fac39e32e067c272e1260b8b9e84","authors":[{"name":"Harald Servat","ids":["2003728"]},{"name":"Antonio J. Peña","ids":["24636606"]},{"name":"Germán Llort","ids":["2600478"]},{"name":"Estanislao Mercadal","ids":["1767394"]},{"name":"Hans-Christian Hoppe","ids":["40215479"]},{"name":"Jesús Labarta","ids":["1699563"]}],"journalName":"2017 IEEE International Conference on Cluster Computing (CLUSTER)","paperAbstract":"Multi-tiered memory systems, such as those based on Intel&#x00AE; Xeon Phi&#x2122;processors, are equipped with several memory tiers with different characteristics including, among others, capacity, access latency, bandwidth, energy consumption, and volatility. The proper distribution of the application data objects into the available memory layers is key to shorten the time&#x2013; to&#x2013;solution, but the way developers and end-users determine the most appropriate memory tier to place the application data objects has not been properly addressed to date.In this paper we present a novel methodology to build an extensible framework to automatically identify and place the application&#x2019;s most relevant memory objects into the Intel Xeon Phi fast on-package memory. Our proposal works on top of inproduction binaries by first exploring the application behavior and then substituting the dynamic memory allocations. This makes this proposal valuable even for end-users who do not have the possibility of modifying the application source code. We demonstrate the value of a framework based in our methodology for several relevant HPC applications using different allocation strategies to help end-users improve performance with minimal intervention. The results of our evaluation reveal that our proposal is able to identify the key objects to be promoted into fast on-package memory in order to optimize performance, leading to even surpassing hardware-based solutions.","inCitations":["1cf5e11f8230c9badb8e963c070ecca2c1bda709"],"pdfUrls":["http://upcommons.upc.edu/bitstream/handle/2117/109407/Automating+the+Application+Data+Placement+in.pdf;jsessionid=EC18689FCF2BDA1227D746629E3340AA?sequence=4","http://doi.ieeecomputersociety.org/10.1109/CLUSTER.2017.50"],"title":"Automating the Application Data Placement in Hybrid Memory Systems","doi":"10.1109/CLUSTER.2017.50","sources":["DBLP"],"doiUrl":"https://doi.org/10.1109/CLUSTER.2017.50","venue":"2017 IEEE International Conference on Cluster Computing (CLUSTER)"},
{"entities":["Algorithm","Application checkpointing","Central processing unit","Directed acyclic graph","Dynamic programming","Fail-stop","Failure rate","Graph (discrete mathematics)","Job shop scheduling","List scheduling","Makespan","Order of approximation","Parallel computing","Recursion","Run time (program lifecycle phase)","Schedule (project management)","Scheduling (computing)","Series-parallel graph","Stable storage"],"journalVolume":"","journalPages":"487-497","pmid":"","year":2017,"outCitations":["6fb9d8334659baa82339cc4af52b59abbc8316ac","3466c2fd45c0a8f49b897a2b6f195c330c468cb3","4908fe53a91465eaf95b21c4ca4f05378b90dcc4","53242679719fbf985d7a1132a6d538f4dded42c6","512bbff1834aa5f9e346521ce538ef2d09e8b304","174707ad91eebda40bbca68fff2eb15a741196fb","4224374796da64e17fce96033d4cd42240d80eaf","16258e60ad93771968f0c74ee18cc4850cbf6946","9d974bff46b6a4a5a889a30ac37a5fce2c5b634d","02c125aaea27be981fd9f0012c2c55436aace1ea","9c8bbee60dac4ab599276815068e11f487ccb69e","bf502232cc9c04728ad2308fd30c043a1aaee305","1324f1d5b20f08cac775f10089a788767c56d5a9","3087a47c1fdb5ebb1b28f3562533e3cce782dd36","3b4d5b3e93aa6d5ea413c726c17057d8d1eafb2d","95e91566920af5ea257986daee9356d7beacb478","7e944c565a5719e054ce4f52f06af06932b4c72d","02d3739f3d1af8a529fb60366c854b4e207e6e75","ea1e01aa57774c0adff45dbfe9cd5a47d8b163c4","050348e54d59952782ace21cac48735bc0d23b8e","244f2bb3a33b20ac31d53097a767bb317ce0dcaa","525b50b4ae438d89f2b088c781583bb136f8a083","12d8599ceee4e88993293669b42482e0be193449","8fc52ce413863e5b9d78f884912858cd8a1f4ad9","954e25d80547d6478ad78fce26cce41d1c7a8415","24c4b220eb8717d1cad7f577db5837005e46f69e"],"s2Url":"https://semanticscholar.org/paper/5a6d6b73083571cce741122ce7ae49939f26b80d","s2PdfUrl":"","id":"5a6d6b73083571cce741122ce7ae49939f26b80d","authors":[{"name":"Li Han","ids":["1751385"]},{"name":"Louis-Claude Canon","ids":["2180844"]},{"name":"Henri Casanova","ids":["1707417"]},{"name":"Yves Robert","ids":["1735015"]},{"name":"Frédéric Vivien","ids":["1736346"]}],"journalName":"2017 IEEE International Conference on Cluster Computing (CLUSTER)","paperAbstract":"We consider the problem of orchestrating the execution of workflow applications structured as Directed Acyclic Graphs (DAGs) on parallel computing platforms that are subject to fail-stop failures. The objective is to minimize expected overall execution time, or makespan. A solution to this problem consists of a schedule of the workflow tasks on the available processors and of a decision of which application data to checkpoint to stable storage, so as to mitigate the impact of processor failures. For general DAGs this problem is hopelessly intractable. In fact, given a solution, computing its expected makespan is still a difficult problem. To address this challenge, we consider a restricted class of graphs, Minimal Series-Parallel Graphs (M-SPGS). It turns out that many real-world workflow applications are naturally structured as M-SPGS. For this class of graphs, we propose a recursive list-scheduling algorithm that exploits the M-SPG structure to assign sub-graphs to individual processors, and uses dynamic programming to decide which tasks in these sub-gaphs should be checkpointed. Furthermore, it is possible to efficiently compute the expected makespan for the solution produced by this algorithm, using a first-order approximation of task weights and existing evaluation algorithms for 2-state probabilistic DAGs. We assess the performance of our algorithm for production workflow configurations, comparing it to (i) an approach in which all application data is checkpointed, which corresponds to the standard way in which most production workflows are executed today; and (ii) an approach in which no application data is checkpointed. Our results demonstrate that our algorithm strikes a good compromise between these two approaches, leading to lower checkpointing overhead than the former and to better resilience to failure than the latter.","inCitations":[],"pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/CLUSTER.2017.14","http://www.icl.utk.edu/files/publications/2017/icl-utk-964-2017.pdf"],"title":"Checkpointing Workflows for Fail-Stop Errors","doi":"10.1109/CLUSTER.2017.14","sources":["DBLP"],"doiUrl":"https://doi.org/10.1109/CLUSTER.2017.14","venue":"2017 IEEE International Conference on Cluster Computing (CLUSTER)"},
{"entities":["Computer cluster","Concurrency (computer science)","Haswell (microarchitecture)","Intel Core (microarchitecture)","Memory module","Multi-core processor","Parallel computing","Power management","Power supply","Scalability","Scheduling (computing)"],"journalVolume":"","journalPages":"541-551","pmid":"","year":2017,"outCitations":["77f826132cf09ac91ea9c859387a8d52221a019a","7e48637082584703bae2dfc83953e7ff0c32e9b6","35bc9e9d0a8c0451c47131875e88d7c20f28aa92","7e757fff66a63b268da83ffccf464437492ac8b6","20d6a8a39ebecd21bc8a4df53f248356d38ea6d9","89d4dcc25809693fa3505d09b2721c1c2c2559b2","1585eaffcf9c9836eb1607e279e43ce2793e59a0","a47b408349a8146f71cb54c38226d2f7d92700fe","cedeaf86f2e06e4ab3b218aaf6cdfe65e2d9cbe1","efee61acb1847de685817b7d9bc1b6b095ef5026","7d52953086089b85db2bedb16f56790b9116a2b6","15860f9f774f19f245f016d9cf479222e4f9a6ba","a9831b6062fb678d4591eee853e81116d038bb05","b04391910d19d2d0c64b62d300927f527417414e","4a6bf6c38051ec5f81be18de75e8ecb6e5e72c06","1ea7d63617a0fdc5eadf37596d00688615565351","073e26aa7192825a8d872fb0c6f25bc31aca77cf","00cf571f4060063b79e7f64eab42b1ef064660f4","610d61a4543bdb1109de0e5f9760d44e44e6014d","9a000edf8d478fa3b0d7f74fb966664da5d33354","1f5b507c038b09f017bffd51d4f4e4257bef6ef4","1031ac970dfc4afd1cda54aca8f6ddce234edc89","3462fb38042f0bde20c758728d7c8c28a1f47e09","387eb8909b5527dd2513cfdd2f376a3a1f2973b3","86abc95269643d9ce18286f896b486bb3026f1ee","1726f30174b09c0ba28899e81b00a1e3305e52a5","87a34f805b3316ac75c6b3110d36a4bc576ac063","9efa7f12bfd9d8ed38c29c5e128b21b07a438cd9","1fd674f96ef677bf09d7538673eda576aa8102c9","14bd3627a85b658ea1b8450039df7fe0fb57379e","02ebdcf8200135ec0433e12e4ef2459ac740370b","f6ab527a5919b48b66908954a3086947c5bffde6","0e8e26e9b86b8bc74997e6a28aeb49c0e8a31404"],"s2Url":"https://semanticscholar.org/paper/66ace9694c92cbfad85ca3fccb79215b44b6d126","s2PdfUrl":"","id":"66ace9694c92cbfad85ca3fccb79215b44b6d126","authors":[{"name":"Pengfei Zou","ids":["39797395"]},{"name":"Tyler Allen","ids":["40278775"]},{"name":"Claude H. Davis IV","ids":["11023982"]},{"name":"Xizhou Feng","ids":["1781155"]},{"name":"Rong Ge","ids":["38342948"]}],"journalName":"2017 IEEE International Conference on Cluster Computing (CLUSTER)","paperAbstract":"High performance computing systems will need to operate with certain power budgets while maximizing performance in the exascale era. Such systems are built with power aware components, whose collective peak power may exceed the specified power budget. Cluster level power bounded computing addresses this power challenge by coordinating power among components within compute nodes and further adjusting the number of participating nodes. It offers more space to increase system performance by utilizing the available power budget more efficiently within and across the nodes.In this paper, we present the design of a hierarchical multi-dimensional power aware allocation framework, CLIP, for power bounded parallel computing on multicore-based computer clusters. The framework satisfies the specified power bound by managing the power distribution among nodes at the cluster level, and among sockets, cores and NUMA memory modules at the node level. The power allocation is enforced with multiple complementary power management techniques, including memory power level setting, thread concurrency throttling, and core-thread affinity. We present an application characterization method based on applications' scalability and an associated performance model, which can accurately determine the optimal number of participating compute nodes and components, and their power distribution for given applications. Experimental results on a Haswell-based computer cluster show that the proposed scheduler outperforms compared methods by over 20% on average for various power budgets.","inCitations":[],"pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/CLUSTER.2017.98"],"title":"CLIP: Cluster-Level Intelligent Power Coordination for Power-Bounded Systems","doi":"10.1109/CLUSTER.2017.98","sources":["DBLP"],"doiUrl":"https://doi.org/10.1109/CLUSTER.2017.98","venue":"2017 IEEE International Conference on Cluster Computing (CLUSTER)"},
{"entities":["Airborne Ranger","Algorithm","Big data","CUDA","Central processing unit","Cluster analysis","Computational problem","Computer cooling","Data mining","Distributed memory","Ecology","GeForce 700 series","Graphics processing unit","K-means clustering","Memory hierarchy","Message Passing Interface","Numerical analysis","OpenACC","Scalability","Sensor","Simulation","Software deployment","Supercomputer","Titan","Titan (supercomputer)","Unmanned aerial vehicle","Unsupervised learning"],"journalVolume":"","journalPages":"267-277","pmid":"","year":2017,"outCitations":["b7ddb9b89874e517d0c6eee558a9f90bceb14ae7","1ca0eb31f37606cdf77851b29153fbebedcefafc","f67cb6025de5ee7991765c25a445304becd18221","95bcb5593a8916440df60caf5730640d766046d6","03efd91b6a7cab2746d0172eb57e987a5e18f3b7","40fb14ce84e1f1f774de99cae59a06713f5f4155","f2cb09c934265c04a517d8b1ef526378bd13814d","273c7905de2108cd02300927b667248c8ca10035","8c0b0e80514f70b4eef3a274315168c7a5a66335","3e49142a9c4c027f824d57cd830ee408db526a8f","0585f10992496cd80128e82503499042dcdea2ab","408e9cb5000565e5ee1baae673d6e08fbbc55d48","0638dc0565cb11191ab1e2b91cd19b630cfa8c34","3359a6c6d574d81cbf361faf65c0ede7f1fa0c4b","3c7466c1a76c6bf1d08ee3b9361b88c599257e51","abd2344f82da918c1dddb23e0dbe60cdd0e89289","e8e70b147c619c88ccac78479005a5cdf8b54079","488d2a77874ff8b98f6464df1d93d68c211f1f73","badb2fb3c8792d5b70aa27ae1ae231208ba4253f","c62d0aede7ca7d4aa19099dc646f4cc7584a00ea","0653e2ed9f683868cb4539eb8718551242834f6b"],"s2Url":"https://semanticscholar.org/paper/8935940fe5077f5862ddc5fc32ce7f396ce85a9a","s2PdfUrl":"","id":"8935940fe5077f5862ddc5fc32ce7f396ce85a9a","authors":[{"name":"Sarat Sreepathi","ids":["2052055"]},{"name":"Jitendra Kumar","ids":["1892474"]},{"name":"Richard Tran Mills","ids":["2275691"]},{"name":"Forrest M. Hoffman","ids":["7642805"]},{"name":"Vamsi Sripathi","ids":["33884461"]},{"name":"William W. Hargrove","ids":["1791950"]}],"journalName":"2017 IEEE International Conference on Cluster Computing (CLUSTER)","paperAbstract":"A proliferation of data from vast networks of remote sensing platforms (satellites, unmanned aircraft systems (UAS), airborne etc.), observational facilities (meteorological, eddy covariance etc.), state-of-the-art sensors, and simulation models offer unprecedented opportunities for scientific discovery. Unsupervised classification is a widely applied data mining approach to derive insights from such data. However, classification of very large data sets is a complex computational problem that requires efficient numerical algorithms and implementations on high performance computing (HPC) platforms. Additionally, increasing power, space, cooling and efficiency requirements has led to the deployment of hybrid supercomputing platforms with complex architectures and memory hierarchies like the Titan system at Oak Ridge National Laboratory. The advent of such accelerated computing architectures offers new challenges and opportunities for big data analytics in general and specifically, large scale cluster analysis in our case. Although there is an existing body of work on parallel cluster analysis, those approaches do not fully meet the needs imposed by the nature and size of our large data sets. Moreover, they had scaling limitations and were mostly limited to traditional distributed memory computing platforms. We present a parallel Multivariate Spatio-Temporal Clustering (MSTC) technique based on k-means cluster analysis that can target hybrid supercomputers like Titan. We developed a hybrid MPI, CUDA and OpenACC implementation that can utilize both CPU and GPU resources on computational nodes. We describe performance results on Titan that demonstrate the scalability and efficacy of our approach in processing large ecological data sets.","inCitations":[],"pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/CLUSTER.2017.88"],"title":"Parallel Multivariate Spatio-Temporal Clustering of Large Ecological Datasets on Hybrid Supercomputers","doi":"10.1109/CLUSTER.2017.88","sources":["DBLP"],"doiUrl":"https://doi.org/10.1109/CLUSTER.2017.88","venue":"2017 IEEE International Conference on Cluster Computing (CLUSTER)"},
{"entities":["Data buffer","Data compression","Memory hierarchy","Run time (program lifecycle phase)","Simulation","Solid-state drive","Solid-state electronics","Supercomputer","Wavelet","Wavelet transform"],"journalVolume":"","journalPages":"216-227","pmid":"","year":2017,"outCitations":["46db3ada0980b2d513b73fcef5383d6b6c63cffe","d07a7772a5e8b101fc93027c4ab172c5967e09d1","fdaa9a15aa97e385cc240c517e1d255ae92f4fa9","01fbae01b5cba998995180bfd03136f75c2dd4a5","03b70c782df4806b0e2bc27d6ca7085fca47d7e1","ba625fb8f294a5003a0880096695a92bc9bb843a","de4b00d227e41344edf1d7c1a9b77b7b2ef5a9bd","c750b9288ed25777e5b7129139e01c143177324c","59a902d3a87001aaf091752773e8b4679651499c","892a63a99e7c5cede6093e809e0c350da56ab8cc","962f449c4edd4ef723b48a936a168fd43c9f1ec6","44860f86dc2d5b526298890f407c8d253a621cea","3f9edf67472607344b451acf8a6f2e9cc38ae730","29b0ff8ef513b3c4ab3c8882c8f61f906eb249fa","9c0b153ea0b741107301c2a13dc0e0f2f92c863c","8832d189d0d5d3886bb2fcda809beba409af7c8e","230dfb469a5cd733eba745815cf25e968bd4ac1c","5c027b4c0ab42649eb5778b4153240184a644c95","f15ca310a4519e98185d22b3b73fd2509c6ef032","44d2c8bec79fb924f7de9507bbf63a5cfbd255ee","c6c7fb1a8da010ce9aa660c47e3399cc43fa58ab","dac1b3293224845695062b178a5caa1e76f87503","0c274baed7507a4fb400c6919069ff3191fc4d13","d38be5c5894355a9d2bb4c84b5680a7a2be89924","bbd6c87a25d320f851b24c964c3725e1428d642a","8e4dad48cbc091ac08b81d273dfd7b22b84a266f","7d68de2e79c25ec0c69b58e72cc2054343dae1ed","aa64b802ad1b89a27d8f5db849d41a33a519475f","59ba9f62728b6231f982ea3b59f9ba7422182f28","20cc5fdba0915a3958c31d7b18763e82a5418856","5ae8c2924636b3ac45ce5d272f61da0927d597ae"],"s2Url":"https://semanticscholar.org/paper/83fb6276431a40d8b3fed09eca59cbd6d8e7b307","s2PdfUrl":"","id":"83fb6276431a40d8b3fed09eca59cbd6d8e7b307","authors":[{"name":"Shaomeng Li","ids":["2944920"]},{"name":"Sudhanshu Sane","ids":["26783785"]},{"name":"Leigh Orf","ids":["2160727"]},{"name":"Pablo D. Mininni","ids":["1823535"]},{"name":"John Clyne","ids":["1773970"]},{"name":"Hank Childs","ids":["2405249"]}],"journalName":"2017 IEEE International Conference on Cluster Computing (CLUSTER)","paperAbstract":"Data reduction through compression is emerging as a promising approach to ease I/O costs for simulation codes on supercomputers. Typically, this compression is achieved by techniques that operate on individual time slices. However, as simulation codes advance in time, outputting multiple time slices as they go, the opportunity for compression incorporating the time dimension has not been extensively explored. Moreover, recent supercomputers are increasingly equipped with deeper memory hierarchies, including solid state drives and burst buffers, which creates the opportunity to temporarily store multiple time slices and then apply compression to them all at once, i.e., spatiotemporal compression. This paper explores the benefits of incorporating the time dimension into existing wavelet compression, including studying its key parameters and demonstrating its benefits in three axes: storage, accuracy, and temporal resolution. Our results demonstrate that temporal compression can improve each of these axes, and that the impact on performance for real systems, including tradeoffs in memory usage and execution time, is acceptable. We also demonstrate the benefits of spatiotemporal wavelet compression with real-world visualization use cases and tailored evaluation metrics.","inCitations":[],"pdfUrls":["http://cdux.cs.uoregon.edu/pubs/LiCluster.pdf","http://doi.ieeecomputersociety.org/10.1109/CLUSTER.2017.15"],"title":"Spatiotemporal Wavelet Compression for Visualization of Scientific Simulation Data","doi":"10.1109/CLUSTER.2017.15","sources":["DBLP"],"doiUrl":"https://doi.org/10.1109/CLUSTER.2017.15","venue":"2017 IEEE International Conference on Cluster Computing (CLUSTER)"},
{"entities":["Algorithm","Anomaly detection","Low-rank approximation","Supercomputer","Tracking system","Usage data"],"journalVolume":"","journalPages":"410-418","pmid":"","year":2017,"outCitations":["06bd1b7c539e13d30abe7e6715753b12c587058c","d9d055477bfaf383a1b99824215b8a41d0ed3bc1","62dd02837c65b9c90de8d80c493f23ce1116cb3d","bda0fb0cde6a1120c721a6caa9f58f6d049ddb7a","03b84b789cb342587db621c7e88eeb005cc21578","bd1ed675f2fb2d47b7bd9ba8d0bdf71a99699693","27c989c48f81da3a4b27ca5196d3e9ecc24b3b0c","1314e6ea34a8d749ca6190a0d2dd00b3a1879cc6","a20def90994cab53b1e5202147848bb5bd4891a4","0368011142340e9ea904a3c022412f02a8e60f02","f0ba9dfcc0d3de1c1c941c9d42435350ed662557","27d864dc2708a2804cac20606a8195c5313eed5c","6ccf50496e73a69535f50262bd3dba7548677fff","2d1c16f01c89136e171d5421ec303a695de7748b","0ec40760465c0e7cb0f5d25179d1e8b44049a1e3","66479c2251088dae51c228341c26164f21250593","9fdde5b04351bde57e805da89fb6ef93cc1a5aa2","502ff2f9220ebc8c3544e6c4a005e819429ab716","12d4c92f0a3a70538ed609bf6f7b603e44d11abd","33282ae3f2929d70d37234e763f40bac5bbbbc6f","881e0395816f8e8518ae157f3c21898b18cff1d8"],"s2Url":"https://semanticscholar.org/paper/326395e317f1b34926a1ec2482a7fde98ab12869","s2PdfUrl":"","id":"326395e317f1b34926a1ec2482a7fde98ab12869","authors":[{"name":"Niyazi Sorkunlu","ids":["1821026"]},{"name":"Varun Chandola","ids":["3332891"]},{"name":"Abani K. Patra","ids":["34806140"]}],"journalName":"2017 IEEE International Conference on Cluster Computing (CLUSTER)","paperAbstract":"Resource usage data, collected using tools such as TACC_Stats, capture the resource utilization by nodes within a high performance computing system. We present methods to analyze the resource usage data to understand the system performance and identify performance anomalies. The core idea is to model the data as a three-way tensor corresponding to the compute nodes, usage metrics, and time. Using the reconstruction error between the original tensor and the tensor reconstructed from a low rank tensor decomposition, as a scalar performance metric, enables us to monitor the performance of the system in an online fashion. This error statistic is then used for anomaly detection that relies on the assumption that the normal/routine behavior of the system can be captured using a low rank approximation of the original tensor. We evaluate the performance of the algorithm using information gathered from system logs and show that the performance anomalies identified by the proposed method correlates with critical errors reported in the system logs. Results are shown for data collected for 2013 from the Lonestar4 system at the Texas Advanced Computing Center (TACC).","inCitations":["83ef14979ab56a6155ee3dc6bf9fbe7de4c7a4ee"],"pdfUrls":["https://arxiv.org/pdf/1705.10756v1.pdf","http://doi.ieeecomputersociety.org/10.1109/CLUSTER.2017.70","http://arxiv.org/abs/1705.10756"],"title":"Tracking System Behavior from Resource Usage Data","doi":"10.1109/CLUSTER.2017.70","sources":["DBLP"],"doiUrl":"https://doi.org/10.1109/CLUSTER.2017.70","venue":"2017 IEEE International Conference on Cluster Computing (CLUSTER)"},
{"entities":["Backup","Memory protection","Smart Data Compression"],"journalVolume":"","journalPages":"603-607","pmid":"","year":2017,"outCitations":["741a04ef3a0c3953a3d37726bf4d6170eaa68a55","37e95e584af6ce7958d8cf3d5f96dcab0f595cba","16abd837f7fdaa4215ff69852963fef25c0a1bad","64d60b5ea0cb696837fc6001563d070f98f3da4b","19d686007a37f599b850bfbca391a5d7d869def8","0f1cb72117c29da9d1840311dee349fd88c52342","b23f060a4574ff126e98b8fe13f8b508b9f82c1f","4ee0564e83d0252c461087f7fd5963a01716e142","3e99a917b9a4e89497541bbc3bb72079054644c6","6b8b4763d1aea3f9d083f364c841737daab8db67","694d06bb3ff03fb6ff42b7891a42f8d4f3f37f34","14e5bbf94dba58ead368cceab1541cff7cbb0170","dd286cdefbca8f6e435298f058ca413d131f53b0","a19563b4014919c405964cea5271bebe918ad265","108c840d5d1847948a2de0250490a327ae069ee6","747ad718761b7d848a12e4f3a82aa0f46117a815","18fe996c6f43a8f301cd842507045b679ba3506a","34f310dffd51a8f1585b0a6a5ccaf83094d0d663","270c88be02c3c996b652b5410a49f63a2abd7687","b39b8b5be74498b90ae59297a6883e3fd57b1eb8","455d253c61379bce5626fba8ef9897d3ac1307dc","36480300b1e382c062b78c6bd610d1879efd950e"],"s2Url":"https://semanticscholar.org/paper/ec997fd7517daa7d68a6ef66e758c2dd8cee077b","s2PdfUrl":"","id":"ec997fd7517daa7d68a6ef66e758c2dd8cee077b","authors":[{"name":"Scott Levy","ids":["40448030"]},{"name":"Kurt B. Ferreira","ids":["1734561"]},{"name":"Patrick G. Bridges","ids":["23308884"]}],"journalName":"2017 IEEE International Conference on Cluster Computing (CLUSTER)","paperAbstract":"Aggregating millions of hardware components to construct an exascale computing platform will pose significant resilience challenges. In addition to slowdowns associated with detected errors, silent errors are likely to further degrade application performance. Moreover, silent data corruption (SDC) has the potential to undermine the integrity of the results produced by important scientific applications.In this paper, we propose an application-independent mechanism to efficiently detect and correct SDC in read-mostly memory, where SDC may be most likely to occur. We use memory protection mechanisms to maintain compressed backups of application memory. We detect SDC by identifying changes in memory contents that occur without explicit write operations. We demonstrate that, for several applications, our approach can potentially protect a significant fraction of application memory pages from SDC with modest overheads. Moreover, our proposed technique can be straightforwardly combined with many other approaches to provide a significant bulwark against SDC.","inCitations":[],"pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/CLUSTER.2017.99"],"title":"Evaluating the Viability of Using Compression to Mitigate Silent Corruption of Read-Mostly Application Data","doi":"10.1109/CLUSTER.2017.99","sources":["DBLP"],"doiUrl":"https://doi.org/10.1109/CLUSTER.2017.99","venue":"2017 IEEE International Conference on Cluster Computing (CLUSTER)"},
{"entities":["Algorithm","Analysis of algorithms","Benchmark (computing)","Bottleneck (software)","Computer data storage","Cray XC40","Data aggregation","Job control (Unix)","Lustre","Memory-mapped I/O","Message Passing Interface","Multiple buffering","Network topology","Simulation","Supercomputer","Two-phase commit protocol"],"journalVolume":"","journalPages":"70-80","pmid":"","year":2017,"outCitations":["2da4ab6c02d97fe47b589ddd450a5c41f2b47bb9","6ebc0c4e3e14a644a71ec7db11707e917693ffcb","ebc09b04a900afc6c3cf53a4b7ff6035f33f02b2","e1f02518a257af5af200b77e0518cfe6c6f437f8","a14e1d1d3eea6803ac34b904a4c619f8f686370c","25d5f7757ebd0b7a5cde7bf64c83ad0020318f39","2a11832bb798de3315838c327bdcec6493cd2a5c","4b0db76df9148e80806e7b45b13e85ee54cd5b6c","171450cd7ed50d4c50955949c87df08bbb115549","55224d0eedb75cde4474667ad01417eb502b05cd","0a564c5117375287c60d3a27a96003f30396f62f","cc5e8e2b073c41983a76e38183f89ea724307175","36d76c6079f993685354edb10d68df971d7da519","c0c56908d343d52669e1aee072dd611681dc831f","2d60d3596490d9999d8433bf41405060779bc11d","409ed5839cf6d0ba246d91f82d1ac33cbe600c27","9ca4fb478cfa38ed8a490bcb361dd7631aa3af37","07f5e78517d0baf8c64ab7b6461cb33a2a1bceed","f8779e0694bd5f37fda4e8d06572056d09d13d51","176d712d084112b2e65e385e8220e4679c24f28a","8b334741506521040da36c23982d071f1e4143da"],"s2Url":"https://semanticscholar.org/paper/6c6baac8a7e3ea43cf875da9cafd6f1f06d0cea6","s2PdfUrl":"","id":"6c6baac8a7e3ea43cf875da9cafd6f1f06d0cea6","authors":[{"name":"Francois Tessier","ids":["2965669"]},{"name":"Venkatram Vishwanath","ids":["3348747"]},{"name":"Emmanuel Jeannot","ids":["1795494"]}],"journalName":"2017 IEEE International Conference on Cluster Computing (CLUSTER)","paperAbstract":"Reading and writing data efficiently from storage system is necessary for most scientific simulations to achieve good performance at scale. Many software solutions have been developed to decrease the I/O bottleneck. One well-known strategy, in the context of collective I/O operations, is the two-phase I/O scheme. This strategy consists of selecting a subset of processes to aggregate contiguous pieces of data before performing reads/writes. In this paper, we present TAPIOCA, an MPI-based library implementing an efficient topology-aware two-phase I/O algorithm. We show how TAPIOCA can take advantage of double-buffering and one-sided communication to reduce as much as possible the idle time during data aggregation. We also introduce our cost model leading to a topology-aware aggregator placement optimizing the movements of data. We validate our approach at large scale on two leadership-class supercomputers: Mira (IBM BG/Q) and Theta (Cray XC40). We present the results obtained with TAPIOCA on a micro-benchmark and the I/O kernel of a large-scale simulation. On both architectures, we show a substantial improvement of I/O performance compared with the default MPI I/O implementation. On BG/Q+GPFS, for instance, our algorithm leads to a performance improvement by a factor of twelve while on the Cray XC40 system associated with a Lustre filesystem, we achieve an improvement of four.","inCitations":[],"pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/CLUSTER.2017.80"],"title":"TAPIOCA: An I/O Library for Optimized Topology-Aware Data Aggregation on Large-Scale Supercomputers","doi":"10.1109/CLUSTER.2017.80","sources":["DBLP"],"doiUrl":"https://doi.org/10.1109/CLUSTER.2017.80","venue":"2017 IEEE International Conference on Cluster Computing (CLUSTER)"},
{"entities":["Computation","Error detection and correction","Experiment","Gaussian process","Kriging","Soft error","Solid-state drive","Supercomputer"],"journalVolume":"","journalPages":"608-612","pmid":"","year":2017,"outCitations":["270c88be02c3c996b652b5410a49f63a2abd7687","7027eb880ee4d5a1f71bfc861bb36ae980f781fc","dd998158ec990f66282758bd4d3c093ac1e81937","5d5ac7167bc5f834173aa4c63821916a1bdf487a","dd286cdefbca8f6e435298f058ca413d131f53b0","51ae243a89cb1f12ddc25eec902cae134913f6ba","91a26971ed09692b026e5db76f05e88cfad5b549","b23f060a4574ff126e98b8fe13f8b508b9f82c1f","a61c25b0f50e4886381b7a083d27fb5b4d101317","b8d5cc73054874b49c4ee1033717678719440145","108c840d5d1847948a2de0250490a327ae069ee6","4ee0564e83d0252c461087f7fd5963a01716e142","14e5b814d398d2c7e9ed78d12a2c286c733116f5","36480300b1e382c062b78c6bd610d1879efd950e","990e1ff6477328ca6cfb57c27e563d8f04c6b411"],"s2Url":"https://semanticscholar.org/paper/4ac3157a7cd44d268f33e96c9141e9f2c99f139c","s2PdfUrl":"","id":"4ac3157a7cd44d268f33e96c9141e9f2c99f139c","authors":[{"name":"Omer Subasi","ids":["1773557"]},{"name":"Sriram Krishnamoorthy","ids":["1679176"]}],"journalName":"2017 IEEE International Conference on Cluster Computing (CLUSTER)","paperAbstract":"In this paper, we present a non-parametric dataanalytic soft-error detector. Our detector uses the key properties of Gaussian process regression. First, because Gaussian process regression provides confidence on the prediction, this confidence can be used to automatize construction of the detection range. Second, because the correlation model of a Gaussian process captures the similarity among neighboring point values, only one-time online training is needed. This leads to very low online performance overheads. Finally, Gaussian process regression localizes the detection range computation, thereby avoiding communication costs. We compare our detector with the adaptive impact-driven (AID) and spatial supportvector- machine (SSD) detectors, two effective detectors based on observation of the temporal and spatial evolution of data, respectively. Experiments with five failure distributions and six real-world high-performance computing applications reveal that the Gaussian-process-based detector achieves low false positive rate and high recall while incurring less than 0.1% performance and memory overheads. Considering the detection performance and overheads, our Gaussian process detector provides the best trade-off.","inCitations":[],"pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/CLUSTER.2017.129"],"title":"A Gaussian Process Approach for Effective Soft Error Detection","doi":"10.1109/CLUSTER.2017.129","sources":["DBLP"],"doiUrl":"https://doi.org/10.1109/CLUSTER.2017.129","venue":"2017 IEEE International Conference on Cluster Computing (CLUSTER)"},
{"entities":["Binary code","Experiment","Parsing","Performance prediction","Performance tuning","Program analysis","Software development","Static program analysis","Supercomputer","Usability"],"journalVolume":"","journalPages":"103-113","pmid":"","year":2017,"outCitations":["138a4fa853bd6b02052085fffbaacab38ee1473a","9d30381c49afa033eacc04fb68975762eb7bafab","6800600e6451d0bf0a4e866a483cac8c8617da88","1657da35049f5ade21c169b274c3c0adee288f46","51ec4530e2b6d73f410568952db220c05865e073","f0f4757aa2f923a349e8357e73850a78e9b80fee","3218bbfd89deae4134d6c6d7f8f3ceb5c3a361f7","fef1056a69be6f597b4866bc3ee306bf01a4df0d","60d4b2c4d9630e4905748e7d3565a013d2304906","15708276fbb98a7d3f8835a2c51cb522eeab9967","42be8c9380613754c82782ae86291d3c379f2ead","0a89b4a34c86cb65689f5e79a88fd4bc7c8f63c5","d32d4ff33b1b2665d6081194eb6acdc3c7dd6891","a0b1b8ee4a9e6ae68ce6a712ad0a66ddb4a12117","0e95e0ff4014053ac11fcbcee556eaab4dc1a92d","092217c2267f6e0673590aa151d811e579ff7760","b04391910d19d2d0c64b62d300927f527417414e","c3745b88bff2e49543056666816d130e9d2d1baa","1f2ff98f9413bb36c641e9edcfa79f7b33eeb80a","3dc60796f2b423dfd089f79e54bd6caf764a41e6","91e413ece54c911087295f1d9c06397e961e361f","91607d7bc71823360de59b894ae37b4f1738bca0"],"s2Url":"https://semanticscholar.org/paper/a6a906ae9727a33409eaa207253a6ad32a871c11","s2PdfUrl":"","id":"a6a906ae9727a33409eaa207253a6ad32a871c11","authors":[{"name":"Kewen Meng","ids":["12418191"]},{"name":"Boyana Norris","ids":["1763308"]}],"journalName":"2017 IEEE International Conference on Cluster Computing (CLUSTER)","paperAbstract":"The performance model of an application can provide understanding about its runtime behavior on particular hardware. Such information can be analyzed by developers for performance tuning. However, model building and analyzing is frequently ignored during software development until performance problems arise because they require significant expertise and can involve many time-consuming application runs. In this paper, we propose a fast, accurate, flexible and user-friendly tool, Mira, for generating performance models by applying static program analysis, targeting scientific applications running on supercomputers. We parse both the source code and binary to estimate performance attributes with better accuracy than considering just source or just binary code. Because our analysis is static, the target program does not need to be executed on the target architecture, which enables users to perform analysis on available machines instead of conducting expensive experiments on potentially expensive resources. Moreover, statically generated models enable performance prediction on nonexistent or unavailable architectures. In addition to flexibility, because model generation time is significantly reduced compared to dynamic analysis approaches, our method is suitable for rapid application performance analysis and improvement. We present empirical validation results to demonstrate the current capabilities of our approach on small benchmarks and a mini application.","inCitations":[],"pdfUrls":["https://arxiv.org/pdf/1705.07575v1.pdf","http://arxiv.org/abs/1705.07575","http://doi.ieeecomputersociety.org/10.1109/CLUSTER.2017.43"],"title":"Mira: A Framework for Static Performance Analysis","doi":"10.1109/CLUSTER.2017.43","sources":["DBLP"],"doiUrl":"https://doi.org/10.1109/CLUSTER.2017.43","venue":"2017 IEEE International Conference on Cluster Computing (CLUSTER)"},
{"entities":["CUDA","Distributed memory","Graphics processing unit","Lattice QCD","Message Passing Interface","OpenACC","Programmer","Programming complexity","Quantum mechanics"],"journalVolume":"","journalPages":"429-438","pmid":"","year":2017,"outCitations":["21be843f22e313bac6d1dc19fc53535f9b413033","13f3f2b23e733a0297beaf4e70375134c30113b0","3bda9a412d20fdd74ef96a845adfa800d2aa7d9a","780af8ae12ebb6be7b737ec17cc8357bb090b84e","5048b1199db383beda869e742691c22ca15e1d56","368226068a865b5ebfe1590b47a368558e6af36f","47808cb20e9cc0d2d873d5b7eed936e2e77fe691","9e490640f84581bd8a63b785e16ebdd2649a32be","2f3c8e4bb0a738aecfad51bdff7dc75a4fea28fd","28e32f4ec3395476a6885149c1d73ff3d0edbb46","f61f7446ad75776cc7b2f2b1fb8fd184e3c455f5","49c866296354ab54c42d234645cf0700ff4a7315","b16d97845cae75606faa59aa69552ec426f01d1e","38c48d4a31ab050c6e750cdae21e00421172f694","aca67206447039a63bd3bed50381003cd10ac882"],"s2Url":"https://semanticscholar.org/paper/33595f1ada823f49766b981b546214f60ad14b4f","s2PdfUrl":"","id":"33595f1ada823f49766b981b546214f60ad14b4f","authors":[{"name":"Masahiro Nakao","ids":["2814225"]},{"name":"Hitoshi Murai","ids":["2575510"]},{"name":"Hidetoshi Iwashita","ids":["35486615"]},{"name":"Akihiro Tabuchi","ids":["1814141"]},{"name":"Taisuke Boku","ids":["2616076"]},{"name":"Mitsuhisa Sato","ids":["1744801"]}],"journalName":"2017 IEEE International Conference on Cluster Computing (CLUSTER)","paperAbstract":"Accelerated clusters, which are distributed memory systems equipped with accelerators, have been used in various fields. For accelerated clusters, programmers often implement their applications by a combination of MPI and CUDA (MPI+CUDA). However, the approach faces programming complexity issues. This paper introduces the XcalableACC (XACC) language, which is a hybrid model of XcalableMP (XMP) and OpenACC. While XMP is a directive-based language for distributed memory systems, OpenACC is also a directive-based language for accelerators. XACC enables programmers to develop applications on accelerated clusters with ease. To evaluate XACC performance and productivity levels, we implemented a lattice quantum chromodynamics (Lattice QCD) application using XACC on 64 compute nodes and 256 GPUs and found its performance was almost the same as that of MPI+CUDA. Moreover, we found that XACC requires much less change from the serial Lattice QCD code than MPI+CUDA to implement the parallel Lattice QCD code.","inCitations":["e9a79a290f0ca04507c1e909dd6ba93746136f8f","a164d6544b48c8c1dcdd3773064fdcaf828879b3"],"pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/CLUSTER.2017.58"],"title":"Implementing Lattice QCD Application with XcalableACC Language on Accelerated Cluster","doi":"10.1109/CLUSTER.2017.58","sources":["DBLP"],"doiUrl":"https://doi.org/10.1109/CLUSTER.2017.58","venue":"2017 IEEE International Conference on Cluster Computing (CLUSTER)"},
{"entities":["ACID","Commodity computing","Concurrency (computer science)","Concurrency control","Data store","Database","Materialized view","NewSQL","NoSQL","Qualitative comparative analysis","Scalability","Synergy"],"journalVolume":"","journalPages":"384-388","pmid":"","year":2017,"outCitations":["0538e05e1ced11b91cda5d1aed88a73969def882","9aa0d7253574e50fe3a190ccd924433f048997dd","33457f49553d918e912c2d8c54b81f4fd8a4c234","18a5f443299784479e78d9e77f175af57cb2fa2b","2b300024ac736f7181f6d35392ec3a65f49457bd","412a9e54bbb31e12d008a9579994e009c5b40b46","2321a150c84d771d81fd81759757795dcda25750","0c97fa96d179dec4f5a9349c4e5203205d427fb8","6c523c74fb0b99440982375608d880fdb2752d21","07d847f310d5fa9138f461f0a25c5e0024f1c4af","363116c764453d9b740c46d23b1f5a3c5801d76e","15caf02eadcd5b043e795a60853010c03ebc9246","10723678d19bab6a52c8ee9b89f9118536044033","0dafdc7debdcae528b2549489a03509cb4ecb9fe","d6cd585cdc5bdaf55dffc2752d3c2a086cfc27b1"],"s2Url":"https://semanticscholar.org/paper/6ecfc6ef1d571637cdd02aae709464dffb268ac8","s2PdfUrl":"","id":"6ecfc6ef1d571637cdd02aae709464dffb268ac8","authors":[{"name":"Ashish Tapdiya","ids":["2786856"]},{"name":"Yuan Xue","ids":["1702786"]},{"name":"Daniel Fabbri","ids":["3025858"]}],"journalName":"2017 IEEE International Conference on Cluster Computing (CLUSTER)","paperAbstract":"Relational databases are well suited for vertical scaling; however, specialized hardware can be expensive. Conversely, NewSQL and NoSQL data stores are designed to scale horizontally. NewSQL databases provide ACID transaction support; however, joins are limited to the partition keys, resulting in restricted query expressiveness. On the other hand, NoSQL databases are designed to scale out on commodity hardware; however, they are limited by slow join performance. Hence, we consider if the NoSQL join performance can be improved while ensuring ACID semantics and without drastically sacrificing write performance, disk utilization and query expressiveness.This paper presents the Synergy system that leverages schema and workload driven mechanisms to identify materialized views, and a specialized concurrency control system on top of a NoSQL database to enable scalable data management with familiar relational conventions. Synergy trades slight write performance degradation and increased disk utilization for faster join performance (compared to standard NoSQL databases) and improved query expressiveness (compared to NewSQL databases).","inCitations":[],"pdfUrls":["https://arxiv.org/pdf/1710.01792v1.pdf","http://doi.ieeecomputersociety.org/10.1109/CLUSTER.2017.79","http://arxiv.org/abs/1710.01792"],"title":"A Comparative Analysis of Materialized Views Selection and Concurrency Control Mechanisms in NoSQL Databases","doi":"10.1109/CLUSTER.2017.79","sources":["DBLP"],"doiUrl":"https://doi.org/10.1109/CLUSTER.2017.79","venue":"2017 IEEE International Conference on Cluster Computing (CLUSTER)"},
{"entities":["Analysis of algorithms","Cache (computing)","Instruction pipelining","Matrix multiplication","Memory hierarchy","OpenMP","Runtime system","Scalability","Shared memory","Video tracking"],"journalVolume":"","journalPages":"389-399","pmid":"","year":2017,"outCitations":["04717b51d7c7dd598d732fb8a293d69d45c0fa9f","e2040ba29ebab0e5ef48d9edb857378bc13b4f8c","3245b047c145272e16b097ba21939c80ffead168","917fc743f23a795cf86d65da9f20b3f67dbbb7dd","087bc3160eafd51c513ea677159e34b111b4615e","6bfc68c836d92d5ec8d9f5e71336e7809c70f367","80d4a300e7b8bf77533a9be0ddd0de09470488bd","d57212cbbc3cd44179b782256173149aaeb5b1b9","bc681b0350757ee8aecc9703e44251e4c6ee56fd","9ee8943866b9e7771f957dd5721128fe6afedbf5","927808939090d881a56fdbec28e7f8341f996c4f","7421d28428e041c271fe6370c331353f4a3fa974","10b71b1e95db11c0ad0429a1d2c75811573106db","7d76ba8c4f6776c645673e2c3f6eb88b1a0ca7aa","688a0e25527122c65f983cc65be3196b75d6b66a"],"s2Url":"https://semanticscholar.org/paper/3d860e0e8240fca52f7b28f8b82c3ba0bb2868ae","s2PdfUrl":"","id":"3d860e0e8240fca52f7b28f8b82c3ba0bb2868ae","authors":[{"name":"Jens Gustedt","ids":["3124331"]},{"name":"Emmanuel Jeannot","ids":["1795494"]},{"name":"Farouk Mansouri","ids":["2756384"]}],"journalName":"2017 IEEE International Conference on Cluster Computing (CLUSTER)","paperAbstract":"Efficiently programming shared-memory machines is a difficult challenge because mapping application threads onto the memory hierarchy has a strong impact on the performance. However, optimizing such thread placement is difficult: architectures become increasingly complex and application behavior changes with implementations and input parameters, e.g problem size and number of threads. In this work, we propose a fully automatic, abstracted and portable affinity module. It produces and implements an optimized affinity strategy that combines knowledge about application characteristics and the platform topology. Implemented in theback-end of our runtime system (ORWL), our approach was used to enhance the performance and the scalability of several unmodified ORWL-coded applications: matrix multiplication, a 2D stencil (Livermore Kernel 23), and a video tracking real world application. On two SMP machines with quite different hardware characteristics, our tests show spectacular performance improvements for these unmodified application codes due to a dramatic decrease of cache misses and pipeline stalls. A comparison to reference implementations using OpenMP confirms this performance gain of almost one order of magnitude.","inCitations":[],"pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/CLUSTER.2017.71"],"title":"Automatic, Abstracted and Portable Topology-Aware Thread Placement","doi":"10.1109/CLUSTER.2017.71","sources":["DBLP"],"doiUrl":"https://doi.org/10.1109/CLUSTER.2017.71","venue":"2017 IEEE International Conference on Cluster Computing (CLUSTER)"},
{"entities":["Artificial neural network","Context switch","Critical system","Experiment","Hypervisor","Interrupt","Memory management","Mutual exclusion","Performance prediction","Run time (program lifecycle phase)","Semiconductor consolidation","Server (computing)","Virtual machine"],"journalVolume":"","journalPages":"557-562","pmid":"","year":2017,"outCitations":["35e7b16b618ca6bf63446372afb2a0ca071f2f13","58fc1c39da88a8a90e22c7b07fd2339b1a716ee4","332f77fd05703c1607e3b57884ad31fb1fad0104","dbcdb4c402756b2b5ac910b9eb17ddb412290d16","32a7c5e10a09b5532d56af50ef2f60d9776cc56a","2461db22ab3ebe9eaf8e23b42ab2449bbf06d721","4b7b45aa74d84f5b86ef3d8bc8bf460602e97d38","0df1f6a4bd337bda593f0a9dc120841ce933a1dc","1f612acad3f98f4f63a01e6d8632e50a0aef4257","3af5d2164fdbcbb47f64044e62445ed5dec0c245","0558c94a094158ecd64f0d5014d3d9668054fb97","51e878ed0979919041030f871f6e34531ca39750","3c4ae51452823afafabe8d33d51218d1d95c2795","141099ae8ea1e8c76d30e3a97df389de6a07890c","5fe4eb1749a823469950456a123c77530e33ad73","48ea9605ac31a13b5b9fc81b5311b51384fdb3e1","0541d5338adc48276b3b8cd3a141d799e2d40150","477b56dd761d802805dc984afb39363a51579975","110c050c6c992d2b956f7b47d717810ac5c91bdc","01dca7c7612aa71e5da87087c97a8dfffe94d43b","14b8cbbaf08d1f00145f83c0270833a03e434af3","0de0c3240bda7972bd0a3c8369ebc4b4f2e4f9c2","c7bb9a172b477f180ed330619f2fab3d1d54a9ea","aeb982a2bf63181e9f440e22bd015afd143ce9cb"],"s2Url":"https://semanticscholar.org/paper/e84e857c48f4c0e4438441610b08ff665d912009","s2PdfUrl":"","id":"e84e857c48f4c0e4438441610b08ff665d912009","authors":[{"name":"Maruf Ahmed","ids":["34972007"]},{"name":"Albert Y. Zomaya","ids":["9392149"]}],"journalName":"2017 IEEE International Conference on Cluster Computing (CLUSTER)","paperAbstract":"Virtual machine (VM) consolidation is necessary for increasing the server utilization; however, it also leads to VM performance degradation. This work presents a method to predict the consolidated VMs performance from the critical system events data. Experiments are designed to demonstrate the effect of system events like interrupts, page faults, mutex operations, and context switching on the consolidated VMs. Results show that the host server counters are not reliable for such predictions. On the other hand, the coupling of the task execution time with the VM system events is an effective way to predict the consolidation performance. Results further show that the VM memory allocation plays an important part in the consolidated tasks performance. The system event data is also used to train an Artificial Neural Network (ANN) for performance prediction on three hypervisors; ESXi, Xen, and XenServer and similar results are observed in all three.","inCitations":[],"pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/CLUSTER.2017.20"],"title":"The Effect of Resource Allocation and System Events on VM Consolidation","doi":"10.1109/CLUSTER.2017.20","sources":["DBLP"],"doiUrl":"https://doi.org/10.1109/CLUSTER.2017.20","venue":"2017 IEEE International Conference on Cluster Computing (CLUSTER)"},
{"entities":["Catastrophic interference","Data aggregation","Interference (communication)","Network congestion","Network performance","Network topology","Next-generation network","Routing","Scalability","Simulation","Tree network","Utility","Visual analytics"],"journalVolume":"","journalPages":"193-203","pmid":"","year":2017,"outCitations":["3c3fcd7a025f50bf598de03d41bc0fd00660f11f","9d30381c49afa033eacc04fb68975762eb7bafab","251544e7c508771ab34cb2d6b97800960cde1f1e","8fc0623a28cc193927cd012bd8daac5e6cad75d3","5f8991828def57d2f0cda942566afff56740d150","dc39c68a00e38f2993b450eb01c96e1d032ab850","56d1d13d74407f25516ab28140256f2a67cc1b4d","9c4b6c885bfc6038cdac56763663880e0f2624e6","b29373bf0d480561d668f302fe447de4d7c9a405","86dd6cffcb498c282c22966507fb533ae8901dd6","663e064469ad91e6bda345d216504b4c868f537b","83f2087f3c602d043277927380e35885879210f5","0417ed50c871b0f41782705112eea8936241ec91","3c3abe3d8519b3637223ab9e518b459d5218a903","c39c26d510c1a965c5f132edc989a598ca92b700","cf44c98bc058e690d2434efe3355995aaf6d03c9","0736d68aad2c198a8f6dda851c27bd180421c2aa","ece84a462853368c21a570a73707d857bf0e10b4","8cf9e252c8314e26f20b619acb6392d52abac647","04232e5f25ed9df85dd755dd991baae5caeecd2a","5c8f8cc934949901555c80777488eeff1596dc1a","3ec4cf958f6ee00dc00aa14840c96268c4c3f9c9","15c0ff1ace0798e8ef9767a76f90a32d1ee3ee8b","d6be948f6efd5960f6a65f3b56524011e2a411e7","89199b87c710d654c7285afa0eab5c88ee0427ff","a15bc58fa496b6cca937713723f19f45380fc2fe","582389c37f27dd69b39e949257f7fe83a6fee8d9","b040e7de49f4b4e8e9e007d7e4149d7ef277c609","09a941005b6b60d1cf013f992a6012b9d2e41b47","3834b27a7c684afabf4464a15cb5d3a0c4d4918d","f0982dfd3071d33296c22a4c38343887dd5b2a9b","18fbcb1de113f5d60c8e81566231a0ecea46f3fe"],"s2Url":"https://semanticscholar.org/paper/ab9d01de8bb09ece4e4097c735bf0d6079ae5077","s2PdfUrl":"","id":"ab9d01de8bb09ece4e4097c735bf0d6079ae5077","authors":[{"name":"Jianping Kelvin Li","ids":["8549365"]},{"name":"Misbah Mubarak","ids":["2383364"]},{"name":"Robert B. Ross","ids":["40211322"]},{"name":"Christopher D. Carothers","ids":["1759102"]},{"name":"Kwan-Liu Ma","ids":["1707383"]}],"journalName":"2017 IEEE International Conference on Cluster Computing (CLUSTER)","paperAbstract":"High-radix, low-diameter, hierarchical networks based on the Dragonfly topology are common picks for building next generation HPC systems. However, effective tools are lacking for analyzing the network performance and exploring the design choices for such emerging networks at scale. In this paper, we present visual analytics methods that couple data aggregation techniques with interactive visualizations for analyzing large-scale Dragonfly networks. We create an interactive visual analytics system based on these techniques. To facilitate effective analysis and exploration of network behaviors, our system provides intuitive, scalable visualizations that can be customized to show various traffic characteristics and correlate between different performance metrics. Using high-fidelity network simulation and HPC applications communication traces, we demonstrate the usefulness of our system with several case studies on exploring network behaviors at scale with different workloads, routing strategies, and job placement policies. Our simulations and visualizations provide valuable insights for mitigating network congestion and inter-job interference.","inCitations":[],"pdfUrls":["http://www.mcs.anl.gov/papers/P7079-0717.pdf","http://doi.ieeecomputersociety.org/10.1109/CLUSTER.2017.26"],"title":"Visual Analytics Techniques for Exploring the Design Space of Large-Scale High-Radix Networks","doi":"10.1109/CLUSTER.2017.26","sources":["DBLP"],"doiUrl":"https://doi.org/10.1109/CLUSTER.2017.26","venue":"2017 IEEE International Conference on Cluster Computing (CLUSTER)"},
{"entities":["Automatic parallelization","Central processing unit","Compiler","GNU Compiler Collection","Infinite loop","Parallel computing","Polyhedron","Programming language","The C Programming Language","Toolchain"],"journalVolume":"","journalPages":"552-556","pmid":"","year":2017,"outCitations":["4baccc40c32cffbbe7d7235e72c23ec9c4ad22c7","1bb4d630e8288968c3b0193691a53fe36987352a","f93860914f723d0d48c9fd6bef515d75c3c9715e","16de6f9e2bf6ee1068dbca8c9e5446295c904315","61c6377d5437abb0eed21c8cadcceffd4b8c49a5","f0f4757aa2f923a349e8357e73850a78e9b80fee","a0c7dc8201c0813203b2960a90a82bacff236717","60d4b2c4d9630e4905748e7d3565a013d2304906","0f4855901a89813cf39104293f086214200e5421","0606b9b2bedb67039eb1615f8ef5b13f42f8339a","9bf03c62f747d78fd01ee917725d4c411c021d12","151d002425a85b04eb76f1b485461020a04cefc4","4d603c30e3db8f8a0a17c1fec57b9ad1fa957c1d","f2335cee748590d17864c261cba791390a947f2e","23affb01412312341fb336943756800c0bf2468c","18e87fb45e3d501577724c4ed6a9f6d2f753c8e1","afe199df9fcc5e3c4f39b380da74659b2c106ee9","0125bdc8819ac8951d811efaa0914fc7ecb83a78","6efa98d44b48a44394dbc90b9c67abc4b11c738c","91e413ece54c911087295f1d9c06397e961e361f","d6f883b74472da070df99475e41add97be23b901","32ef8d891edde06cc01357fa5c4d1ab7fe631720"],"s2Url":"https://semanticscholar.org/paper/0cd2dd864b17f17daba6096fcdccc7cec0a68e52","s2PdfUrl":"","id":"0cd2dd864b17f17daba6096fcdccc7cec0a68e52","authors":[{"name":"Tim Süß","ids":["1783053"]},{"name":"Lars Nagel","ids":["2107074"]},{"name":"Marc-Andre Vef","ids":["26383371"]},{"name":"André Brinkmann","ids":["1726087"]},{"name":"Dustin Feld","ids":["17631382"]},{"name":"Thomas Soddemann","ids":["1767942"]}],"journalName":"2017 IEEE International Conference on Cluster Computing (CLUSTER)","paperAbstract":"The need for parallel task execution has been steadily growing in recent years since manufacturers mainly improve processor performance by scaling the number of installed cores instead of the frequency of processors. To make use of this potential, an essential technique to increase the parallelism of a program is to parallelize loops. However, a main restriction of available tools for automatic loop parallelization is that the loops often have to be 'polyhedral' and that it is, e.g., not allowed to call functions from within the loops.In this paper, we present a seemingly simple extension to the C programming language which marks functions without side-effects. These functions can then basically be ignored when checking the parallelization opportunities for polyhedral loops. We extended the GCC compiler toolchain accordingly and evaluated several real-world applications showing that our extension helps to identify additional parallelization chances and, thus, to significantly enhance the performance of applications.","inCitations":[],"pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/CLUSTER.2017.32"],"title":"Pure Functions in C: A Small Keyword for Automatic Parallelization","doi":"10.1109/CLUSTER.2017.32","sources":["DBLP"],"doiUrl":"https://doi.org/10.1109/CLUSTER.2017.32","venue":"2017 IEEE International Conference on Cluster Computing (CLUSTER)"},
{"entities":["Addressing mode","Benchmark (computing)","Computer memory","Graphics processing unit","Mathematical model","Memory hierarchy","Performance prediction","Queuing delay","Replay attack","X86"],"journalVolume":"","journalPages":"166-177","pmid":"","year":2017,"outCitations":["2c2fa29dfbbab106f1b94cdfdc67939d3355bd30","1eb3992563b7b9fbf0c1da57d62f47220e6af5d5","34e2b75fd5717029fc9da92dd6160eb6e2d19ad9","0ef82bbfdec840663026dc2fa9e3db111add7efa","c3c244e6a07810e738c8eb3c10d652b7da0267d6","04ec5964a08a2ad62a30fea1fb9eff1e484a4524","25296a1ee792b8709e037aa3da7ea156c41f5ccf","00156e79606084497789662dfaf59c3b54a10722","35b1b5a69d7882053aa35e7463ceb903733a2cce","85398d5f19157c91bf00da3d36210e72d57887e4","339b09ff3d328d5ec6542a6eaa57045d4fe61c5c","5ab997b6ddb66fde2c9ea0cffcb6b869e2b74409","2bedfffe72f27ab4c8ac67d2c17bda4ea23df748","a5eb8900450908f3e245c3740420af4cb2348ef8","75489469044a0bd8dd6a2d785873b48128bbf845","2a660e81e6501ec3489d962fe87448ecf277237f","2b85f7d3cc58dad3cf913e4a85b4e7108dc2ebbc","23177452df15b652dd54a59324502b92c99687a7","55220bc99ffe36591a4b31a2ee9e40620381e0ca","702ad24deb683795e03ff1a79e96afb73cb4d988","1401df37cc3fc78f26570d601fd123f17646b2d2","2d6f002477015469075954c6748a1a85af352c94","2954071739e1df663ee207e130465cb1789ae982","2ad29134da93304e72dd047ca99ec6cfef2b4990","d9b47764db442dc1bc1dad1570c85367002afe4a","40718dab3e261c2456c3576d15dd0105f1e2e4e2","32d40133459c318bc66aa781b6ce3c1921c0c13a","548c6ffefd409f77fe24c3482257e03be2cb5617"],"s2Url":"https://semanticscholar.org/paper/f3b9114447a2c1e5bcadb4d0f713765bcf1834f2","s2PdfUrl":"","id":"f3b9114447a2c1e5bcadb4d0f713765bcf1834f2","authors":[{"name":"Yingchao Huang","ids":["11833602"]},{"name":"Dong Li","ids":["1678390"]}],"journalName":"2017 IEEE International Conference on Cluster Computing (CLUSTER)","paperAbstract":"A heterogeneous memory system (HMS) consists of multiple memory components with different properties. GPU is a representative architecture with HMS. It is challenging to decide optimal placement of data objects on HMS because of the large exploration space and complicated memory hierarchy on HMS. In this paper, we introduce performance modeling techniques to predict performance of various data placements on GPU. In essence, our models quantify and capture implicit performance correlation between different data placements. Given the memory access information and performance of a sample data placement, our models predict performance for other data placements based on the quantified correlation. We reveal critical performance factors that cause performance variation across data placements. Those factors include instruction replay, addressing mode, hardware queuing delay of memory requests, off-chip memory access latency, and caching effects. Those factors, which are often not sufficiently considered in the existing performance models, can significantly impact modeling accuracy. We introduce a series of techniques to model those factors. We extensively evaluate our models with a variety of benchmarks with various data placements. Our models are able to quantitatively predict the benefit or performance loss of data placements.","inCitations":[],"pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/CLUSTER.2017.42"],"title":"Performance Modeling for Optimal Data Placement on GPU with Heterogeneous Memory Systems","doi":"10.1109/CLUSTER.2017.42","sources":["DBLP"],"doiUrl":"https://doi.org/10.1109/CLUSTER.2017.42","venue":"2017 IEEE International Conference on Cluster Computing (CLUSTER)"},
{"entities":["Atom","Field (computer science)","Middleware","Molecular dynamics","Run time (program lifecycle phase)","Simulation","Synthetic data"],"journalVolume":"","journalPages":"370-378","pmid":"","year":2017,"outCitations":["36ae53af9ef18f0f66bd6ebc7bdbea7893805011","56cc64d474dd9f3d87560a0db81d3e14e6146e54","3a10fad57f186e8da3b912ac96e8cfa853734417","53bcccd314c5c7483933881a6c29235407b3e1c3","85f2aaff0e0c06bbd0cd5d52a9bfee4d8d7ab910","701c90f0593e5675d62fc3882bd5da9b7c296394","abc9d1c519c350845506d58bdd109c9f4d5c2492","771156b34f7f4f539ef7289027e2205692206aed","2073266dfb3f034d55cd5a3fca62d230832afd43","7bb58fe298cda4983f3a51d3ad64298ce68dd136","1a88656a4fdb9989b503a8622273b5d8b9c4b64b","70e1cef1129793954694e0f4519441284448d938","3641cb70c8b14a4840c2f18fce982d00637cb6f9","6d968fd88abf401a5dfef60096acfe658fb178fc","22461880994425508a659df74f8df6ddf2cec3da","3197fcfe91b96162a07b9351dbdf62bb6ec98f3d","43c67bdea30b1a94b77b93128b8d69c451350f4b","4e63fd64ac90778b8c431e37f54b5cfd01e0f379","c1fb67543d08642be5619f43ff5ffff62942bacf","4fe2bf624e18d71d87ae36824606c42c64446562"],"s2Url":"https://semanticscholar.org/paper/ea5cec32e04610174e53ddd02c8ea784de2c44b3","s2PdfUrl":"","id":"ea5cec32e04610174e53ddd02c8ea784de2c44b3","authors":[{"name":"Clément Mommessin","ids":["23198960"]},{"name":"Matthieu Dreher","ids":["40293316"]},{"name":"Bruno Raffin","ids":["2583571"]},{"name":"Tom Peterka","ids":["2284463"]}],"journalName":"2017 IEEE International Conference on Cluster Computing (CLUSTER)","paperAbstract":"In situ workflows contain tasks that exchange messages composed of several data fields. However, a consumer task may not necessarily need all the data fields from its producer. For example, a molecular dynamics simulation can produce atom positions, velocities, and forces; but some analyses require only atom positions. The user should decide whether to specialize the output of a producer task for a particular consumer and get better performance or to send more data than required by the consumer. The first option limits task portability, while the second wastes resources. In this paper, we introduce contracts for in situ tasks. A contract specifies for a producer each data field available for output and for a consumer the data fields needed as input. Comparing a producer and consumer contract allows automatic selection of the data fields a producer has to send for that consumer. We integrated our contracts mechanism within Decaf, a middleware for building and executing in situ workflows. Contracts enable to automatically extract at the producer the data the consumer needs. We evaluate the cost and performance of message extraction at runtime with both synthetic examples and a real scientific workflow coupling a molecular dynamics simulation with three different data analytics codes. Our contract-based automatic data extraction removes the need to specialize producers while entailing small overheads.","inCitations":["3641cb70c8b14a4840c2f18fce982d00637cb6f9"],"pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/CLUSTER.2017.35"],"title":"Automatic Data Filtering for In Situ Workflows","doi":"10.1109/CLUSTER.2017.35","sources":["DBLP"],"doiUrl":"https://doi.org/10.1109/CLUSTER.2017.35","venue":"2017 IEEE International Conference on Cluster Computing (CLUSTER)"},
{"entities":["Computer data storage","Data-intensive computing","Dynamic random-access memory","Locality of reference","Memory hierarchy","Memory management","Memory-level parallelism","Non-volatile memory","Parallel computing","Scalability","Volatile memory"],"journalVolume":"","journalPages":"152-165","pmid":"","year":2017,"outCitations":["3c761857787b3efe5e65b25bd94c737bf2cd7632","007394c2bae389cf43e46db4567dafe206355c25","46eea309204f088ef9dda197d8273465a641f60c","ab6888a1b024d109c768f81b49c77b585efc975a","b308718d1ede5272ce1eeab1ea3cf5a5e57422df","6d2465be30dbcbf9b76509eed81cd5f32c4f8618","096ee9c89d43fd03a602aed3a37fdf43dc8e60ae","4f70e1583f5d31d29ceed2998c52b2bf6c01e2ec","0314667b05134952a7c0997a245b12dc7190f83b","9efc374f6aef60429750859204aa15fadbee291f","3c89345bb88a440096f7a057c28857cc4baf3695","07a1373065dcb4305f6eb91418c5e8b06b7d9433","468035263afa59095614f26a62e0217da4a1aeed","0552af5b205a1dca3a22a062580651f41708ee05","3ef44315caebb6169b7ffbf9886cd782193ab40e","dc2e2b794a784782d7d9860f1358aa107f71c1bf","74fcc3a4806da111405f057dc84de39f8fed15d7","496a76da3380e45b187f32a7d55ad773a923ffad","86b83278e895f53d95159fe8147e427c32ee9ef5","03a4ead5f929d7a8b661a4626d3c868cff0ed225","1d68e6f94aa8e10ebfdd785843c50427d1e820c4","05c56f4abc527fbf384ad011dc9c0a613955641a","0970e4835df451fe4793d22070ecf34f518e5cfe","40718dab3e261c2456c3576d15dd0105f1e2e4e2","8af127739fd9cac1a56b2466f643d25307fe9eb6","094b881edab3f5833c4ff2f38d4ed207af141bcd","1b938edfde3b3b04c13599c2db87c72b7962f383","1223b0c6c944eef6f907f3e59ed0ac23aecf6d2c","870403ceaadbe9579b1841baa39c1ac2d03fef3e","6f45e84202ee1678772899d3473a0b5d5ee4d886","5f7c6e456216a2741702ddc2e18cd7f740d5a962","0dc38d3afb68f617e23eced7ce2994a0a82feb11","e4aed18e1b965f90a86b57a787c52af44a8c20a4","65edeedb41696f66634627984573885e0bf6f55e","eea1d3484c4f817bddcfd729e374ff7776eee89e","15aa9bdac48ab6c3b1c223a676240b3cbbd4c3d6","57f84612b664dde818ca651e137981baa1e237d9","108c840d5d1847948a2de0250490a327ae069ee6","012d556d67acedc6898930b4c93f54b87aabf5ee","18633256bb17ba0744518479c0752ca87f0d03c6","4acbc54d2494f67badd1b084f1696d1b43336534","6c61473130ccb2009717a28962096d146fbde038","30bb582c2c09abc7eb9dda7d9f80804eeb89f9d7","1dcaf21ff8e70d9a5dd85c8a8bd6ead7201fa08a","968f21d66c841a7e564271bae73893dcce162531","4390f4a06a036b8f04cbb4fe7611fa5af9492797","0d2b7c2421475c7f054a2c1bba9a12b434de47c4","24df43f92741fdc7d8a3ecb87d856b8486baf041","1b62e2f7ac7a8d3c13773dfa23d05f8ab82b1f23","8301c813277cc59b47a84d25dc1e307eee8ce310","1824677a301280f6e8278a9bd256174131476369","2621b8f63247ea5af03f4ea0e83c3b528238c4a1","2996f72197c69044f90f203274a0135f8dc6e157","1c15910d27ee940f71bd1d9a5c25c0230e3025fb","40ccd404abbc52c306442fc7c396e50021d764e7","50de0f6a952131dfe562c5b3836e5d934b39b939","03b6a916498fa8591201a2de5f22344609b1e457","27b09d430e0c1ecec6593596754f9a29661c21c0","a95436fb5417f16497d90cd2aeb11a0e2873f55f","8291ac440fb905ed9406ae8ff4d753635fa59a8d","68073f621072d793e95b9562bf9a9245415d5a96","426527a6ce80455948ac453b09dfa7d8001bd7c4","38f7a05fc9a9aa10165550372e5c18c8643db9f8","ab82581f2225072865c1bf49c0044b05e5afca30","8c34cdd2bab66623d2831004fbd1fa1cdf8a0366","45ce4be870f0a5be7b45b064726696dacd83c786","1f80d8bdf5a0a1787a36ccfc4929f71d14a94e57","705a129de84bcf24b4039150c2fc2be1c24cc24a","fae8a785260ac5c34be82fca92a4abef4c30d655","2092d64f8d99ab8cc5b353bbc3dddf4186bcb461","2fee80acb6f7b4172622e0f40d350339ca4e3dc9","5a113bf27c8f3602e56c78291a9e61800b9ec212","0fca03c476d869660dec04fb83f54161767a4ba7","2f919f99bf5b6d5667968c318b62d7335814ceff","33e64874996ac6d163e4e5a97e28b617de7cc0f5","11d50c97cca6eb9469aeab501c7ce8f9655e08b5","9aa0d7253574e50fe3a190ccd924433f048997dd","26e72340c47b7348e1b1de285f89dd96cc925b27","0b885bb186445ee0c50277d990eca18c53fef09b","8c3b449ed5e0e32e1e1934176265cec8dbc2bb4f","3e74ae88cdaa33bf89136800258bde97ab397ec9","2dc59e60b34b3863e4eb381b17384105fe523cec","8009b1c8cc4af8d3d4b792ac32926487a428172e","109f26c285d48ba8f7b5e259364fecef0b3273f6","89b11dc5ec54d088be960e305aa442ff565fbfd9","85398d5f19157c91bf00da3d36210e72d57887e4","d98df13995c1098756e24a49e39872db09f3a537","434fa04db769935ae61bbcf4d9faa602b9a8c730","64723f2309639eba4d321becc250e4373592144c","0494a1ab6f0dd764fb9039772818b8f269ed70b4","447f492235719d7c2b061b95d818f928d6cbdac5","0edf4ef1b8e09e4abc994f7d450bc090262e2c9b","2394c6644efa856f0da160a0f0031d74cd3b5000","80b7dc0de1c9c4fd16b37e2cdcf3965745ce253e","62a75fe31462ec1ad899aaa29b41bf654fce8799","36de396ee9d1c9991e44c01be35e5206d79c3328","e762b1b654798cec0fb9d6000c7f7c777ac0689f","37b5850e3e75a3462f3991491ca26674925f233b","1c32ad0a42109fab826eb3054df7cfc33b424125","2cfa2068d49fc0e9cc5d96bc498c63e782f7478f","44d886f89cdbd4fdf5dd25d83b2d37deb7541bf7","0eacd1b47786f740b723d906d46e160f143c0378","15e63d368aa803c73b8f5d1315a51ebd7ceea3c3","e8f37a91c2c341c2ba3d082be7d602142784d0a1","54a15f2c25bec4274d5bb423dbc5002426a506a3","6902867509928c0e5c19aff3e62e1def3a19d581","12203385fbe8e26aefa1d82c9effaacb44f27a98","42f174df3876256dd5606bb61b366116e9943beb","4c689148ee5e9d6d116f6babbfab21bf2116802e","7738859976be5d1e4fdeec73a847ccb509138290","2263e2beda96efcff8ff19efaefcb85f5136aeca","c3da4e02b8a474d6f094f1e5ac435049e0fe3e49","9341125876271d46cc25f86dac93f25acb343e8d","a6aef44e203a16abe0b6354cf9e4856f211e8bde","084037d504c95c1af6fb1398179f8495618b72d7","96a0c614362aa4c4a9caf63359ac7cd8742c4539","4b82766a16aa951020e43d6f70b5cf097a6b353c","2a660e81e6501ec3489d962fe87448ecf277237f","0653e2ed9f683868cb4539eb8718551242834f6b"],"s2Url":"https://semanticscholar.org/paper/15388b06b42d9a61a1d083bc3bf140ef40f066fa","s2PdfUrl":"","id":"15388b06b42d9a61a1d083bc3bf140ef40f066fa","authors":[{"name":"Yang Li","ids":["1678662"]},{"name":"Saugata Ghose","ids":["33801185"]},{"name":"Jongmoo Choi","ids":["1689391"]},{"name":"Jin Sun","ids":["1894402"]},{"name":"Hui Wang","ids":["1685600"]},{"name":"Onur Mutlu","ids":["1734461"]}],"journalName":"2017 IEEE International Conference on Cluster Computing (CLUSTER)","paperAbstract":"While the memory footprints of cloud and HPC applications continue to increase, fundamental issues with DRAM scaling are likely to prevent traditional main memory systems, composed of monolithic DRAM, from greatly growing in capacity. Hybrid memory systems can mitigate the scaling limitations of monolithic DRAM by pairing together multiple memory technologies (e.g., different types of DRAM, or DRAM and non-volatile memory) at the same level of the memory hierarchy. The goal of a hybrid main memory is to combine the different advantages of the multiple memory types in a cost-effective manner while avoiding the disadvantages of each technology. Memory pages are placed in and migrated between the different memories within a hybrid memory system, based on the properties of each page. It is important to make intelligent page management (i.e., placement and migration) decisions, as they can significantly affect system performance.In this paper, we propose utility-based hybrid memory management (UH-MEM), a new page management mechanism for various hybrid memories, that systematically estimates the utility (i.e., the system performance benefit) of migrating a page between different memory types, and uses this information to guide data placement. UH-MEM operates in two steps. First, it estimates how much a single application would benefit from migrating one of its pages to a different type of memory, by comprehensively considering access frequency, row buffer locality, and memory-level parallelism. Second, it translates the estimated benefit of a single application to an estimate of the overall system performance benefit from such a migration.We evaluate the effectiveness of UH-MEM with various types of hybrid memories, and show that it significantly improves system performance on each of these hybrid memories. For a memory system with DRAM and non-volatile memory, UH-MEM improves performance by 14% on average (and up to 26%) compared to the best of three evaluated state-of-the-art mechanisms across a large number of data-intensive workloads.","inCitations":["1cf5e11f8230c9badb8e963c070ecca2c1bda709","15aa9bdac48ab6c3b1c223a676240b3cbbd4c3d6","0b393cab00401cb971cf71970e00c2767f881f75","2561d914980ab90d0e92fa045cbdc24867fe132c"],"pdfUrls":["http://www.andrew.cmu.edu/user/yangli1/UHMEM.pdf","http://www.pdl.cmu.edu/PDL-FTP/NVM/17cluster_uhmem.pdf","http://doi.ieeecomputersociety.org/10.1109/CLUSTER.2017.130"],"title":"Utility-Based Hybrid Memory Management","doi":"10.1109/CLUSTER.2017.130","sources":["DBLP"],"doiUrl":"https://doi.org/10.1109/CLUSTER.2017.130","venue":"2017 IEEE International Conference on Cluster Computing (CLUSTER)"},
{"entities":["Algorithm","Benchmark (computing)","Big data","Graph (abstract data type)","Intrusion detection system","Network traffic control","Next-generation network","Scalability","Synthetic data","Veracity"],"journalVolume":"","journalPages":"278-289","pmid":"","year":2017,"outCitations":["05d6e0185bcb48d396fe778ceedb2078e37e72ef","1a48cfe2da6f5f0b79b332525e9df699c07a2ec4","67d0ef219bb1dfd2deeb005cf328f5281f5a92a9","0202d62899140f1c358ead59e92caabd4b6994d1","38c978de58ef96b4ad66bc5ad81a01fa16fc306d","41e8c5603c0429c5caf460b3ecf7262c0c29b270","74ab1f58c81889deea75f87da74e3c62911ceda6","8a2041e7c6a28fde0b773a3010ee4c58cacc5572","5becbe1a43145a46160903a5f966570e342c68f0","4cdcbd877edd74d96f3c900e7c60a579a140b782","1d57d5b9832f936e6482ab4e9950e2a862262fe9","64da5b530d72699adcb295a13064842595e22cf8","37a50f1591d382869a3c66f66c0a5d0ae95daad9","e9fa7e93d2f4e958cde16c6cf9d5bc966f8e7ffc","84bb64f5b935818163c668dcb5025214e7334a73","2a005868b79511cf8c924cd5990e2497527a0527","92163d1bee0f7a2e10f8abefcdf2cf6b520cf836","0cb9928daa3b90ac6959d9fb863c5f8ad6422dde","3315793b73a971dce255eb634343c16606c62b76","52ff64f7f26b28447af255fedeb2216a70b48d66","f975b3a59d95b9e5d00134d52fde7bdad119f958","40eb1f990ac292b14b56ea06e61d9aeb9bfa28c3","141e35263ab810983c90d47ad62eb4fab5e51717","0fb0781cac8b3caac879f19d53cce72bd3de2397","c3cf0ab1a6d4a47dce389bfe42ad5cfc685946aa","3aed29136db8f1e5c6a89fc22d3ae4b4926a3555","39e7bd27ae9c20e597f522bc358f6feadc7e382e","0f3b35cecd0ea8cbbb5a8508924880a40c117df2","025e224cf0f12f772c7efba4f7c6b769a2bf298b","0d2c4723e9e5925cde74bd879611fda6f6e3980b","4653782d8416577d6f8f47cc4d4c7ea424536f40","0041adeb171b36f7ef568b6934ea3f4263163c05","9ee76efb171dbc1264ab4b22933e3deedfd7fde8","cd642576ce8502b533e229b537f9ffbe9254aef6","996263c3ddbb50f0198354827445abd214f83030","1d6320a672b866444737880cee8a980f5cca6864","1f0612de1f191abadf250b78cd78f884203cca5e","0371f9e3efbcd4829b5ffbff585155746ef05284","4cee38d9d088cf021bc5f5b9fda6764feeb1806a","17ca3ac5ce530d6a5b5c70ca54e93362e34b9b82","31f27864950a6c417cf996927b2d5558f70d2b14","6b1146e51b2c4d5b8433d4d9c6dbf87c6c484196","0112891050537d4f587529c396c8b9855796d182","eb82d3035849cd23578096462ba419b53198a556","4755952f4c96d22407a62a5937478d0979e4840b","01c1f0e97ce5c74c714dc7aa43cb064f45cc3b04","f4e959d8b5c09739931a2d9e4a9f27ddd1f31d60","4ea7f0e7a13b555ee3adc404de38e5d5ae5c8a67","9dbae30f8253791138e6c1031c5b7e4c7b321185","6f5c6297f9c7dccddac313c8344061cfd12509f7","06c9511a3a29f1afa3971b1885ad56b5a890dbdc","f2f7774c9f56ad18e2bda3f1a62200e0b5e63b65","a5aad5abb32f6b15f31b92312bb3b0f7b6470977","dbc0e5e5cda117a89989937fc3406b7d3f1ea9b6","01c9330af094b7fba0f2d355d043270176b97614","7578f6ce96bcf3215928c2ec5d8666e6baa9c8d9","846fcf30dc75f04886092891e754791e9704f69f"],"s2Url":"https://semanticscholar.org/paper/ca616f7fb2bf65e41e96532a44997ff8e5f4741e","s2PdfUrl":"","id":"ca616f7fb2bf65e41e96532a44997ff8e5f4741e","authors":[{"name":"Stefano Iannucci","ids":["2764399"]},{"name":"Hisham A. Kholidy","ids":["2026840"]},{"name":"Amrita Dhakal Ghimire","ids":["26425345"]},{"name":"Rui Jia","ids":["2322206"]},{"name":"Sherif Abdelwahed","ids":["1913953"]},{"name":"Ioana Banicescu","ids":["3343395"]}],"journalName":"2017 IEEE International Conference on Cluster Computing (CLUSTER)","paperAbstract":"Property-graphs are becoming popular for Intrusion Detection Systems (IDSs) because they allow to leverage distributed graph processing platforms in order to identify malicious network traffic patterns. However, a benchmark for studying their performance when operating on big data has not yet been reported. In general, benchmarking a system involves the execution of workloads on datasets, where both of them must be representative of the application of interest. However, few datasets containing real network traffic are openly available due to privacy concerns, which in turn could limit the scope and results of the benchmark. In this work, we build two synthetic data generators for benchmarking next generation IDSs by introducing the support for property-graphs in two well-known graph generation algorithms: Barab&#xe1;si-Albert and Kronecker. We run an extensive experimental evaluation using a publicly available dataset as seed for the data generation, and we show that the proposed approach is able to generate synthetic datasets with high veracity, while also exhibiting linear performance scalability.","inCitations":[],"pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/CLUSTER.2017.54"],"title":"A Comparison of Graph-Based Synthetic Data Generators for Benchmarking Next-Generation Intrusion Detection Systems","doi":"10.1109/CLUSTER.2017.54","sources":["DBLP"],"doiUrl":"https://doi.org/10.1109/CLUSTER.2017.54","venue":"2017 IEEE International Conference on Cluster Computing (CLUSTER)"},
{"entities":["Algorithm","Computation","Distributed memory","Manycore processor","Markov chain","Markov chain Monte Carlo","Monte Carlo","Parallel computing","Pseudorandom number generator","Pseudorandomness","Run time (program lifecycle phase)","Sampling (signal processing)","Shared memory"],"journalVolume":"","journalPages":"520-524","pmid":"","year":2017,"outCitations":["6377fee5214d9ace4ce629c9bfe463bdebbd889f","5aaa65fbe2abe27afb237b2f40909d686b14b1ee","2359b12b0f4c70477f51455d9eb41923e740104a","5cbc9bc870c8501af36e9f31f36f6d6fe4e932d8","2749bc7bdf1a1016134bf10cb2c76c037c8f2c4d","0b21d126da425fba4b14e4bc9f74f4f2221b5bf9","983399a958ca0e6b26886e441ab5c4ddba836fc5","a46726f4f45ba831fb29250a868f3b35869998a9","ffa1dbe3a7edda21daf6e065511569e4ca2987b3","0a8b7e384dcfa89b5466b2a2d2375d61a3d8f1ce","d45ec41b45caa8686fa1788d9191ab4044a18a83"],"s2Url":"https://semanticscholar.org/paper/53b204514625f3fa0afae23c3877580776a3f103","s2PdfUrl":"","id":"53b204514625f3fa0afae23c3877580776a3f103","authors":[{"name":"Balázs Németh","ids":["2481205"]},{"name":"Tom Haber","ids":["1806102"]},{"name":"Jori Liesenborgs","ids":["1984924"]},{"name":"Wim Lamotte","ids":["1775641"]}],"journalName":"2017 IEEE International Conference on Cluster Computing (CLUSTER)","paperAbstract":"Markov Chain Monte Carlo methods provide a tool for tackling high dimensional problems. With many-core systems readily available today, it is no surprise that leveraging parallelism in these samplers has been a subject of recent research. The focus has been on solutions for shared-memory architectures, however these perform poorly in a distributed-memory environment. This paper introduces a fully decentralized version of an affine invariant sampler. By observing that a pseudorandom number generator makes stochastic algorithms deterministic, communication is both minimized and hidden by computation. Two cases at opposite ends of the communication-to-computation ratio spectrum are used during evaluation against the currently available master-slave solution, where a more than tenfold reduction in execution time is measured.","inCitations":[],"pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/CLUSTER.2017.68"],"title":"Distributed Affine-Invariant MCMC Sampler","doi":"10.1109/CLUSTER.2017.68","sources":["DBLP"],"doiUrl":"https://doi.org/10.1109/CLUSTER.2017.68","venue":"2017 IEEE International Conference on Cluster Computing (CLUSTER)"},
{"entities":["Attribute–value pair","Benchmark (computing)","Communications protocol","Data access","Datapath","Deployment environment","Dynamic random-access memory","Forwarding plane","Java virtual machine","Key-value database","Memcached","Middleware","Pipeline (computing)","Remote direct memory access","Solid-state drive","USB flash drive","User space"],"journalVolume":"","journalPages":"582-586","pmid":"","year":2017,"outCitations":["742c641506ac9efc3281af2effb31f2fb31b2dd4","029e03cd045b1fcda76e4c469eedfa0470c79624","78e042b06806df839e0b87d98a0eba9891ab3634","0e6b0665e0fc3c0c152885869f6c0d339aba06a1","fd54293ccb8c629a5fd8c17584cd37121f399149","28f13ebe8e17fdb4c2500c515759a3ee0c2783ce","11c136aa1136ccf6ebbb23c3b3e1fbdd8447bb00","7129b305ce45f83127e928e8510da9fae0783905","29a1148d75878671dc3663bf480e33d7bd91597d","30419d0e0fcaebbfbcdb88f702bd01306d14fb15","0276440f721b17ff77165f2b1ed24e029b9a2432","1c82d6dd3fde20878f9500c31351a3ceb9c05a46","f992322459aceac1567529e8091e5acae3fcb07f","225603198cc415d363db8a8a2bd30b0df3c963b1"],"s2Url":"https://semanticscholar.org/paper/b0e479c5dbc05b9eb4ed63747a5271c3e10ade1c","s2PdfUrl":"","id":"b0e479c5dbc05b9eb4ed63747a5271c3e10ade1c","authors":[{"name":"Zhongqi An","ids":["26373868"]},{"name":"Zhengyu Zhang","ids":["2638943"]},{"name":"Qiang Li","ids":["20638185"]},{"name":"Jing Xing","ids":["2032859"]},{"name":"Hao Du","ids":["1684572"]},{"name":"Zhan Wang","ids":["1718667"]},{"name":"Zhigang Huo","ids":["2462515"]},{"name":"Jie Ma","ids":["1686247"]}],"journalName":"2017 IEEE International Conference on Cluster Computing (CLUSTER)","paperAbstract":"In-memory key-value store is a crucial building block of large-scale web architecture. Given the growth of the data volume and the need for low-latency responses, cost-effective storage expansion and fast large-message processing are the major challenges. In this paper, we explore the design of key-value middleware that takes advantage of modern NVMe SSDs and RDMA interconnects to achieve high performance without excessive DRAM deployment. We propose an all-in-userland approach to improve the data plane efficiency. Both NVMe and RDMA are interfaced directly from the user-space for effective data access and tailored data management. We present a low-latency storage extension framework based on NVMe and a new design of JVM-aware Memcache protocol based on RDMA. To further accelerate large-message transfer, we provide a hybrid communication protocol fusing Eager and Rendezvous schemas, and a united I/O staging approach to achieve maximum latency hiding through pipelining. As the benchmarking results indicate, with the non-negligible JVM overhead taken into account, our solution obtains comparable communication performance with the RDMA-Memcached released by the OSU. For SSD-involved operations, the latency decreases by up to 31% compared to the kernel-based I/O processing.","inCitations":[],"pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/CLUSTER.2017.69"],"title":"Optimizing the Datapath for Key-value Middleware with NVMe SSDs over RDMA Interconnects","doi":"10.1109/CLUSTER.2017.69","sources":["DBLP"],"doiUrl":"https://doi.org/10.1109/CLUSTER.2017.69","venue":"2017 IEEE International Conference on Cluster Computing (CLUSTER)"},
{"entities":["Benchmark (computing)","Directed acyclic graph","Multi-core processor","Parallel computing","Parallel programming model","Programmer","Run time (program lifecycle phase)","Runtime system","Scheduling (computing)","Software suite","Time series","Timeline","Utility"],"journalVolume":"","journalPages":"114-125","pmid":"","year":2017,"outCitations":["0606b9b2bedb67039eb1615f8ef5b13f42f8339a","5e28ff23f2804b5581e000cbed3c58b5ed163854","d2378cbfe444ca619aaf1de6e6240df5b2667912","b3f8d6e69302b0ee1b40e01bf65da138f9d0f281","8fc0623a28cc193927cd012bd8daac5e6cad75d3","053955b8003d82ef26c06ee3a5ffcc49459c4b23","83f2087f3c602d043277927380e35885879210f5","cf7486af6017a124c5070cd1021ed54a52861a72","b67c85e514a4aee75a9348c9c6cae1566e1b3216","ac7e5716b47cc2678b70dadd34d27648ceecfb0c","0fb659af82f2277c8a62ac888f4bfd01570e5470","b20cdd99af5421e93c811873411b55e7c26a4c69","c5d0d547b6a3fa470dcc77f558f6c7c5768edabd","1914348544b6145be77945fce14e2c68b56dd17e","8f26c867d791b619c0867c90b3171d8bf9ed8dc0","1eac8c7fb82607a6d20187cfb29b3f9a02d578c2","1134aaa6a93f502cac9ce551b13c00b10ff34feb","1f2ff98f9413bb36c641e9edcfa79f7b33eeb80a","47fea97038923902a502403219fc44fd22b5d19f","0b7373f2588fda6732aec095f3d98be8b6621124"],"s2Url":"https://semanticscholar.org/paper/f6944bffd4e30f6b406d273d6a84a34a0e834bc2","s2PdfUrl":"","id":"f6944bffd4e30f6b406d273d6a84a34a0e834bc2","authors":[{"name":"An Huynh","ids":["2716195"]},{"name":"Kenjiro Taura","ids":["1724468"]}],"journalName":"2017 IEEE International Conference on Cluster Computing (CLUSTER)","paperAbstract":"Modern task parallel programming models provide sophisticated runtime task schedulers for handling the scheduling of logical tasks on a large and varying number of hardware parallel resources at runtime. The performance of these programming models increasingly rely on how fast their runtime schedulers do their job. The more delay a scheduler incurs in matching a ready task to a free processor core at any point in time, the more impact it causes to the program's parallel execution. We have developed a tool that is able to detect these delayed intervals caused by the scheduler in a parallel execution, and spot them specifically on two kinds of visualizations: the logical task graph captured at runtime (DAG visualizations) and time-series visualizations of threads (timelines). By further analyzing positions of these delays on those visualizations the tool could identify possible scheduling issues in the scheduler that causes these delays, yielding improvement insights for the development of task parallel programming models. From an application programmer's perspective, our tool is useful by being able to contrast differences of various task parallel programming models executing the same program, helping users choose the right model for their application. We demonstrate that usefulness by using the tool to analyze 10 applications in BOTS benchmark suite in our case studies.","inCitations":[],"pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/CLUSTER.2017.82"],"title":"Delay Spotter: A Tool for Spotting Scheduler-Caused Delays in Task Parallel Runtime Systems","doi":"10.1109/CLUSTER.2017.82","sources":["DBLP"],"doiUrl":"https://doi.org/10.1109/CLUSTER.2017.82","venue":"2017 IEEE International Conference on Cluster Computing (CLUSTER)"},
{"entities":["Algorithm","Central processing unit","Computational complexity theory","Experiment","Pattern matching","Scalability","Synthetic data"],"journalVolume":"","journalPages":"1-12","pmid":"","year":2017,"outCitations":["6db8e7dddc951a88cc399c5ffb99ceeeeb870579","2b0cc03aa4625a09958c20dc721f4e0a52c13fd0","1dd8db60043f51c04eb7200915ebd253d2fabf64","9447b4502eed007a117e4ba87278407ca3d7b354","6cee78f7fef3426824ca5d63a58261ebbbe2e74a","f3a39750bc525e9a7fb42b130c2ee58f5faa188e","3c375359b70cf8d96bd586e9cafcd42bd9ef8698","87507a498558ed6ed23115a42f42376c0884f7f2","06f75b1b283569baf96f4a65ec7da734b9c840f8","065066a94860279587ecc7c7caaa65303008940f","4895aa38cd1d7cc7fa1c1817d57a3aa41f786e21","dd31b94077f656630348f810607308204d5fe013","677b78d89b626ddbc8de190f49e07b96f2cb71c1","09b64bbaeaf557a46c6397830eb09f4318600e34","28e994b359c9d843c63128a3b54bb5dda7b7b2ac","420a0e5fc398f197bca3dfe40291a82b2c65655a","0b2fa2eb6e7a731e08a10bf5766061c61e2a9e9b","3246e9e2056ff7330c61f5fb368db002b0fbe129","240ff3ad3f8931f278404ebbd22e13c996b039e0","30b2dc70b23d7033f58d0307dab0f49d015ae09a","5d3158674e1a0fedf69299a905151949fb8b01a5","7d49e994f7feb75efea290f241d9c5122c8b6438","eb82d3035849cd23578096462ba419b53198a556","638deeb9efa10f081f74e6c2ee9195716afd2ceb","039de08a43c1de269652ec991665bca76062a7db","4066409e187467ae5ee989112b07aa9ce263732a","1267347de992c524b933040cb96fabb93cba1738","01c1f0e97ce5c74c714dc7aa43cb064f45cc3b04","d0df7dba44c1d1d287b1fc5abaa34e39dc7d9a53"],"s2Url":"https://semanticscholar.org/paper/68ab59d03f1323a519704afc7f1d70060d668d25","s2PdfUrl":"","id":"68ab59d03f1323a519704afc7f1d70060d668d25","authors":[{"name":"Tahsin Reza","ids":["35885602"]},{"name":"Christine Klymko","ids":["3215132"]},{"name":"Matei Ripeanu","ids":["1747805"]},{"name":"Geoffrey Sanders","ids":["1795587"]},{"name":"Roger A. Pearce","ids":["2157171"]}],"journalName":"2017 IEEE International Conference on Cluster Computing (CLUSTER)","paperAbstract":"Subgraph pattern matching is fundamental to graph analytics and has wide applications. Unfortunately, high computational complexity limits the robustness guarantees of existing algorithms: they do not scale for modern large graph datasets and/or they have limitations in terms of accuracy or in terms of the intricacy of the patterns supported. We present algorithms, theory, and empirical evidence that iteratively eliminating vertices that do not meet local constraints dramatically reduces the search space for pattern matching in real-world graphs, and demonstrate a scalable implementation of our algorithms. We additionally identify the characteristics of patterns for which every non-eliminated vertex participates in a match. These techniques are an essential step to enable scalable, practical solutions for robust pattern matching in large-scale labeled graphs.We demonstrate the advantages of the proposed approach through strong and weak scaling experiments on massive-scale real-world (up to 257 billion edges) and synthetic (up to 2.2 trillion edges) graphs and at scales (256 compute nodes with 6,144 processors) orders of magnitude larger than those used in the past for similar problems.","inCitations":["4ee59a52acd5b5115fede0f466ff5059662f4952"],"pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/CLUSTER.2017.85"],"title":"Towards Practical and Robust Labeled Pattern Matching in Trillion-Edge Graphs","doi":"10.1109/CLUSTER.2017.85","sources":["DBLP"],"doiUrl":"https://doi.org/10.1109/CLUSTER.2017.85","venue":"2017 IEEE International Conference on Cluster Computing (CLUSTER)"},
{"entities":["Assistive technology","Code refactoring","Computer data storage","Data model","Decimation (signal processing)","Delta encoding","IBM WebSphere eXtreme Scale","Multitier architecture","Pervasive informatics","Simulation","Software ecosystem","Supercomputer","Triangulated irregular network"],"journalVolume":"","journalPages":"58-69","pmid":"","year":2017,"outCitations":["55a022ab83a0f848f3e18693ac3a4ffe016f2704","64d4f6759b32697e6cbebf901624c93c0a0c1744","4908fe53a91465eaf95b21c4ca4f05378b90dcc4","5066738eb7bbcd5f2e793488f7645f8ed946161b","8097630668c8115f03b6d320a7b5cdc1f005066e","2b0bce44b3840796d4ed578c43542ebf839d74af","093fc19d440f33247e545ec6c047e0aa0afb0863","05e0dd9ba23f99acf5537b51f3a3263d3febe6dc","41c80e4077e7630688a8a511125c4662f37e6d34","24b25dd17ee2396910f3df74481ee225d5d440bd","1bbdc1c5ee0ac447472bc3f4de720ab885ff4c43","4e63fd64ac90778b8c431e37f54b5cfd01e0f379","2185f1a1e8736207c91ac00cbd34a08e8a8a0c3e","478b9e77e2bd5b4fb4d8628e8fbafdb2d1a0d23d","d2f8a260a7ae14fc162d9c7976c6c6b7e5f00a77","0d98b995638e1aa0de2f4a66cb727b05fea99b89","7e7dc60e3c684e0e626909f73551334b69ea8817","57d0f3ef74f97c58661e705218d9f19e04e519a3","509f4a29499687ff1f0e3af5a00c0149aee66448","a113e9ef7b31d0b9131d905ee15f5556fdb0652c","09b71bc8d83e2583319b5bd42838e6c4ffa0bd70","009342aa77a56c46a475fa85e66506219f271526","1d3a151a18ac5a479fa46d342e464751dd668d23","4e055f0ce6220e6d75aa2c6d7de50455dea572ef","63bcdebeb86065679aaaad1eea7173c633e748e2","4432b1ef0b18015f3f20f09d8a80ee3dc6a3edab","7777d299e7b4217fc4b80234994b5a68b3031199","ea1db5a68cea156b11eadb3d2ddcb791e5991949","5355bcc49732bc71674e872097257c95f9e9a3ac","616c3d993911812577235adfe994fdfe74af8f8f","15c7d3d5cfce46110a5aa5c6a482e359a96082b4","721c5be47c923d9c0303a3eefd3d42a57e0add03","88a32f0546fccb673225fd2fcc4d9918e7a42298","d158b5f2c77b8127aebcfbfceabe0b818c6bdc7f","04fa1a1d9298f7d56cab3f897def24057d48993f","d004de96c6c1712e77a802534c339628e626945d"],"s2Url":"https://semanticscholar.org/paper/1e23b9e0a2f95521d72fe39c0e310450e782b264","s2PdfUrl":"","id":"1e23b9e0a2f95521d72fe39c0e310450e782b264","authors":[{"name":"Tao Lu","ids":["1792031"]},{"name":"Eric Suchyta","ids":["7920553"]},{"name":"David Pugmire","ids":["2193258"]},{"name":"Jong Choi","ids":["39899324"]},{"name":"Scott Klasky","ids":["1736095"]},{"name":"Qing Liu","ids":["1727669"]},{"name":"Norbert Podhorszki","ids":["1734819"]},{"name":"Mark Ainsworth","ids":["2942322"]},{"name":"Matthew Wolf","ids":["4003076"]}],"journalName":"2017 IEEE International Conference on Cluster Computing (CLUSTER)","paperAbstract":"Scientific simulations on high performance computing (HPC) platforms generate large quantities of data. To bridge the widening gap between compute and I/O, and enable data to be more efficiently stored and analyzed, simulation outputs need to be refactored, reduced, and appropriately mapped to storage tiers. However, a systematic solution to support these steps has been lacking on the current HPC software ecosystem. To that end, this paper develops Canopus, a progressive JPEGlike data management scheme for storing and analyzing big scientific data. It co-designs the data decimation, compression and data storage, taking the hardware characteristics of each storage tier into considerations. With reasonably low overhead, our approach refactors simulation data into a much smaller, reduced-accuracy base dataset, and a series of deltas that is used to augment the accuracy if needed. The base dataset and deltas are compressed and written to multiple storage tiers. Data saved on different tiers can then be selectively retrieved to restore the level of accuracy that satisfies data analytics. Thus, Canopus provides a paradigm shift towards elastic data analytics and enables end users to make trade-offs between analysis speed and accuracy on-the-fly. We evaluate the impact of Canopus on unstructured triangular meshes, a pervasive data model used by scientific modeling and simulations. In particular, we demonstrate the progressive data exploration of Canopus using the &#x201c;blob detection&#x201d; use case on the fusion simulation data.","inCitations":[],"pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/CLUSTER.2017.62"],"title":"Canopus: A Paradigm Shift Towards Elastic Extreme-Scale Data Analytics on HPC Storage","doi":"10.1109/CLUSTER.2017.62","sources":["DBLP"],"doiUrl":"https://doi.org/10.1109/CLUSTER.2017.62","venue":"2017 IEEE International Conference on Cluster Computing (CLUSTER)"},
{"entities":["Commodity computing","Computation","Edge computing","Graph (abstract data type)","Graph partition","In-memory database","Model of computation","Out-of-core algorithm","Server (computing)"],"journalVolume":"","journalPages":"256-266","pmid":"","year":2017,"outCitations":["1156f60e40548096df49528b1342bb3e88b0f378","dc23ad6d4eb652718a2674486037454ec509eef5","0ad8e89091eed09217e66adc98136126addc2619","ee947a4654479e4098142c0369de7698c2e1475d","5f3f9223c5c9f896be099bc177929febad508407","2ae3ac3f7463f838c38e6ca250ca294e813529f2","3486aeaf540c48952120fe853d672af984f40a6a","ef9d9821df55442f039b128bb5cef2b41ab2cadc","3726c60552263e648c6856679e672de2e1c110e5","c0bbb56b4428e9a83d067c07054946293b475fe9","24e8be45a2b2a30a01b7e9f1502e7bd6a7870e7a","0c8ed7f86d881dffb82b24f718bece6cb0e5c76f","080f44d89bf6f4404f476ffec8d2f8ad3f60e07d","423befa4222b5b54cf63f0879e99243b0e5139b0","6de3915df2b9927a78f213629f3bcb052ec21e8b","5b6c248974d8e4c1311863454e5c19b44d2aeb4a","6a888f3dd0a17b0241be61daa378ba6caffa6617","859edc821f821b74fc9c818e45bcecb850603d07","2def083fb7fd8f887c507c0b0b32bd921a26df9b","5bd9374195809c73157ba876f463ea7c4ec9abb5","1452f20140dba52b928c9be5f385b5ac35537a2c","0e33dd74064b3d7659d9ab6301c21c0480cfda72","0443504ed242c5b5de741785eeccfb3eac576e12","8d1a4ae1d3a17edd5b16653f2f582f9952b71612","420a0e5fc398f197bca3dfe40291a82b2c65655a"],"s2Url":"https://semanticscholar.org/paper/5b69e39d4f403e3c39ebee0742216e878fee110f","s2PdfUrl":"","id":"5b69e39d4f403e3c39ebee0742216e878fee110f","authors":[{"name":"Peng Sun","ids":["39572993"]},{"name":"Yonggang Wen","ids":["40096128"]},{"name":"Ta Nguyen Binh Duong","ids":["1767765"]},{"name":"Xiaokui Xiao","ids":["33285410"]}],"journalName":"2017 IEEE International Conference on Cluster Computing (CLUSTER)","paperAbstract":"It is common for real-world applications to analyze big graphs using distributed graph processing systems. Popular in-memory systems require an enormous amount of resources to handle big graphs. While several out-of-core approaches have been proposed for processing big graphs on disk, the high disk I/O overhead could significantly reduce performance. In this paper, we propose GraphH to enable high-performance big graph analytics in small clusters. Specifically, we design a two-stage graph partition scheme to evenly divide the input graph into partitions, and propose a GAB (Gather-Apply-Broadcast) computation model to make each worker process a partition in memory at a time. We use an edge cache mechanism to reduce the disk I/O overhead, and design a hybrid strategy to improve the communication performance. GraphH can efficiently process big graphs in small clusters or even a single commodity server. Extensive evaluations have shown that GraphH could be up to 7.8x faster compared to popular in-memory systems, such as Pregel+ and PowerGraph when processing generic graphs, and more than 100x faster than recently proposed out-of-core systems, such as GraphD and Chaos when processing big graphs.","inCitations":["f84aa869a21f083133b74e23d83ab2dd1378b7ff"],"pdfUrls":["https://arxiv.org/pdf/1705.05595v4.pdf","https://arxiv.org/pdf/1705.05595v3.pdf","https://arxiv.org/pdf/1705.05595v1.pdf","http://doi.ieeecomputersociety.org/10.1109/CLUSTER.2017.51","https://arxiv.org/pdf/1705.05595v5.pdf","http://arxiv.org/abs/1705.05595","https://arxiv.org/pdf/1705.05595v2.pdf","https://export.arxiv.org/pdf/1705.05595"],"title":"GraphH: High Performance Big Graph Analytics in Small Clusters","doi":"10.1109/CLUSTER.2017.51","sources":["DBLP"],"doiUrl":"https://doi.org/10.1109/CLUSTER.2017.51","venue":"2017 IEEE International Conference on Cluster Computing (CLUSTER)"},
{"entities":["Big data","Data aggregation","Data model","Hard coding","Profiling (computer programming)","Requirement","Run time (program lifecycle phase)"],"journalVolume":"","journalPages":"419-428","pmid":"","year":2017,"outCitations":["81bd1bd083d8a116a5413c545335c77e5f9e68be","1f2ff98f9413bb36c641e9edcfa79f7b33eeb80a","085a92d20bf2bf091a0729b610e5a4d6df4f0047","55af531059610139bdba4f2ac4b1e63062712d6d","e3d80fb27944a0dbc493886faf18472f83aaa227","e65012425ff445a11728cc4922cfc09a4dfd6fd1","51ec4530e2b6d73f410568952db220c05865e073","afc22073b0c2fc62e742dd1a6e7fba6d54fb5e1f","3218bbfd89deae4134d6c6d7f8f3ceb5c3a361f7","869f19dca06cd831ebee02b29cf6319fb25f197a","7c8f5cfe90578324da0fc7815075e610fc1161ff","6af3228141a9891e57f879c6ea2b48787e56e17f","972b71e221cbe66f3daa33b00a678d3cfd42b5af","3fcb48efe8489061deeea37ceafdabe115ca4789","2190f5b82326f4d61312aaa6e6226f1ae618fb0d"],"s2Url":"https://semanticscholar.org/paper/24184cacf24f42ec8e12831d20a295c4141b9526","s2PdfUrl":"","id":"24184cacf24f42ec8e12831d20a295c4141b9526","authors":[{"name":"David Böhme","ids":["1998865"]},{"name":"D. A. Beckingsale","ids":["2309393"]},{"name":"Martin Schulz","ids":["1772965"]}],"journalName":"2017 IEEE International Conference on Cluster Computing (CLUSTER)","paperAbstract":"Almost all performance analysis tools in the HPC space perform some form of aggregation to compute summary information of a series of performance measurements, from summations to more complex operations like histograms. Aggregation not only reduces data volumes and consequently storage space requirements and overheads, but is also crucial to extract insights from recorded measurement data. In current tools, however, most aspects that control the aggregation, such as the data dimensions to be reduced, are hard-coded in the tool for a set of particular use cases identified by the tool developer and cannot be extended or modified by the user. This limits their flexibility and often results in users having to learn and use multiple tools with different aggregation options for their performance analysis needs.We present a novel approach for performance data aggregation based on a flexible key:value data model with user-defined attributes, where users can define custom aggregation schemes in a simple description language. This not only gives users the control to deploy the particular data aggregation they need, but also opens the door for aggregations along application-specific data dimensions that cannot be achieved with traditional profiling tools. We show how our approach can be applied for performance profiling at runtime, cross-process data aggregation, and interactive data analysis and demonstrate its functionality with several case studies driven by real world codes.","inCitations":[],"pdfUrls":["http://doi.ieeecomputersociety.org/10.1109/CLUSTER.2017.34"],"title":"Flexible Data Aggregation for Performance Profiling","doi":"10.1109/CLUSTER.2017.34","sources":["DBLP"],"doiUrl":"https://doi.org/10.1109/CLUSTER.2017.34","venue":"2017 IEEE International Conference on Cluster Computing (CLUSTER)"},
