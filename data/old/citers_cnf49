{"entities":["Apache Hadoop","Array processing","C++","Correctness (computer science)","Excalibur: Morgana's Revenge","Formal verification","Parallel computing"],"journalVolume":"","journalPages":"572-585","pmid":"","year":2017,"outCitations":["879734aa4da358358249be82b4a6b2051ad5aa09","25af0e970cecbe879bbc3b9ff8f24475a9557ea1","6f6e7035d2873ae3eb0f3aaa010e22cf2bed6b18","bf176b6d05fdfdbc1711be2ed568db9cb0153ea4","3793cd493c6b59bdb39593e370a542af84bf8a56","0558c94a094158ecd64f0d5014d3d9668054fb97","67d18339ed72b7fc2152cb42b63362b570c11946","44a796b9a01c2adc6b7978359f3cdc10356e03ce","099cfdca0f11eeaf46dd6457f33caff8e8fbcb41","1e52a2e8535509ab0111c0c5d89a88d3bb10b34c","5412468cac5613762699d107dd519da94541017c","0de7254fb42f09a0bfbc191c71553ab8278636a9","716b3455c4df7b8cfaade6801adf4e8538279ebd","36ef8e3f51b57ee2ec3da311f1a7afa3a09c36a2","c608fece067b98f739ccbc9ce0a434d41997bed9","0282e990528c6a9b4aa92cc196f46257fb4ccee1","1ef301c1b275091b6a50d620b41df4722f2108f0","202e33581369f6050fc800ebc31615eb65649e78","f5f7c64a1b60fa2992044828f1fd099118b05c33","87de316ea08272afbda356b8d580385dd0d8382f","2d692211c220f4b16eabb7639108fba88d00cf2f","80527e7595530951081494d1b98f3f13da3033a2","2311e71450d1c02703749dba3f96a9f842b80cf9","49d5f1340aad43d48bbb3b9df58eb5a250a57396","332f77fd05703c1607e3b57884ad31fb1fad0104","5d0c6e456a9b4f858da875a7d758bc6134f643a7","9e27e05d92801844c06d5296e370bd2b781ef150","569a01f3506a46f2df1a1db1fc5fa638ed01334f","0f37e37310dbeaa063589830159467c5a6f958a9","0541d5338adc48276b3b8cd3a141d799e2d40150","2cdcddb08ae6060e94cba6c9b2b58b87324e686f","25929ecf00df179c51b95b7be250a5220d86d6f6","99cc63730e3079ed58311a4ec88f4f0c891ed61d"],"s2Url":"https://semanticscholar.org/paper/1a0e59990e14d30665bd87030d9c895c7a650e71","s2PdfUrl":"","id":"1a0e59990e14d30665bd87030d9c895c7a650e71","authors":[{"name":"Grigory Fedyukovich","ids":["1736858"]},{"name":"Maaz Bin Safeer Ahmad","ids":["7745797"]},{"name":"Rastislav Bodík","ids":["1991345"]}],"journalName":"","paperAbstract":"Parallelizing of software improves its effectiveness and productivity. To guarantee correctness, the parallel and serial versions of the same code must be formally verified to be equivalent. We present a novel approach, called GRASSP, that automatically synthesizes parallel single-pass array-processing programs by treating the given serial versions as specifications. Given arbitrary segmentation of the input array, GRASSP synthesizes a code to determine a new segmentation of the array that allows computing partial results for each segment and merging them. In contrast to other parallelizers, GRASSP gradually considers several parallelization scenarios and certifies the results using constrained Horn solving. For several classes of programs, we show that such parallelization can be performed efficiently. The C++ translations of the GRASSP solutions sped performance by up to 5X relative to serial code on an 8-thread machine and Hadoop translations by up to 10X on a 10-node Amazon EMR cluster.","inCitations":["413c1a8a26de4de49c2b1204208ca5eb4386b5a6","3d60feda349eccde4a36d2acb3b9f8ed2e799644","0f37e37310dbeaa063589830159467c5a6f958a9"],"pdfUrls":["http://doi.acm.org/10.1145/3062341.3062382","http://grassp.uwplse.org/paper.pdf"],"title":"Gradual synthesis for static parallelization of single-pass array-processing programs","doi":"10.1145/3062341.3062382","sources":["DBLP"],"doiUrl":"https://doi.org/10.1145/3062341.3062382","venue":"PLDI"},
{"entities":["Control flow","Correctness (computer science)","Debugging","Deploy","Failure rate","Formal language","Formal proof","Heart failure","Languages","Polynomial","Polynomial","Programming Languages","Question (inquiry)","Software deployment","Time complexity"],"journalVolume":"","journalPages":"","pmid":"","year":2017,"outCitations":["705d9d72015edb7d9aec9ff5a0742fdf45fbe6d8","d48631279c3a28bb8b1ea5f3b211b3f73b7e44c4","6dd474a2ce9bc35c9d7518c981715bd394a9cd86","97bfbeece4bf1dfcf83dcdef94718bd9c78aa8c2","215aad1520ec1b087ab2ba4043f5e0ecc32e7482","5f1828ca9178ad0fe10b0ad108e4c2720106b890"],"s2Url":"https://semanticscholar.org/paper/0dc147319a8d01b48718b31ed6afe71113702bef","s2PdfUrl":"http://pdfs.semanticscholar.org/1eca/35051c1bbcfafb994e02405bbea7bbbd86f5.pdf","id":"0dc147319a8d01b48718b31ed6afe71113702bef","authors":[{"name":"Peter Ohmann","ids":["34747583"]},{"name":"Alexander Brooks","ids":["40456419"]},{"name":"Loris D’Antoni","ids":[]},{"name":"Ben Liblit","ids":["1697640"]}],"journalName":"","paperAbstract":"Debugging post-deployment failures is difficult, in part because failure reports from these applications usually provide only partial information about what occurred during the failing execution. We introduce approaches that answer control-flow queries about a failing program’s execution based on failure constraints given as formal languages. A key component of our approach is the introduction of a new class of subregular languages, the unreliable trace languages (UTL), which allow us to answer many common queries in polynomial time. This report supplements the description of these new approaches with formal proofs. Specifically: we prove completeness for our context-insensitive query problem, tightly bind polynomial-time decidability of query recovery to the UTL class, and prove partial correctness for our approach to answering user queries with UTL constraints.","inCitations":["705d9d72015edb7d9aec9ff5a0742fdf45fbe6d8"],"pdfUrls":["https://minds.wisconsin.edu/bitstream/handle/1793/76432/TR1845.pdf?sequence=1","http://pages.cs.wisc.edu/~liblit/tr-1845/tr-1845.pdf","http://pages.cs.wisc.edu/~ohmann/papers/tr-1845.pdf"],"title":"Supporting Proofs for Control-Flow Recovery from Partial Failure Reports","doi":"","sources":[],"doiUrl":"","venue":""},
{"entities":["Compiler","Continuation","Haskell","Intermediate representation","Join (SQL)","Join point","The Glorious Glasgow Haskell Compilation System"],"journalVolume":"","journalPages":"482-494","pmid":"","year":2017,"outCitations":["46bd1887e7d15dba0ccfb410a37c6e6bec6f217c","2de3e6d85b2e7cb41a6e87b3188b9101d147dd82","73062e44e8a4b3d80c0a98e009c9604dc90d3911","132aac31b7355eb3f3d110e7cca0c321e43541d6","d5791235ad4439f48fcdd359d45d144ee6a23ac9","13c1c1c4f8b7b87404a929e4e9b339008fd27138","cfe34582dbc7ac0e48796494b4cc2d397c4ae5c2","1e2c604e66c4439ad343b70d7ec0abedf72d006a","d5b48ff78a806fe957eda9b951d726c8029cfd9b","66da7820ffe84c54a6fdb9ac28e27f6346874a93","43252ba020a456768700880f1e10eff3b30d4526","984c126b595d1ce39500757ef85567b32b465250","06e998884b6a31e0d09338377d78b52be402714b","64d2a65a7d559f9b05570fb0fea8bb4cccd83ae2","2f59689eb4801ca6bb8aaabad40abd333dda803a","132a213cc5e8a1ca3be0cdeb2accd42511bde33f","ac62767d3666b692dc55688e3650e6477d3bffa7","1c7487d0879f29facd5f5644b250246b4cd11769","98445cbe502ccc6b45953487aa926cdbe980a491","917a644267e919e329b4f47665586beeece6b004","0723583c40abf490571b6ce62afdac2ab28afd8e","3ebb770bdd641a492f6159d7076ed3609ce7af47","8a3a538bcdfbed630f6bf2fc3ee49fe740819602"],"s2Url":"https://semanticscholar.org/paper/18344f8daf595f91f18fdd6d2b2dff99423087b6","s2PdfUrl":"","id":"18344f8daf595f91f18fdd6d2b2dff99423087b6","authors":[{"name":"Luke Maurer","ids":["38504194"]},{"name":"Paul Downen","ids":["2065311"]},{"name":"Zena M. Ariola","ids":["1706685"]},{"name":"Simon L. Peyton Jones","ids":["35372552"]}],"journalName":"","paperAbstract":"Many fields of study in compilers give rise to the concept of a join point&#8212;a place where different execution paths come together. Join points are often treated as functions or continuations, but we believe it is time to study them in their own right. We show that adding join points to a direct-style functional intermediate language is a simple but powerful change that allows new optimizations to be performed, including a significant improvement to list fusion. Finally, we report on recent work on adding join points to the intermediate language of the Glasgow Haskell Compiler.","inCitations":["acad7ce64bc015c6a2b3581e460b50d656d8126c","6db61a6cdabb9036db9d1c6820fcbb52e13f153b","08de23da3ed2240cce1b6f48e1096cfe806d90d2","65e87447a357c0f2fccf3111ac0241aaec3f0a7f"],"pdfUrls":["http://ix.cs.uoregon.edu/~pdownen/publications/pldi17_appendix.pdf","http://doi.acm.org/10.1145/3062341.3062380","https://www.microsoft.com/en-us/research/wp-content/uploads/2016/11/join-points-pldi17.pdf","https://www.microsoft.com/en-us/research/wp-content/uploads/2016/11/compiling-without-continuations.pdf","http://ix.cs.uoregon.edu/~pdownen/publications/pldi17.pdf"],"title":"Compiling without continuations","doi":"10.1145/3062341.3062380","sources":["DBLP"],"doiUrl":"https://doi.org/10.1145/3062341.3062380","venue":"PLDI"},
{"entities":["Ability to Write question","Appear Here","Calling convention","Categorization","Compiler","Compiler","Conferences","Conflict (Psychology)","Cyclone","Cyclonic Storms","Double-precision floating-point format","Duplicate code","Executable","Float","Formal proof","Gene Duplication Abnormality","Genetic Polymorphism","Haskell","Html Link Type - copyright","Information","Object type (object-oriented programming)","Optimizing compiler","Parametric polymorphism","Pervasive informatics","Pointer (computer programming)","Programming Languages","Rule (guideline)","Tension","The Glorious Glasgow Haskell Compilation System","Type system","Unintended consequences","Value (ethics)"],"journalVolume":"","journalPages":"","pmid":"","year":2016,"outCitations":["1856b5946b1c3acb008ce0bfdb478c46c1028e33","c58994b0e6614b3dcc01a6f1eee3ca0ec477f8f2","2d4aa0f63c26dee36310c6c1ce3fe1fe4b4551e9","1e2c604e66c4439ad343b70d7ec0abedf72d006a","0d5bdcfea6df1d8c7adf71dcc8c5b782f5c3e9d1","184e9846b86c0d95a104fe15ffc78b23684ff7d9","75f70e8992fd12c61edac2059b3e5614c5c61f65","cc7f2242dba6f09023128897762d07517f13ba4a","0df445ca53975d93f27c9def03e964d3113a4607","0fd582a8335379c50cbee8957c5da063b802fec7","07dc9cbc4c4a8a6c10287b793f3b3e0cdf4590b0","d1f5d96374e32f02e1e85c7710fa26860cdc48d3","933aa774c4fd242f6deb0fa6a20daab32481bec5"],"s2Url":"https://semanticscholar.org/paper/ca43e1a3bca565351ea6e05144fa7fbb3418dfb9","s2PdfUrl":"http://pdfs.semanticscholar.org/ca43/e1a3bca565351ea6e05144fa7fbb3418dfb9.pdf","id":"ca43e1a3bca565351ea6e05144fa7fbb3418dfb9","authors":[{"name":"Richard A. Eisenberg","ids":["35238735"]},{"name":"Bryn Mawr","ids":["3977741"]},{"name":"Simon Peyton Jones","ids":["1767406"]}],"journalName":"","paperAbstract":"Parametric polymorphism is one of the lynchpins of modern typed programming. A function that can work seamlessly over a variety of types simplifies code, helps to avoid errors introduced through duplication, and and is easy to maintain. However, polymorphism comes at a very real cost, one that each language with support for polymorphism has paid in different ways. This paper describes this cost, proposes a theoretically simple way to reason about the cost—that kinds, not types, are calling conventions—and details one approach to dealing with polymorphism that works in the context of a language, Haskell, that prizes both efficiency and a principled type system. This approach, levity polymorphism, allows the user to abstract over calling conventions; we detail and verify restrictions that are necessary in order to compile levity-polymorphic functions. Levity polymorphism has opened up surprising new opportunities for library design in Haskell. 1. The cost of polymorphism Consider the following Haskell function: bTwice :: ∀ a. Bool → a→ (a→ a)→ a bTwice b x f = case b of True → f (f x) False → x The function is polymorphic in a; that is, the same function works regardless of the type of x , provided f and x are compatible. When we say “the same function” we usually mean “the same compiled code for bTwice works for any type of argument x”. But the type of x influences the calling convention, and hence the executable code for bTwice! For example, if x were a list, it would be passed in a register pointing into the heap; if it were a double-precision float, it would be passed in a special floating-point register; and so on. Thus, sharing code conflicts with polymorphism. A simple and widely-used solution is this: represent every value uniformly, as a pointer to a heap-allocated object. That solves the problem, but it is terribly slow (Section 2.1). Thus motivated, most polymorphic languages also support some form of unboxed values that are represented not by a pointer but by the value itself. Our 1 We use the term polymorphism to refer exclusively to parametric polymorphism. [Copyright notice will appear here once ’preprint’ option is removed.] context is the Glasgow Haskell Compiler (GHC), a state of the art optimizing compiler for Haskell. It has had unboxed values for decades, but not without the inevitable tensions that arise between unboxed values and polymorphism (Section 3). Other languages deal differently with this challenge (Section 8). In this paper we describe an elegant new approach to reconciling high performance with pervasive polymorphism. Our contributions are as follows: • We present (Section 4) a principled way to reason about compiling polymorphic functions and datatypes, by categorizing types into kinds. Each kind describes the memory layout of its types, thus determining the calling convention of functions over those types. • Having a principled way to describe memory layout and calling convention, we go one step further and embrace levity polymorphism, allowing functions to be abstracted over choices of memory layout provided that they never move or store data with an abstract representation (Section 5). We believe we are the first to describe and implement levity polymorphism. • It is tricky to be sure precisely when it is, and is not, OK to permit levity polymorphism. We give a formal proof that our rules are sufficient to guarantee that levity-polymorphic functions can indeed be compiled into concrete code (Section 6). • With levity polymorphism in hand, a range of new possibilities open up—including the ability to write an informative kind for (→) and to overload operations over both boxed and unboxed types in a principled way. We explore these in Section 7. Levity polymorphism is implemented in GHC, version 8.0.1, released early 2016. We are not the first to use kinds in this way— Cyclone [5] uses a similar approach Section 8.1—but we take the idea much further than any other compiler we know, with happy consequences. Remember: it’s all about performance. If you don’t care about performance, life is much simpler! 2. Background: performance through unboxed types We begin by describing the performance challenges that our paper tackles. We use the language Haskell and the compiler GHC as a concrete setting for this discussion, but many of our observations apply equally to other languages supporting polymorphism. We discuss other languages and compilers in Section 8.","inCitations":["38c1eba6c628ee7fcf2633aa0299b091e6378b60"],"pdfUrls":["http://cs.brynmawr.edu/~rae/papers/2017/levity/levity.pdf"],"title":"Levity Polymorphism (extended version)","doi":"","sources":[],"doiUrl":"","venue":""},
{"entities":["Automaton","Boolean satisfiability problem","Checking (action)","Experiment"],"journalVolume":"","journalPages":"","pmid":"","year":2017,"outCitations":["389f432f73ccc95b5738e80781e085bb00dfd8c4","3d933f007baa84865f609a154d2a378b3d62442f","fbb56b289afd5edb72505188af55282ca13e3ca0","2100d14e855e4c66e845fb4dbddf00849b1be758","d000d98371c266e64c136155b54f7749bdcffaf0","3960dda299e0f8615a7db675b8e6905b375ecf8a","f3c59aff8a7648fd29a31728f39e3aa9ee3f2528","363ff906a42d2ac95456d7d8fb2527989a42f1ec","088a382a5af6a44ccb69c2f49517bf8d99ab6759","1246299e80c8a403313adf0f5c28e4127ed15001","90c28e38506d9cb1c3e3186d7e3971805dfcc675","231bed067708e80dae6d836f8359fb6a2766283d","306f123c6af2577a61f67e9155cf3897ea149d1c","e05e140132042f494e6b64f52c0654bc55d0bbae","5518d5853b694c865a55be5628e7205007806563","06f2bdf034e02b3735f044589dbe60b5665cca49","1dba737ed6207fbd69afd8a5bf9938a8790c4b92","8db84d7974e80554930576e926b9d6de9727daed","0e4a694548b731930cefe91330ece0737c415d32","8eb819823e254c232f2635e69c9ee54a7e2ff387","0634ba86fab525d37981280400963541a2e433b8","595642f2eb3ae457911e710ef5c1816e0d51fd4b","12f000052d3e168a41b2e1968b4488a982cf9fd8","1019341b60ccd9e2144bb1bf8cf7ab2275b12d07","2c173140a36f44fa2767156900cc52e71f75748e","02ea2faa6190bc14f4244386996054ef11b0d89c","6a659685bff877d39cb4d5a20fcdbbf42151b84f","341d33498388711a5303c5f51433b3d5739a21d2","6bb40354a670b1509ee312ec045326791e197ba8","db8fe43a20608c3da49d2edce96377b7d571436e","decb7c40e12fb4e20f04b2b514704575e4481ff8","144382ef2ee1d00ce3d36c61601afecca5620c7d","c537444e60011f290c48dc768cafe3d7d5d3cd1f"],"s2Url":"https://semanticscholar.org/paper/bbd9ef6c67d73e2d780d1d3c484a6ab2b44e156d","s2PdfUrl":"http://pdfs.semanticscholar.org/bbd9/ef6c67d73e2d780d1d3c484a6ab2b44e156d.pdf","id":"bbd9ef6c67d73e2d780d1d3c484a6ab2b44e156d","authors":[],"journalName":"","paperAbstract":"We describe a uniform and efficient framework for checking the satisfiability of a large class of string constraints. The framework is based on the observation that both satisfiability and unsatisfiability of common constraints can be demonstrated through witnesses with simple patterns. These patterns are captured using flat automata each of which consists of a sequence of simple loops. We build a Counter-Example Guided Abstraction Refinement (CEGAR) framework which contains both an underand an over-approximation module. The flow of information between the modules allows to increase the precision in an automatic manner. We have implemented the framework as a tool and performed extensive experimentation that demonstrates both the generality and efficiency of our method.","inCitations":[],"pdfUrls":["http://www.iis.sinica.edu.tw/~yfc/lib/exe/fetch.php?media=pldi2017.pdf"],"title":"Flatten and Conquer (A Framework for Efficient Analysis of String Constraints)","doi":"","sources":[],"doiUrl":"","venue":""},
